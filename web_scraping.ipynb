{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a906ab9a",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "- Ensure no deduplication\n",
    "- Ensure that when re-run old csv with data is kept \n",
    "- Ensure that csv contains time the data is scraped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca0a9d2",
   "metadata": {},
   "source": [
    "# Left to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b094019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://megatix.in.th/events?category=31\n",
    "# https://www.savaya.com/\n",
    "# https://ra.co/events/id/bali\n",
    "# jakarta has issues scraping\n",
    "\n",
    "#https://www.singaporeexpo.com.sg/what-s-on/events-expo?utm_source=google_pmax_sitelink&utm_medium=social_ads&utm_campaign=eventpackages-2526&utm_content=cny2026_general&gad_source=1&gad_campaignid=23175866147&gbraid=0AAAABBnFXuJ1_a_rRdCD1OzIMMiCFAgK_&gclid=CjwKCAiA_dDIBhB6EiwAvzc1cI6Q8DekR41jwb9VBhRaHME2P3lrb1gre2Apdfbq5tl-9aU5F7dkFRoCW8AQAvD_BwE\n",
    "#https://www.thestar.sg/events\n",
    "#https://www.songkick.com/venues/4360044-pasir-panjang-power-station\n",
    "#https://phuket.cafedelmar.com/events\n",
    "#https://www.musinsagarage.com/program\n",
    "#https://www.wanderlochhall.com/15\n",
    "#https://www.musinsagarage.com/program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e922ee9",
   "metadata": {},
   "source": [
    "# Singapore (Completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeff49a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü† Eventbrite: grabbed 6 events\n",
      "üîµ Ticketmaster: grabbed 52 events\n",
      "üü¢ Megatix (Featured): grabbed 5 events\n",
      "\n",
      "‚úÖ Newly scraped this run: 63\n",
      "‚ûï New (not seen before): 7\n",
      "üì¶ Total in master after save: 103\n",
      "\n",
      "üíæ CSV saved: all_singapore.csv\n"
     ]
    }
   ],
   "source": [
    "# COMBINED: Eventbrite + Ticketmaster + Megatix (Featured)\n",
    "# NO DEDUP ‚Äî KEEP ALL EVENTS\n",
    "# pip install tabulate\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.set_window_size(1400, 1000)\n",
    "\n",
    "# ---------- small helpers (added) ----------\n",
    "def slow_scroll_page(driver, max_loops=200, pause=0.8):\n",
    "    \"\"\"\n",
    "    Incrementally scrolls the page and stops when scrollHeight stops growing\n",
    "    for several consecutive loops. Gentle pause to let lazy-load content render.\n",
    "    \"\"\"\n",
    "    prev_h = 0\n",
    "    no_growth = 0\n",
    "    for _ in range(max_loops):\n",
    "        driver.execute_script(\"window.scrollBy(0, Math.max(500, window.innerHeight*0.85));\")\n",
    "        time.sleep(pause)\n",
    "        h = driver.execute_script(\"return Math.max(document.body.scrollHeight, document.documentElement.scrollHeight);\")\n",
    "        if h <= prev_h:\n",
    "            no_growth += 1\n",
    "            if no_growth >= 4:\n",
    "                break\n",
    "        else:\n",
    "            no_growth = 0\n",
    "            prev_h = h\n",
    "\n",
    "def try_click_load_more_by_text(possible_texts=(\"Load more\",\"Show more\",\"See more\",\"More\")):\n",
    "    \"\"\"\n",
    "    Best-effort clicker for generic 'Load more' buttons if present on screen.\n",
    "    Safe to call even if not present.\n",
    "    \"\"\"\n",
    "    xp = \"//*[self::button or self::a][\" + \" or \".join([f\"contains(translate(normalize-space(.), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'), '{t.lower()}')\" for t in possible_texts]) + \"]\"\n",
    "    try:\n",
    "        btns = driver.find_elements(By.XPATH, xp)\n",
    "        for b in btns:\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", b)\n",
    "                time.sleep(0.3)\n",
    "                driver.execute_script(\"arguments[0].click();\", b)\n",
    "                time.sleep(1.2)\n",
    "                return True\n",
    "            except:\n",
    "                pass\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "# -------------------- 1) EVENTBRITE (with auto-scroll + load more) --------------------\n",
    "try:\n",
    "    url = 'https://www.eventbrite.sg/d/singapore--singapore/singapore/?subcategories=3006%2C3025&page=1'\n",
    "    driver.get(url)\n",
    "\n",
    "    LIST_X = \"//*[@id='root']/div/div[2]/div/div/div/div[1]/div/main/div/div/div/section[1]/div/section/div/div/section/ul\"\n",
    "    CARD_X = LIST_X + \"/li\"\n",
    "    BASE   = \".//div/div/div[2]/section/div/section[2]/div\"\n",
    "\n",
    "    WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, LIST_X)))\n",
    "    time.sleep(1)\n",
    "\n",
    "    # --- slow scroll phase (added) ---\n",
    "    for _ in range(8):  # a few cycles: scroll, then try to click 'load more' if it appears\n",
    "        slow_scroll_page(driver, max_loops=40, pause=0.8)\n",
    "        # try expanding if a button exists, then scroll again next iteration\n",
    "        clicked = try_click_load_more_by_text((\"Load more\",\"Show more\"))\n",
    "        if not clicked:\n",
    "            # if nothing to click and page stopped growing, break early\n",
    "            break\n",
    "\n",
    "    for ev in driver.find_elements(By.XPATH, CARD_X):\n",
    "        try: title = ev.find_element(By.XPATH, f\"{BASE}/a/h3\").text.strip()\n",
    "        except: title = \"No Title\"\n",
    "        try: date = ev.find_element(By.XPATH, f\"{BASE}/p[1]\").text.strip()\n",
    "        except: date = \"No Date\"\n",
    "        try: location = ev.find_element(By.XPATH, f\"{BASE}/p[2]\").text.strip()\n",
    "        except: location = \"No Location\"\n",
    "        try: link = ev.find_element(By.XPATH, f\"{BASE}/a\").get_attribute(\"href\")\n",
    "        except: link = \"No Link\"\n",
    "        all_rows.append({\"Title\": title, \"Date\": date, \"Location\": location, \"Link\": link})\n",
    "\n",
    "    print(f\"üü† Eventbrite: grabbed {len(driver.find_elements(By.XPATH, CARD_X))} events\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Eventbrite error: {e}\")\n",
    "\n",
    "# -------------------- 2) TICKETMASTER (scroll) --------------------\n",
    "try:\n",
    "    url = 'https://ticketmaster.sg/categories/concerts'\n",
    "    driver.get(url)\n",
    "\n",
    "    EVENTS_X = \"//*[@id='events']\"\n",
    "    ANCHOR_X = \"//*[@id='events']//a[contains(@href,'/activity/detail/')]\"\n",
    "    TITLE_REL = \".//div[2]/div[2]\"\n",
    "    DATE_REL  = \".//div[2]/div[1]\"\n",
    "\n",
    "    WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, EVENTS_X)))\n",
    "    time.sleep(1)\n",
    "\n",
    "    # --- slow scroll phase (added) ---\n",
    "    driver.execute_script(\"document.getElementById('events')?.scrollIntoView({block:'start'});\")\n",
    "    for _ in range(6):\n",
    "        slow_scroll_page(driver, max_loops=40, pause=0.8)\n",
    "        # some pages lazy-load in batches; small pause between cycles\n",
    "        time.sleep(0.6)\n",
    "\n",
    "    for a in driver.find_elements(By.XPATH, ANCHOR_X):\n",
    "        try: title = a.find_element(By.XPATH, TITLE_REL).text.strip()\n",
    "        except: title = \"No Title\"\n",
    "        try: date = a.find_element(By.XPATH, DATE_REL).text.strip()\n",
    "        except: date = \"No Date\"\n",
    "        link = a.get_attribute(\"href\") or \"No Link\"\n",
    "        location = \"Singapore\"\n",
    "        all_rows.append({\"Title\": title, \"Date\": date, \"Location\": location, \"Link\": link})\n",
    "\n",
    "    print(f\"üîµ Ticketmaster: grabbed {len(driver.find_elements(By.XPATH, ANCHOR_X))} events\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Ticketmaster error: {e}\")\n",
    "\n",
    "# -------------------- 3) MEGATIX (Featured scroll) --------------------\n",
    "try:\n",
    "    url = \"https://megatix.com.sg/?page=2\"\n",
    "    driver.get(url)\n",
    "\n",
    "    WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='__nuxt']\")))\n",
    "    FEATURED_WRAP_X = \"//*[@id='__nuxt']/div/div[3]/div/main/section/section[2]/div[2]/div\"\n",
    "    FEATURED_CARD_X = \"//*[@id='__nuxt']/div/div[3]/div/main/section/section[2]/div[2]//article[.//h3]\"\n",
    "\n",
    "    wrap = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, FEATURED_WRAP_X)))\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", wrap)\n",
    "    time.sleep(0.8)\n",
    "\n",
    "    # --- slow scroll phase (added) ---\n",
    "    for _ in range(6):\n",
    "        slow_scroll_page(driver, max_loops=40, pause=0.9)\n",
    "        # if site uses infinite list inside this section, ensure it's in view\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", wrap)\n",
    "        time.sleep(0.4)\n",
    "\n",
    "    for ev in driver.find_elements(By.XPATH, FEATURED_CARD_X):\n",
    "        try: title = ev.find_element(By.XPATH, \".//h3/span\").text.strip()\n",
    "        except: title = \"No Title\"\n",
    "        try: date = ev.find_element(By.XPATH, \".//div[1]/div[1]/span\").text.strip()\n",
    "        except: date = \"No Date\"\n",
    "        try: location = ev.find_element(By.XPATH, \".//div[1]/div[3]/span\").text.strip()\n",
    "        except: location = \"Singapore\"\n",
    "        try: link = ev.find_element(By.XPATH, \".//ancestor::a\").get_attribute(\"href\") or \"No Link\"\n",
    "        except: link = \"No Link\"\n",
    "        all_rows.append({\"Title\": title, \"Date\": date, \"Location\": location, \"Link\": link})\n",
    "\n",
    "    print(f\"üü¢ Megatix (Featured): grabbed {len(driver.find_elements(By.XPATH, FEATURED_CARD_X))} events\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Megatix error: {e}\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# -------------------- FINAL OUTPUT (append-only, no duplicates across runs) --------------------\n",
    "OUTPUT_CSV = \"all_singapore.csv\"\n",
    "\n",
    "# Add scrape timestamp\n",
    "scraped_at = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
    "for row in all_rows:\n",
    "    row[\"scraped_at_utc\"] = scraped_at\n",
    "\n",
    "df_all = pd.DataFrame(all_rows, columns=[\"Title\", \"Date\", \"Location\", \"Link\", \"scraped_at_utc\"]).reset_index(drop=True)\n",
    "\n",
    "# Clean up text fields\n",
    "df_all[\"Link\"] = df_all[\"Link\"].fillna(\"\").str.strip()\n",
    "df_all[\"Title\"] = df_all[\"Title\"].fillna(\"\").str.strip()\n",
    "df_all[\"Date\"] = df_all[\"Date\"].fillna(\"\").str.strip()\n",
    "df_all[\"Location\"] = df_all[\"Location\"].fillna(\"\").str.strip()\n",
    "\n",
    "# Load previous CSV if exists\n",
    "try:\n",
    "    df_existing = pd.read_csv(OUTPUT_CSV)\n",
    "    if \"Link\" not in df_existing.columns:\n",
    "        for col in [\"Title\", \"Date\", \"Location\", \"Link\", \"scraped_at_utc\"]:\n",
    "            if col not in df_existing.columns:\n",
    "                df_existing[col] = \"\"\n",
    "        df_existing = df_existing[[\"Title\", \"Date\", \"Location\", \"Link\", \"scraped_at_utc\"]]\n",
    "except FileNotFoundError:\n",
    "    df_existing = pd.DataFrame(columns=[\"Title\", \"Date\", \"Location\", \"Link\", \"scraped_at_utc\"])\n",
    "\n",
    "existing_links = set(df_existing[\"Link\"].astype(str).tolist())\n",
    "new_rows = df_all[~df_all[\"Link\"].astype(str).isin(existing_links)]\n",
    "\n",
    "df_out = pd.concat([df_existing, new_rows], ignore_index=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Newly scraped this run: {len(df_all)}\")\n",
    "print(f\"‚ûï New (not seen before): {len(new_rows)}\")\n",
    "print(f\"üì¶ Total in master after save: {len(df_out)}\")\n",
    "\n",
    "df_out.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\nüíæ CSV saved: {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542bed64",
   "metadata": {},
   "source": [
    "# Singapore (Venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6127ec71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 2 events.\n",
      "Saved 42 rows to all_singapore_venues.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "URL = \"https://www.singaporeexpo.com.sg/what-s-on/events-expo?utm_source=google_pmax_sitelink&utm_medium=social_ads&utm_campaign=eventpackages-2526&utm_content=cny2026_general&gad_source=1&gad_campaignid=23175866147&gbraid=0AAAABBnFXuJ1_a_rRdCD1OzIMMiCFAgK_&gclid=CjwKCAiA_dDIBhB6EiwAvzc1cI6Q8DekR41jwb9VBhRaHME2P3lrb1gre2Apdfbq5tl-9aU5F7dkFRoCW8AQAvD_BwE\"\n",
    "CSV_PATH = \"all_singapore_venues.csv\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# UTILITIES\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def get_utc_timestamp():\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S%z\")\n",
    "\n",
    "\n",
    "def setup_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    # options.add_argument(\"--headless=new\")  # enable if you want headless\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "\n",
    "def wait_for_page_loaded(driver, timeout=30):\n",
    "    WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_element_located((By.ID, \"eventListPage\"))\n",
    "    )\n",
    "\n",
    "\n",
    "def lazy_scroll(driver, pause=2, max_loops=20):\n",
    "    \"\"\"Scroll until no more lazy-loading occurs.\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    for _ in range(max_loops):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(pause)\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# SCRAPER\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def scrape_events(driver):\n",
    "    driver.get(URL)\n",
    "    wait_for_page_loaded(driver)\n",
    "    lazy_scroll(driver)\n",
    "\n",
    "    # event section (outer container)\n",
    "    try:\n",
    "        section_el = driver.find_element(By.XPATH, \"//*[@id='eventListPage']/div[2]\")\n",
    "        section_text = section_el.text.strip()\n",
    "    except:\n",
    "        section_text = \"\"\n",
    "\n",
    "    # all event card LI items\n",
    "    items = driver.find_elements(By.XPATH, \"//*[@id='eventListPage']/div[2]/ul/li\")\n",
    "\n",
    "    scraped_at = get_utc_timestamp()\n",
    "    rows = []\n",
    "\n",
    "    for li in items:\n",
    "        try:\n",
    "            name_el = li.find_element(By.XPATH, \".//div/a/div[2]/div[3]/h2\")\n",
    "            event_name = name_el.text.strip()\n",
    "        except:\n",
    "            event_name = \"\"\n",
    "\n",
    "        try:\n",
    "            dl_el = li.find_element(By.XPATH, \".//div/a/div[2]/div[1]\")\n",
    "            dl_text = dl_el.text.strip()\n",
    "            parts = [x.strip() for x in dl_text.split(\"\\n\") if x.strip()]\n",
    "            event_date = parts[0] if len(parts) > 0 else \"\"\n",
    "            event_location = parts[1] if len(parts) > 1 else \"\"\n",
    "        except:\n",
    "            event_date = \"\"\n",
    "            event_location = \"\"\n",
    "\n",
    "        try:\n",
    "            link_el = li.find_element(By.XPATH, \".//div/a\")\n",
    "            event_link = link_el.get_attribute(\"href\") or \"\"\n",
    "        except:\n",
    "            event_link = \"\"\n",
    "\n",
    "        if not event_name:\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"event_name\": event_name,\n",
    "            \"event_date\": event_date,\n",
    "            \"event_location\": event_location,\n",
    "            \"event_section\": section_text,\n",
    "            \"event_link\": event_link,\n",
    "            \"scraped_at_utc\": scraped_at\n",
    "        })\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# MERGING / SAVING (NO ROW REMOVAL)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def build_uid(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Stable unique key for dedupe.\"\"\"\n",
    "    return (\n",
    "        df.get(\"event_name\", \"\").fillna(\"\") + \" | \" +\n",
    "        df.get(\"event_date\", \"\").fillna(\"\") + \" | \" +\n",
    "        df.get(\"event_location\", \"\").fillna(\"\") + \" | \" +\n",
    "        df.get(\"event_link\", \"\").fillna(\"\")\n",
    "    )\n",
    "\n",
    "\n",
    "def save_merged_csv(new_rows, path=CSV_PATH):\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    if new_df.empty:\n",
    "        print(\"No events scraped.\")\n",
    "        return\n",
    "\n",
    "    # Build new UID column\n",
    "    new_df[\"uid\"] = build_uid(new_df)\n",
    "\n",
    "    # Case 1: CSV already exists ‚Üí merge without deleting anything\n",
    "    if os.path.exists(path):\n",
    "        existing_df = pd.read_csv(path, dtype=str)\n",
    "\n",
    "        if \"uid\" not in existing_df.columns:\n",
    "            existing_df[\"uid\"] = build_uid(existing_df)\n",
    "\n",
    "        # Combine\n",
    "        combined = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "\n",
    "        # Drop duplicates (keep existing rows)\n",
    "        combined = combined.drop_duplicates(subset=[\"uid\"], keep=\"first\")\n",
    "\n",
    "    else:\n",
    "        # First creation\n",
    "        combined = new_df\n",
    "\n",
    "    combined.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Saved {len(combined)} rows to {path}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# MAIN\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    driver = setup_driver()\n",
    "    try:\n",
    "        events = scrape_events(driver)\n",
    "        print(f\"Scraped {len(events)} events.\")\n",
    "        save_merged_csv(events)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23f7c5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 18 events from The Star.\n",
      "Saved 46 rows to all_singapore_venues.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "URL = \"https://www.thestar.sg/events\"\n",
    "CSV_PATH = \"all_singapore_venues.csv\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# UTILITIES\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def get_utc_timestamp():\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S%z\")\n",
    "\n",
    "\n",
    "def setup_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    # options.add_argument(\"--headless=new\")  # uncomment if you want headless\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def wait_for_page_loaded(driver, timeout=30):\n",
    "    # Wait for the main event section container to appear\n",
    "    WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_element_located(\n",
    "            (By.XPATH, \"/html/body/section/div/div/div[3]/div\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def lazy_scroll(driver, pause=2.0, max_loops=20):\n",
    "    \"\"\"\n",
    "    Scroll down repeatedly to trigger any lazy-loading until\n",
    "    page height stops changing or max_loops is reached.\n",
    "    \"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    for _ in range(max_loops):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(pause)\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# SCRAPER\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def scrape_events(driver):\n",
    "    driver.get(URL)\n",
    "    wait_for_page_loaded(driver)\n",
    "    lazy_scroll(driver)\n",
    "\n",
    "    # Event section container (where all the event cards live)\n",
    "    try:\n",
    "        section_el = driver.find_element(\n",
    "            By.XPATH, \"/html/body/section/div/div/div[3]/div\"\n",
    "        )\n",
    "    except Exception:\n",
    "        print(\"Could not find main events section.\")\n",
    "        return []\n",
    "\n",
    "    # Each event card is under that div as a direct child div\n",
    "    items = section_el.find_elements(By.XPATH, \"./div\")\n",
    "\n",
    "    scraped_at = get_utc_timestamp()\n",
    "    rows = []\n",
    "\n",
    "    for item in items:\n",
    "        # Using your reference XPaths but in relative form:\n",
    "        # /html/body/section/div/div/div[3]/div/div[1]/a/h4 ‚Üí .//a/h4\n",
    "        # /html/body/section/div/div/div[3]/div/div[1]/a/h5 ‚Üí .//a/h5\n",
    "\n",
    "        try:\n",
    "            name_el = item.find_element(By.XPATH, \".//a/h4\")\n",
    "            event_name = name_el.text.strip()\n",
    "        except Exception:\n",
    "            event_name = \"\"\n",
    "\n",
    "        try:\n",
    "            date_el = item.find_element(By.XPATH, \".//a/h5\")\n",
    "            event_date = date_el.text.strip()\n",
    "        except Exception:\n",
    "            event_date = \"\"\n",
    "\n",
    "        try:\n",
    "            link_el = item.find_element(By.XPATH, \".//a\")\n",
    "            event_link = link_el.get_attribute(\"href\") or \"\"\n",
    "        except Exception:\n",
    "            event_link = \"\"\n",
    "\n",
    "        if not event_name:\n",
    "            # Skip blank cards\n",
    "            continue\n",
    "\n",
    "        row = {\n",
    "            \"event_name\": event_name,\n",
    "            \"event_date\": event_date,\n",
    "            \"event_location\": \"The Star\",  # you can change or remove this\n",
    "            \"event_section\": \"The Star Events\",  # label for source/section\n",
    "            \"event_link\": event_link,\n",
    "            \"scraped_at_utc\": scraped_at,\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# MERGING / SAVING (NO ROW REMOVAL)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def build_uid(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Build a stable unique ID for each event based on key fields.\n",
    "    Used for dedup when merging with existing CSV.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.get(\"event_name\", \"\").fillna(\"\") + \" | \" +\n",
    "        df.get(\"event_date\", \"\").fillna(\"\") + \" | \" +\n",
    "        df.get(\"event_location\", \"\").fillna(\"\") + \" | \" +\n",
    "        df.get(\"event_link\", \"\").fillna(\"\")\n",
    "    )\n",
    "\n",
    "\n",
    "def save_merged_csv(new_rows, path=CSV_PATH):\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    if new_df.empty:\n",
    "        print(\"No events scraped.\")\n",
    "        return\n",
    "\n",
    "    # Add UID to new data\n",
    "    new_df[\"uid\"] = build_uid(new_df)\n",
    "\n",
    "    # If file exists, load and merge; otherwise, just create it\n",
    "    if os.path.exists(path):\n",
    "        existing_df = pd.read_csv(path, dtype=str)\n",
    "\n",
    "        if \"uid\" not in existing_df.columns:\n",
    "            existing_df[\"uid\"] = build_uid(existing_df)\n",
    "\n",
    "        # Combine old + new\n",
    "        combined = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "\n",
    "        # Drop duplicates based on uid (keep the original/oldest)\n",
    "        combined = combined.drop_duplicates(subset=[\"uid\"], keep=\"first\")\n",
    "    else:\n",
    "        combined = new_df\n",
    "\n",
    "    combined.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Saved {len(combined)} rows to {path}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# MAIN\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    driver = setup_driver()\n",
    "    try:\n",
    "        rows = scrape_events(driver)\n",
    "        print(f\"Scraped {len(rows)} events from The Star.\")\n",
    "        save_merged_csv(rows)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81cc3b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 2 events from Songkick Pasir Panjang Power Station.\n",
      "Saved 46 rows to all_singapore_venues.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "URL = \"https://www.songkick.com/venues/4360044-pasir-panjang-power-station\"\n",
    "CSV_PATH = \"all_singapore_venues.csv\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# UTILITIES\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def get_utc_timestamp():\n",
    "    \"\"\"Return current UTC time as a string.\"\"\"\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S%z\")\n",
    "\n",
    "\n",
    "def setup_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    # options.add_argument(\"--headless=new\")  # uncomment if you want headless mode\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def wait_for_page_loaded(driver, timeout=30):\n",
    "    \"\"\"\n",
    "    Wait until the calendar-summary section is present.\n",
    "    This is the container you referenced: //*[@id=\"calendar-summary\"]\n",
    "    \"\"\"\n",
    "    WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_element_located((By.ID, \"calendar-summary\"))\n",
    "    )\n",
    "\n",
    "\n",
    "def lazy_scroll(driver, pause=2.0, max_loops=20):\n",
    "    \"\"\"\n",
    "    Scroll down repeatedly to trigger lazy-loading until the page\n",
    "    height stops changing or max_loops is reached.\n",
    "    \"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    for _ in range(max_loops):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(pause)\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# SCRAPER\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def scrape_events(driver):\n",
    "    driver.get(URL)\n",
    "    wait_for_page_loaded(driver)\n",
    "    lazy_scroll(driver)\n",
    "\n",
    "    # Main calendar section\n",
    "    section_el = driver.find_element(By.ID, \"calendar-summary\")\n",
    "\n",
    "    # Each li under the UL is an event\n",
    "    items = section_el.find_elements(By.XPATH, \"./ul/li\")\n",
    "\n",
    "    scraped_at = get_utc_timestamp()\n",
    "    rows = []\n",
    "\n",
    "    for li in items:\n",
    "        # event name: //*[@id=\"calendar-summary\"]/ul/li[2]/p[1]/a/span/strong\n",
    "        try:\n",
    "            name_el = li.find_element(By.XPATH, \".//p[1]/a/span/strong\")\n",
    "            event_name = name_el.text.strip()\n",
    "        except Exception:\n",
    "            event_name = \"\"\n",
    "\n",
    "        # event venue: //*[@id=\"calendar-summary\"]/ul/li[2]/p[2]/span[1]/a\n",
    "        try:\n",
    "            venue_el = li.find_element(By.XPATH, \".//p[2]/span[1]/a\")\n",
    "            event_location = venue_el.text.strip()\n",
    "        except Exception:\n",
    "            # fallback: whole p[2] text if span/a not present\n",
    "            try:\n",
    "                venue_fallback = li.find_element(By.XPATH, \".//p[2]\")\n",
    "                event_location = venue_fallback.text.strip()\n",
    "            except Exception:\n",
    "                event_location = \"\"\n",
    "\n",
    "        # event date:\n",
    "        # structure on Songkick is usually \"date\" as first line of the LI text,\n",
    "        # so we safely derive it from the LI's text content.\n",
    "        try:\n",
    "            lines = [x.strip() for x in li.text.split(\"\\n\") if x.strip()]\n",
    "            event_date = lines[0] if lines else \"\"\n",
    "        except Exception:\n",
    "            event_date = \"\"\n",
    "\n",
    "        # event link (artist/event link)\n",
    "        try:\n",
    "            link_el = li.find_element(By.XPATH, \".//p[1]/a\")\n",
    "            event_link = link_el.get_attribute(\"href\") or \"\"\n",
    "        except Exception:\n",
    "            event_link = \"\"\n",
    "\n",
    "        if not event_name:\n",
    "            # skip weird/empty entries\n",
    "            continue\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"event_name\": event_name,\n",
    "                \"event_date\": event_date,\n",
    "                \"event_location\": event_location,\n",
    "                \"event_section\": \"Songkick Pasir Panjang Power Station\",\n",
    "                \"event_link\": event_link,\n",
    "                \"scraped_at_utc\": scraped_at,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# MERGING / SAVING (NO ROW REMOVAL)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def build_uid(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Build a stable unique ID for each event for deduplication.\n",
    "    Uses event_name + date + location + link.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.get(\"event_name\", \"\").fillna(\"\") + \" | \" +\n",
    "        df.get(\"event_date\", \"\").fillna(\"\") + \" | \" +\n",
    "        df.get(\"event_location\", \"\").fillna(\"\") + \" | \" +\n",
    "        df.get(\"event_link\", \"\").fillna(\"\")\n",
    "    )\n",
    "\n",
    "\n",
    "def save_merged_csv(new_rows, path=CSV_PATH):\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    if new_df.empty:\n",
    "        print(\"No events scraped.\")\n",
    "        return\n",
    "\n",
    "    new_df[\"uid\"] = build_uid(new_df)\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        existing_df = pd.read_csv(path, dtype=str)\n",
    "\n",
    "        if \"uid\" not in existing_df.columns:\n",
    "            existing_df[\"uid\"] = build_uid(existing_df)\n",
    "\n",
    "        # combine old + new\n",
    "        combined = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "\n",
    "        # drop duplicates by uid; keep the first (oldest) occurrence\n",
    "        combined = combined.drop_duplicates(subset=[\"uid\"], keep=\"first\")\n",
    "    else:\n",
    "        combined = new_df\n",
    "\n",
    "    combined.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Saved {len(combined)} rows to {path}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# MAIN\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    driver = setup_driver()\n",
    "    try:\n",
    "        rows = scrape_events(driver)\n",
    "        print(f\"Scraped {len(rows)} events from Songkick Pasir Panjang Power Station.\")\n",
    "        save_merged_csv(rows)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8569b2",
   "metadata": {},
   "source": [
    "# Kuala Lumpur (Completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eca76fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ Found 11 event cards\n",
      "‚úÖ Live Nation MY (All Events): grabbed 0 new events this run\n",
      "\n",
      "‚úÖ TOTAL Unique Events: 118\n",
      "| Title                                                                                 | Date                  | Location                                                    | Link                                                                                                                                                                                                                                                                         | TimeScraped      |\n",
      "|---------------------------------------------------------------------------------------|-----------------------|-------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------|\n",
      "| Shila Amzah & Priscilla Abby Cultural Concert                                         | 29/11/2025 19:00      | 21/11/2025 16:03                                            | Miri Indoor Stadium                                                                                                                                                                                                                                                          | nan              |\n",
      "| Yue YunPeng & Sun Yue 2025 Crosstalk World Tour in Malaysia                           | 4/12/2025 19:30       | 21/11/2025 16:03                                            | Mega Star Arena, Sungei Wang Plaza                                                                                                                                                                                                                                           | nan              |\n",
      "| MAN WITH A MISSION 'HOWLING ACROSS THE WORLD 2025 - ASIA TOUR' in Kuala Lumpur        | 12/12/2025 20:00      | 21/11/2025 16:03                                            | Zepp Kuala Lumpur                                                                                                                                                                                                                                                            | nan              |\n",
      "| DIOR KAMPUNG GIRL First Solo Concert                                                  | 13/12/2025 18:00      | 21/11/2025 16:03                                            | Arena of Stars, Resorts World Genting                                                                                                                                                                                                                                        | nan              |\n",
      "| Demon Slayer: Kimetsu no Yaiba in Concert - Kuala Lumpur                              | 27/12/2025 20:00      | 21/11/2025 16:03                                            | Mega Star Arena                                                                                                                                                                                                                                                              | nan              |\n",
      "| Firdaus 'Hello Again' World Tour ‚Äì The Last Dance ‚Äì Penang                            | 3/1/2026 20:00        | 21/11/2025 16:03                                            | Setia Spice Arena, Penang                                                                                                                                                                                                                                                    | nan              |\n",
      "| MAYDAY #5525 LIVE TOUR in Malaysia                                                    | 31/1/2026 20:00       | 21/11/2025 16:03                                            | National Stadium Bukit Jalil                                                                                                                                                                                                                                                 | nan              |\n",
      "| MAYDAY #5525 LIVE TOUR in Malaysia ‰∫îÊúàÂ§© #5525 „ÄäÂõûÂà∞ÈÇ£‰∏ÄÂ§©„ÄãÂ∑°ÂõûÊºîÂî±‰ºö - È©¨Êù•Ë•ø‰∫öÁ´ô | SAT 31 JAN            | Kuala Lumpur | National Stadium Bukit Jalil                 | https://www.livenation.my/event/mayday-5525-live-tour-in-malaysia-%E4%BA%94%E6%9C%88%E5%A4%A9-5525-%E5%9B%9E%E5%88%B0%E9%82%A3%E4%B8%80%E5%A4%A9-%E5%B7%A1%E5%9B%9E%E6%BC%94%E5%94%B1%E4%BC%9A-%E9%A9%AC%E6%9D%A5%E8%A5%BF%E4%BA%9A%E7%AB%99-kuala-lumpur-tickets-edp1635862 | nan              |\n",
      "| GIVƒíON - DEAR BELOVED THE TOUR                                                        | SAT 07 FEB            | Kuala Lumpur | Idea Live Arena                              | https://www.livenation.my/event/giv%C4%93on-dear-beloved-the-tour-kuala-lumpur-tickets-edp1636477                                                                                                                                                                            | nan              |\n",
      "| PRYVT 'BACK TO REALITY' World Tour in Kuala Lumpur                                    | TUE 10 FEB            | Kuala Lumpur | The Platform, Menara KEN TTDI                | https://www.livenation.my/event/pryvt-back-to-reality-world-tour-in-kuala-lumpur-kuala-lumpur-tickets-edp1637444                                                                                                                                                             | nan              |\n",
      "| JUNGLE CITY BEATS FEAT. WAN ISSARA                                                    | DATE 06 Dec 2025      | LOCATION Wet Deck W KL W Hotel Kuala Lumpur                 | https://myticket.asia/events/jungle-city-beats-feat-wan-issara/?occurrence=2025-12-06                                                                                                                                                                                        | 25/11/2025 11:36 |\n",
      "| NOVA WET SESSIONS FEAT MARTIN ROTH                                                    | DATE 13 Dec 2025      | LOCATION Wet Deck W KL W Hotel Kuala Lumpur                 | https://myticket.asia/events/nova-wet-sessions-feat-martin-roth/?occurrence=2025-12-13                                                                                                                                                                                       | 25/11/2025 11:36 |\n",
      "| EXISTS X SLAM SINGAPORE                                                               | DATE 25 - 29 Nov 2025 | LOCATION The Star Theatre Singapore                         | https://myticket.asia/events/exists-x-slam-singapore/?occurrence=2025-11-25                                                                                                                                                                                                  | 25/11/2025 11:38 |\n",
      "| IVE WORLD TOUR <SHOW WHAT I AM> IN KUALA LUMPUR                                       | SAT 04 APR            | Kuala Lumpur | Axiata Arena                                 | https://www.livenation.my/event/ive-world-tour-show-what-i-am-in-kuala-lumpur-kuala-lumpur-tickets-edp1640257                                                                                                                                                                | nan              |\n",
      "| Vir Das: Hey Stranger                                                                 | SAT 23 MAY            | Kuala Lumpur | Plenary Hall, Kuala Lumpur Convention Centre | https://www.livenation.my/event/vir-das-hey-stranger-kuala-lumpur-tickets-edp1640195                                                                                                                                                                                         | nan              |\n",
      "| THE SOCIAL SATURDAY SSWIM CLUB SPECIAL FEAT CEZAIRE                                   | DATE 20 Dec 2025      | LOCATION Wet Deck W KL W Hotel Kuala Lumpur                 | https://myticket.asia/events/the-social-saturday-sswim-club-special-feat-cezaire/?occurrence=2025-12-20                                                                                                                                                                      | 2/12/2025 14:42  |\n",
      "| YOURS SATYAN LIVE IN KL ONE VOICE ONE SOUL                                            | DATE 10 Jan 2026      | LOCATION Auditorium Jeffrey Cheah Subang Jaya               | https://myticket.asia/events/yours-satyan-live-in-kl-one-voice-one-soul/?occurrence=2026-01-10                                                                                                                                                                               | 2/12/2025 14:43  |\n",
      "| SERUMPUN MALAYSIA‚ÄôS WOVEN PATHS                                                       | DATE 03 Dec 2025      | LOCATION Tong Shin Terrace Kuala Lumpur                     | https://myticket.asia/events/serumpun-malaysias-woven-paths/?occurrence=2025-12-03&time=1764763200                                                                                                                                                                           | 2/12/2025 14:44  |\n",
      "| ELECTRIFY THE SKY WHITE EDITION 2.0                                                   | DATE 31 Dec 2025      | LOCATION Wet Deck W KL W Hotel Kuala Lumpur                 | https://myticket.asia/events/electrify-the-sky-white-edition-2-0/?occurrence=2025-12-31                                                                                                                                                                                      | 8/12/2025 10:17  |\n",
      "| SERUMPUN MALAYSIA‚ÄôS WOVEN PATHS                                                       | DATE 08 Dec 2025      | LOCATION Tong Shin Terrace Kuala Lumpur                     | https://myticket.asia/events/serumpun-malaysias-woven-paths/?occurrence=2025-12-08&time=1765195200                                                                                                                                                                           | 8/12/2025 10:18  |\n",
      "\n",
      "üíæ CSV saved: all_events_KL.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Live Nation Malaysia: All Events Scraper (deduplicated) ---\n",
    "# URL: https://www.livenation.my/event/allevents\n",
    "# Output CSV: livenation_my_all_events.csv\n",
    "# Columns: Title, Date, Location, Link\n",
    "# pip install selenium tabulate pandas\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "OUTPUT_CSV = \"all_events_KL.csv\"\n",
    "URL = \"https://www.livenation.my/event/allevents\"\n",
    "\n",
    "# Load existing CSV (if any) to prevent duplicates\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    try:\n",
    "        existing_df = pd.read_csv(OUTPUT_CSV)\n",
    "        seen_links = set(existing_df[\"Link\"].dropna().astype(str))\n",
    "    except Exception:\n",
    "        existing_df = pd.DataFrame(columns=[\"Title\", \"Date\", \"Location\", \"Link\"])\n",
    "        seen_links = set()\n",
    "else:\n",
    "    existing_df = pd.DataFrame(columns=[\"Title\", \"Date\", \"Location\", \"Link\"])\n",
    "    seen_links = set()\n",
    "\n",
    "# ---------------- DRIVER SETUP ----------------\n",
    "opts = webdriver.ChromeOptions()\n",
    "opts.add_argument(\"--start-maximized\")\n",
    "opts.add_argument(\"--disable-notifications\")\n",
    "opts.add_argument(\"--disable-popup-blocking\")\n",
    "driver = webdriver.Chrome(options=opts)\n",
    "driver.set_window_size(1400, 1000)\n",
    "all_rows = []\n",
    "\n",
    "def accept_cookies(drv):\n",
    "    for xp in [\n",
    "        \"//button[contains(., 'Accept All')]\",\n",
    "        \"//button[contains(., 'Accept all')]\",\n",
    "        \"//button[contains(., 'I Accept')]\",\n",
    "        \"//button[contains(., 'Agree')]\",\n",
    "        \"//button[contains(., 'OK')]\",\n",
    "        \"//*[@id='onetrust-accept-btn-handler']\",\n",
    "    ]:\n",
    "        try:\n",
    "            btn = WebDriverWait(drv, 3).until(EC.element_to_be_clickable((By.XPATH, xp)))\n",
    "            drv.execute_script(\"arguments[0].click();\", btn)\n",
    "            time.sleep(0.2)\n",
    "            break\n",
    "        except TimeoutException:\n",
    "            continue\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    # Collapse whitespace/newlines and trim\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
    "\n",
    "# ---------------- SCRAPER ----------------\n",
    "try:\n",
    "    driver.get(URL)\n",
    "    WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main']\")))\n",
    "    time.sleep(1.0)\n",
    "    accept_cookies(driver)\n",
    "\n",
    "    # Each LI that has an event link and the date container\n",
    "    ALL_CARDS_X = \"//*[@id='main']//ul//li[.//a[contains(@href,'/event/')]]\"\n",
    "\n",
    "    # Scroll to load everything (and click Load/Show more if present)\n",
    "    prev = -1\n",
    "    stable = 0\n",
    "    for _ in range(80):\n",
    "        cards = driver.find_elements(By.XPATH, ALL_CARDS_X)\n",
    "        count = len(cards)\n",
    "        if count == prev:\n",
    "            stable += 1\n",
    "            # Try \"Load/Show more\" if present\n",
    "            try:\n",
    "                more = driver.find_element(By.XPATH, \"//button[contains(.,'Load more') or contains(.,'Show more')]\")\n",
    "                if more.is_enabled() and more.is_displayed():\n",
    "                    driver.execute_script(\"arguments[0].click();\", more)\n",
    "                    time.sleep(1.2)\n",
    "                    cards = driver.find_elements(By.XPATH, ALL_CARDS_X)\n",
    "                    count = len(cards)\n",
    "                    if count > prev:\n",
    "                        stable = 0\n",
    "            except Exception:\n",
    "                pass\n",
    "            if stable >= 2:\n",
    "                break\n",
    "        else:\n",
    "            stable = 0\n",
    "        prev = count\n",
    "        driver.execute_script(\"window.scrollBy(0, 1200);\")\n",
    "        time.sleep(1.1)\n",
    "\n",
    "    cards = driver.find_elements(By.XPATH, ALL_CARDS_X)\n",
    "    print(f\"üü¢ Found {len(cards)} event cards\")\n",
    "\n",
    "    for li in cards:\n",
    "        try:\n",
    "            # Link (primary dedup key)\n",
    "            try:\n",
    "                link = li.find_element(By.XPATH, \".//a[contains(@href,'/event/')]\").get_attribute(\"href\") or \"\"\n",
    "            except Exception:\n",
    "                link = \"\"\n",
    "            if not link or link in seen_links:\n",
    "                continue\n",
    "\n",
    "            # Title (your path first)\n",
    "            title = \"\"\n",
    "            for xp in [\n",
    "                \".//span/div/div/p[1]\",   # close to //*[@id='1616419']/span/div/div/p[1]\n",
    "                \".//p[1]\",\n",
    "                \".//div//p[1]\"\n",
    "            ]:\n",
    "                try:\n",
    "                    title = clean_text(li.find_element(By.XPATH, xp).text)\n",
    "                    if title:\n",
    "                        break\n",
    "                except Exception:\n",
    "                    continue\n",
    "            if not title:\n",
    "                title = \"No Title\"\n",
    "\n",
    "            # Location (your path first)\n",
    "            location = \"\"\n",
    "            for xp in [\n",
    "                \".//span/div/div/p[3]/span\",  # close to //*[@id='1616419']/span/div/div/p[3]/span\n",
    "                \".//p[3]/span\",\n",
    "                \".//p[3]\"\n",
    "            ]:\n",
    "                try:\n",
    "                    location = clean_text(li.find_element(By.XPATH, xp).text)\n",
    "                    if location:\n",
    "                        break\n",
    "                except Exception:\n",
    "                    continue\n",
    "            if not location:\n",
    "                location = \"Malaysia\"\n",
    "\n",
    "            # Date ‚Äî USE YOUR DIV CONTAINER XPATH RELATIVELY, then fall back to <time>\n",
    "            # Provided absolute: //*[@id=\"main\"]/div[2]/div[1]/div/div/div/div[2]/ul[1]/li[1]/div/div[1]\n",
    "            # Relative to each li, this corresponds to: .//div/div[1]\n",
    "            date_text = \"\"\n",
    "            for xp in [\n",
    "                \".//div/div[1]\",          # your container (first choice)\n",
    "                \".//time\",                # fallback: visible text\n",
    "            ]:\n",
    "                try:\n",
    "                    node = li.find_element(By.XPATH, xp)\n",
    "                    # prefer node.text, otherwise datetime attribute if it's <time>\n",
    "                    date_text = clean_text(node.text or node.get_attribute(\"datetime\") or \"\")\n",
    "                    if date_text:\n",
    "                        break\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            all_rows.append({\n",
    "                \"Title\": title,\n",
    "                \"Date\": date_text,\n",
    "                \"Location\": location,\n",
    "                \"Link\": link\n",
    "            })\n",
    "            seen_links.add(link)\n",
    "\n",
    "        except StaleElementReferenceException:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Skipped one card due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"‚úÖ Live Nation MY (All Events): grabbed {len(all_rows)} new events this run\")\n",
    "\n",
    "except TimeoutException:\n",
    "    print(\"‚ö† Timeout waiting for Live Nation MY page\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "# ---------------- OUTPUT ----------------\n",
    "if all_rows:\n",
    "    new_df = pd.DataFrame(all_rows, columns=[\"Title\", \"Date\", \"Location\", \"Link\"])\n",
    "    final_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "else:\n",
    "    final_df = existing_df.copy()\n",
    "\n",
    "# Final safety dedup\n",
    "if not final_df.empty:\n",
    "    final_df[\"Link\"] = final_df[\"Link\"].astype(str).str.strip()\n",
    "    final_df = final_df.drop_duplicates(subset=[\"Link\"], keep=\"first\")\n",
    "    final_df = final_df.drop_duplicates(subset=[\"Title\", \"Date\", \"Location\"], keep=\"first\")\n",
    "\n",
    "print(f\"\\n‚úÖ TOTAL Unique Events: {len(final_df)}\")\n",
    "print(tabulate(final_df.tail(20), headers=\"keys\", tablefmt=\"github\", showindex=False))\n",
    "\n",
    "final_df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\nüíæ CSV saved: {OUTPUT_CSV}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6435c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# BookMyShow MY ‚Äî Kuala Lumpur scraper (visible tabs; NaN-safe merge; 4-column CSV)\n",
    "# - Stable scrape with slower listing reading\n",
    "# - Safe merge that handles NaN, floats, blanks gracefully\n",
    "# - CSV schema: Title | Date | Location | Link\n",
    "# \"\"\"\n",
    "\n",
    "# import os\n",
    "# import time\n",
    "# import sys\n",
    "# import traceback\n",
    "# import hashlib\n",
    "# from typing import Optional, List, Dict\n",
    "\n",
    "# import pandas as pd\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.common.exceptions import (\n",
    "#     TimeoutException,\n",
    "#     NoSuchElementException,\n",
    "#     StaleElementReferenceException,\n",
    "#     WebDriverException,\n",
    "# )\n",
    "\n",
    "# # ===================== CONFIG =====================\n",
    "# LIST_URL   = \"https://my.bookmyshow.com/en/collections/e?cities=Kuala+Lumpur___Kuala+Lumpur\"\n",
    "# OUTPUT_CSV = \"all_events_KL.csv\"\n",
    "\n",
    "# HEADLESS          = False\n",
    "# OPEN_IN_NEW_TAB   = True\n",
    "# PAUSE_ON_EACH_TAB = 0.8\n",
    "# TAKE_ERROR_SHOTS  = True\n",
    "\n",
    "# SLOW_MODE               = True\n",
    "# LISTING_INITIAL_PAUSE   = 1.2\n",
    "# PER_CARD_SCROLL_SETTLE  = 0.35\n",
    "# PER_CARD_READ_RETRIES   = 3\n",
    "# PER_CARD_READ_SLEEP     = 0.25\n",
    "# BETWEEN_CARD_PAUSE      = 0.1\n",
    "\n",
    "# WAIT_SEC          = 20\n",
    "# PAGE_LOAD_TIMEOUT = 25\n",
    "\n",
    "# X_LIST_CONTAINER = \"/html/body/div[1]/main/div/div/div[2]\"\n",
    "# X_CARD_ANCHOR    = X_LIST_CONTAINER + \"/a[{i}]\"\n",
    "# X_NAME           = \"/html/body/div[1]/main/div/div/div[2]/a[{i}]/div/div[2]/div/div/div[1]\"\n",
    "# X_DATE           = \"/html/body/div[1]/main/div/div/div[2]/a[{i}]/div/div[2]/div/div/div[2]/span\"\n",
    "# X_VENUE_DETAIL   = \"/html/body/div[1]/main/div/div/div[2]/div[1]/div[3]/div[4]/div[1]\"\n",
    "\n",
    "# # ===================== DRIVER =====================\n",
    "# def build_driver(headless: bool = HEADLESS) -> webdriver.Chrome:\n",
    "#     ua = (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "#           \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\")\n",
    "#     opts = webdriver.ChromeOptions()\n",
    "#     opts.add_argument(f\"user-agent={ua}\")\n",
    "#     if headless:\n",
    "#         opts.add_argument(\"--headless=new\")\n",
    "#     opts.add_argument(\"--window-size=1400,1000\")\n",
    "#     opts.add_argument(\"--disable-notifications\")\n",
    "#     opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "#     opts.page_load_strategy = \"eager\"\n",
    "\n",
    "#     driver = webdriver.Chrome(options=opts)\n",
    "#     driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)\n",
    "#     driver.implicitly_wait(2)\n",
    "#     return driver\n",
    "\n",
    "# # ===================== SAFE STRING HELPERS =====================\n",
    "# def _s(x) -> str:\n",
    "#     \"\"\"Convert any NaN, None, or float to safe string.\"\"\"\n",
    "#     if x is None:\n",
    "#         return \"\"\n",
    "#     try:\n",
    "#         if isinstance(x, float) and pd.isna(x):\n",
    "#             return \"\"\n",
    "#     except Exception:\n",
    "#         pass\n",
    "#     return str(x)\n",
    "\n",
    "# def normalize_link(u) -> str:\n",
    "#     u = _s(u).strip().lower()\n",
    "#     return u.rstrip(\"/\")\n",
    "\n",
    "# def stable_event_id(title, date_str, location, link_norm) -> str:\n",
    "#     link_norm = _s(link_norm).strip().lower()\n",
    "#     if link_norm:\n",
    "#         return link_norm\n",
    "#     key = \"|\".join([\n",
    "#         _s(title).strip().lower(),\n",
    "#         _s(date_str).strip().lower(),\n",
    "#         _s(location).strip().lower(),\n",
    "#     ])\n",
    "#     return hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# # ===================== HELPERS =====================\n",
    "# def wait_present(driver, xpath, timeout=WAIT_SEC):\n",
    "#     return WebDriverWait(driver, timeout).until(\n",
    "#         EC.presence_of_element_located((By.XPATH, xpath))\n",
    "#     )\n",
    "\n",
    "# def scroll_lazy(driver, rounds=10, pause=0.5):\n",
    "#     last_h = 0\n",
    "#     for _ in range(rounds):\n",
    "#         driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#         time.sleep(pause)\n",
    "#         h = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "#         if h == last_h:\n",
    "#             break\n",
    "#         last_h = h\n",
    "\n",
    "# def get_text_abs(driver, xpath) -> str:\n",
    "#     try:\n",
    "#         return driver.find_element(By.XPATH, xpath).text.strip()\n",
    "#     except NoSuchElementException:\n",
    "#         return \"\"\n",
    "#     except StaleElementReferenceException:\n",
    "#         try:\n",
    "#             return driver.find_element(By.XPATH, xpath).text.strip()\n",
    "#         except Exception:\n",
    "#             return \"\"\n",
    "\n",
    "# def read_text_nonempty(driver, xpath: str, retries: int, sleep_s: float) -> str:\n",
    "#     txt = get_text_abs(driver, xpath)\n",
    "#     if txt:\n",
    "#         return txt\n",
    "#     for _ in range(retries):\n",
    "#         time.sleep(sleep_s)\n",
    "#         txt = get_text_abs(driver, xpath)\n",
    "#         if txt:\n",
    "#             break\n",
    "#     return txt\n",
    "\n",
    "# def ensure_dir(path: str):\n",
    "#     os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# def safe_screenshot(driver, path: str):\n",
    "#     try:\n",
    "#         driver.save_screenshot(path)\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "# def robust_get(driver, url: str, tries: int = 2, settle: float = 0.4):\n",
    "#     last_err: Optional[Exception] = None\n",
    "#     for _ in range(tries):\n",
    "#         try:\n",
    "#             driver.get(url)\n",
    "#             time.sleep(settle)\n",
    "#             return\n",
    "#         except Exception as e:\n",
    "#             last_err = e\n",
    "#             time.sleep(0.7)\n",
    "#     if last_err:\n",
    "#         raise last_err\n",
    "\n",
    "# def open_in_new_tab(driver, url: str):\n",
    "#     current = driver.current_window_handle\n",
    "#     driver.execute_script(\"window.open(arguments[0], '_blank');\", url)\n",
    "#     WebDriverWait(driver, 10).until(lambda d: len(d.window_handles) > 1)\n",
    "#     new_handle = [h for h in driver.window_handles if h != current][-1]\n",
    "#     driver.switch_to.window(new_handle)\n",
    "#     return current, new_handle\n",
    "\n",
    "# def close_tab_and_return(driver, original_handle: str):\n",
    "#     try:\n",
    "#         driver.close()\n",
    "#     finally:\n",
    "#         driver.switch_to.window(original_handle)\n",
    "\n",
    "# # ===================== MERGE (NaN-SAFE) =====================\n",
    "# def merge_with_existing(df_new: pd.DataFrame, path: str) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Merge with old CSV (if exists):\n",
    "#     - Keeps old rows (no removals)\n",
    "#     - No duplicates\n",
    "#     - Overwrites same events with latest scrape\n",
    "#     - NaN-safe\n",
    "#     \"\"\"\n",
    "#     df_new = df_new.copy()\n",
    "#     for col in [\"Title\",\"Date\",\"Location\",\"Link\"]:\n",
    "#         if col not in df_new.columns:\n",
    "#             df_new[col] = \"\"\n",
    "#     df_new[[\"Title\",\"Date\",\"Location\",\"Link\"]] = df_new[[\"Title\",\"Date\",\"Location\",\"Link\"]].fillna(\"\")\n",
    "#     df_new[\"LinkNorm\"] = df_new[\"Link\"].apply(normalize_link)\n",
    "#     df_new[\"EventID\"]  = df_new.apply(\n",
    "#         lambda r: stable_event_id(r.get(\"Title\",\"\"), r.get(\"Date\",\"\"),\n",
    "#                                   r.get(\"Location\",\"\"), r.get(\"LinkNorm\",\"\")), axis=1\n",
    "#     )\n",
    "\n",
    "#     try:\n",
    "#         df_old = pd.read_csv(path)\n",
    "#     except FileNotFoundError:\n",
    "#         return df_new[[\"Title\",\"Date\",\"Location\",\"Link\"]]\n",
    "\n",
    "#     df_old = df_old.copy()\n",
    "#     for col in [\"Title\",\"Date\",\"Location\",\"Link\"]:\n",
    "#         if col not in df_old.columns:\n",
    "#             df_old[col] = \"\"\n",
    "#     df_old[[\"Title\",\"Date\",\"Location\",\"Link\"]] = df_old[[\"Title\",\"Date\",\"Location\",\"Link\"]].fillna(\"\")\n",
    "#     df_old[\"LinkNorm\"] = df_old[\"Link\"].apply(normalize_link)\n",
    "#     df_old[\"EventID\"]  = df_old.apply(\n",
    "#         lambda r: stable_event_id(r.get(\"Title\",\"\"), r.get(\"Date\",\"\"),\n",
    "#                                   r.get(\"Location\",\"\"), r.get(\"LinkNorm\",\"\")), axis=1\n",
    "#     )\n",
    "\n",
    "#     merged = pd.concat([df_old, df_new], ignore_index=True)\n",
    "#     merged = merged.drop_duplicates(subset=[\"EventID\"], keep=\"last\")\n",
    "#     merged = merged.drop_duplicates(subset=[\"LinkNorm\"], keep=\"last\")\n",
    "#     return merged[[\"Title\",\"Date\",\"Location\",\"Link\"]]\n",
    "\n",
    "# # ===================== CORE SCRAPE =====================\n",
    "# def collect_listing_cards(driver) -> List[Dict[str, str]]:\n",
    "#     print(f\"Opening listing: {LIST_URL}\")\n",
    "#     robust_get(driver, LIST_URL, tries=3, settle=0.6)\n",
    "#     wait_present(driver, X_LIST_CONTAINER, timeout=WAIT_SEC)\n",
    "#     scroll_lazy(driver, rounds=10, pause=0.5)\n",
    "#     if SLOW_MODE:\n",
    "#         time.sleep(LISTING_INITIAL_PAUSE)\n",
    "\n",
    "#     anchors = driver.find_elements(By.XPATH, X_LIST_CONTAINER + \"/a\")\n",
    "#     n = len(anchors)\n",
    "#     print(f\"Found {n} cards in listing.\")\n",
    "#     cards = []\n",
    "#     for i in range(1, n + 1):\n",
    "#         name_x = X_NAME.format(i=i)\n",
    "#         date_x = X_DATE.format(i=i)\n",
    "#         a_x    = X_CARD_ANCHOR.format(i=i)\n",
    "\n",
    "#         try:\n",
    "#             a_el = driver.find_element(By.XPATH, a_x)\n",
    "#             driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", a_el)\n",
    "#         except Exception:\n",
    "#             pass\n",
    "#         time.sleep(PER_CARD_SCROLL_SETTLE)\n",
    "\n",
    "#         title = read_text_nonempty(driver, name_x, PER_CARD_READ_RETRIES, PER_CARD_READ_SLEEP)\n",
    "#         date  = read_text_nonempty(driver, date_x, PER_CARD_READ_RETRIES, PER_CARD_READ_SLEEP)\n",
    "\n",
    "#         href = \"\"\n",
    "#         try:\n",
    "#             a_el = driver.find_element(By.XPATH, a_x)\n",
    "#             href = (a_el.get_attribute(\"href\") or \"\").strip()\n",
    "#         except Exception:\n",
    "#             pass\n",
    "\n",
    "#         cards.append({\"i\": i, \"title\": title, \"date\": date, \"href\": href})\n",
    "#         time.sleep(BETWEEN_CARD_PAUSE)\n",
    "#     return cards\n",
    "\n",
    "# def scrape_detail_for_location(driver, url: str, idx: int) -> str:\n",
    "#     if not url:\n",
    "#         return \"\"\n",
    "#     try:\n",
    "#         if OPEN_IN_NEW_TAB:\n",
    "#             print(f\"  ‚Üí Opening tab #{idx}: {url}\")\n",
    "#             original = driver.current_window_handle\n",
    "#             driver.execute_script(\"window.open(arguments[0], '_blank');\", url)\n",
    "#             WebDriverWait(driver, 10).until(lambda d: len(d.window_handles) > 1)\n",
    "#             new_handle = [h for h in driver.window_handles if h != original][-1]\n",
    "#             driver.switch_to.window(new_handle)\n",
    "#         else:\n",
    "#             original = driver.current_window_handle\n",
    "#             driver.get(url)\n",
    "\n",
    "#         time.sleep(PAUSE_ON_EACH_TAB)\n",
    "\n",
    "#         try:\n",
    "#             WebDriverWait(driver, WAIT_SEC).until(\n",
    "#                 EC.presence_of_element_located((By.XPATH, X_VENUE_DETAIL))\n",
    "#             )\n",
    "#             location = driver.find_element(By.XPATH, X_VENUE_DETAIL).text.strip()\n",
    "#             print(f\"    ‚úì Location scraped: {location[:60]}\")\n",
    "#         except TimeoutException:\n",
    "#             location = \"\"\n",
    "#             print(\"    ‚ö† Location not found (leaving blank).\")\n",
    "\n",
    "#         if OPEN_IN_NEW_TAB:\n",
    "#             driver.close()\n",
    "#             driver.switch_to.window(original)\n",
    "#         return location\n",
    "#     except Exception as e:\n",
    "#         print(f\"    ‚úó Error on detail page: {type(e).__name__}: {e}\")\n",
    "#         if TAKE_ERROR_SHOTS:\n",
    "#             ensure_dir(\"debug_screens\")\n",
    "#             safe_screenshot(driver, f\"debug_screens/error_event_{idx}.png\")\n",
    "#         try:\n",
    "#             if OPEN_IN_NEW_TAB:\n",
    "#                 driver.close()\n",
    "#                 driver.switch_to.window(original)\n",
    "#         except Exception:\n",
    "#             pass\n",
    "#         return \"\"\n",
    "\n",
    "# def scrape_bms_kl(driver: webdriver.Chrome) -> pd.DataFrame:\n",
    "#     cards = collect_listing_cards(driver)\n",
    "#     rows = []\n",
    "#     for idx, c in enumerate(cards, start=1):\n",
    "#         title = c[\"title\"]\n",
    "#         date  = c[\"date\"]\n",
    "#         href  = c[\"href\"]\n",
    "#         print(f\"\\n[{idx}/{len(cards)}] {title} | {date}\")\n",
    "#         location = scrape_detail_for_location(driver, href, idx) if href else \"\"\n",
    "#         rows.append({\"Title\": title, \"Date\": date, \"Location\": location, \"Link\": href})\n",
    "#     return pd.DataFrame(rows, columns=[\"Title\",\"Date\",\"Location\",\"Link\"])\n",
    "\n",
    "# # ===================== MAIN =====================\n",
    "# def main(headless=HEADLESS):\n",
    "#     driver = build_driver(headless=headless)\n",
    "#     try:\n",
    "#         df_new = scrape_bms_kl(driver)\n",
    "#     finally:\n",
    "#         try:\n",
    "#             driver.quit()\n",
    "#         except Exception:\n",
    "#             pass\n",
    "#     if df_new.empty:\n",
    "#         print(\"No events found.\")\n",
    "#         return\n",
    "#     out = merge_with_existing(df_new, OUTPUT_CSV)\n",
    "#     out.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "#     print(f\"\\nSaved {len(out)} total rows -> {OUTPUT_CSV}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         main(headless=HEADLESS)\n",
    "#     except Exception as e:\n",
    "#         print(f\"[fatal] {type(e).__name__}: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         sys.exit(1)\n",
    "\n",
    "\n",
    "## has issues scraping due to limits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ad3f1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üÜï 2 new event(s) added (120 total now in file).\n",
      "| Title                                                                 | Date                  | Location                                        | Link                                                                                                                    | TimeScraped         |\n",
      "|-----------------------------------------------------------------------|-----------------------|-------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|---------------------|\n",
      "| A TRIBUTE TO SHARIFAH AINI                                            | DATE 06 Jun 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/a-tribute-to-sharifah-aini/?occurrence=2026-06-06                                          | 2025-12-15 16:23:24 |\n",
      "| JAZZ, CLASSICAL AND BEYOND WITH THE MPO                               | DATE 13 Jun 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/jazz-classical-and-beyond-with-the-mpo/?occurrence=2026-06-13                              | 2025-12-15 16:23:30 |\n",
      "| ARCTIC WINDS AND SUMMER SUN                                           | DATE 20 Jun 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/arctic-winds-and-summer-sun/?occurrence=2026-06-20                                         | 2025-12-15 16:23:35 |\n",
      "| METROPOLITAN RHYTHMS: THE EMIGR√â AND THE AMERICAN                     | DATE 18 Jul 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/metropolitan-rhythms-the-emigre-and-the-american/?occurrence=2026-07-18                    | 2025-12-15 16:23:41 |\n",
      "| WORKSHOP ‚Äì MAKYONG SHAKESPEARE‚ÄôS THE COMEDY OF ERRORS (AN ADAPTATION) | DATE 25 Jul 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/workshop-makyong-shakespeares-the-comedy-of-errors-an-adaptation/?occurrence=2026-07-25    | 2025-12-15 16:23:46 |\n",
      "| MAK YONG SHAKESPEARE: THE COMEDY OF ERRORS ‚Äì AN ADAPTATION            | DATE 25 Jul 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/mak-yong-shakespeare-the-comedy-of-errors-an-adaptation/?occurrence=2026-07-25             | 2025-12-15 16:23:51 |\n",
      "| A HEART UNVEILED: THE MUSIC OF TCHAIKOVSKY                            | DATE 22 Aug 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/a-heart-unveiled-the-music-of-tchaikovsky/?occurrence=2026-08-22                           | 2025-12-15 16:23:57 |\n",
      "| A KNIGHT‚ÄôS TALE: VALOR AND ROMANCE IN MUSIC                           | DATE 29 Aug 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/a-knights-tale-valor-and-romance-in-music/?occurrence=2026-08-29                           | 2025-12-15 16:24:02 |\n",
      "| A TRIBUTE TO ALFONSO SOLIANO                                          | DATE 02 Sep 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/a-tribute-to-alfonso-soliano/?occurrence=2026-09-02                                        | 2025-12-15 16:24:08 |\n",
      "| YIN AND YANG: A DANCE KALEIDOSCOPE                                    | DATE 05 Sep 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/yin-and-yang-a-dance-kaleidoscope/?occurrence=2026-09-05                                   | 2025-12-15 16:24:14 |\n",
      "| WORKSHOP ‚Äì YIN AND YANG: A DANCE KALEIDOSCOPE                         | DATE 06 Sep 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/workshop-yin-and-yang-a-dance-kaleidoscope/?occurrence=2026-09-06                          | 2025-12-15 16:24:20 |\n",
      "| MAURICE STEGER‚ÄôS NATURE CONCERTI                                      | DATE 12 Sep 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/maurice-stegers-nature-concerti/?occurrence=2026-09-12                                     | 2025-12-15 16:24:26 |\n",
      "| THREE BY THREE                                                        | DATE 03 Oct 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/three-by-three/?occurrence=2026-10-03                                                      | 2025-12-15 16:24:32 |\n",
      "| THE MUSIC OF QUEEN‚Ä¶LIVES ON!                                          | DATE 10 Oct 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/the-music-of-queen-lives-on/?occurrence=2026-10-10                                         | 2025-12-15 16:24:37 |\n",
      "| JACLYN VICTOR GEMILANG BERSAMA MPO                                    | DATE 17 Oct 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/jaclyn-victor-gemilang-bersama-mpo/?occurrence=2026-10-17                                  | 2025-12-15 16:24:42 |\n",
      "| A REGAL EVENING WITH STEPHEN HOUGH                                    | DATE 24 Oct 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/a-regal-evening-with-stephen-hough/?occurrence=2026-10-24                                  | 2025-12-15 16:24:48 |\n",
      "| BEATS OF BORNEO: ALENA MURANG WITH THE MPO                            | DATE 31 Oct 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/beats-of-borneo-alena-murang-with-the-mpo/?occurrence=2026-10-31                           | 2025-12-15 16:24:53 |\n",
      "| AS IF SHE WERE HERE: CHEN JIA SINGS TERESA TENG                       | DATE 07 Nov 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/as-if-she-were-here-chen-jia-sings-teresa-teng/?occurrence=2026-11-07                      | 2025-12-15 16:24:59 |\n",
      "| SIMFONI MANTRA: KUNTO AJI BERSAMA MPO                                 | DATE 14 Nov 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/simfoni-mantra-kunto-aji-bersama-mpo/?occurrence=2026-11-14                                | 2025-12-15 16:25:04 |\n",
      "| A CHORALE SPECTACULAR                                                 | DATE 05 Dec 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/a-chorale-spectacular/?occurrence=2026-12-05                                               | 2025-12-15 16:25:10 |\n",
      "| THE SOCIAL SATURDAY SSWIM CLUB SPECIAL FEAT CEZAIRE                   | DATE 20 Dec 2025      | LOCATION Wet Deck W KL W Hotel Kuala Lumpur     | https://myticket.asia/events/the-social-saturday-sswim-club-special-feat-cezaire/?occurrence=2025-12-20                 | 2025-12-15 16:25:17 |\n",
      "| CLICQUOT COUNTDOWN NUIT DOR√âE EDITION                                 | DATE 31 Dec 2025      | LOCATION W Living Room W Hotel Kuala Lumpur     | https://myticket.asia/events/clicquot-countdown-nuit-doree-edition/?occurrence=2025-12-31                               | 2025-12-15 16:25:22 |\n",
      "| ELECTRIFY THE SKY WHITE EDITION 2.0                                   | DATE 31 Dec 2025      | LOCATION Wet Deck W KL W Hotel Kuala Lumpur     | https://myticket.asia/events/electrify-the-sky-white-edition-2-0/?occurrence=2025-12-31                                 | 2025-12-15 16:25:29 |\n",
      "| YOURS SATYAN LIVE IN KL ONE VOICE ONE SOUL                            | DATE 10 Jan 2026      | LOCATION Auditorium Jeffrey Cheah Subang Jaya   | https://myticket.asia/events/yours-satyan-live-in-kl-one-voice-one-soul/?occurrence=2026-01-10                          | 2025-12-15 16:25:36 |\n",
      "| AMADEUS LIVE                                                          | DATE 10 Jan 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/amadeus-live/?occurrence=2026-01-10                                                        | 2025-12-15 16:25:42 |\n",
      "| FAMILY FUN DAY: CIRQUE DE LA SYMPHONIE                                | DATE 17 Jan 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/family-fun-day-cirque-de-la-symphonie/?occurrence=2026-01-17                               | 2025-12-15 16:25:47 |\n",
      "| CIRQUE DE LA SYMPHONIE                                                | DATE 17 Jan 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/cirque-de-la-symphonie-2/?occurrence=2026-01-17                                            | 2025-12-15 16:25:52 |\n",
      "| SIMFONI GEMILANG SECOND EDITION BERSAMA DATO‚Äô M. NASIR                | DATE 24 Jan 2026      | LOCATION Grand Banquet Hall Residensi UTMKL UTM | https://myticket.asia/events/simfoni-gemilang-second-edition-bersama-dato-m-nasir/?occurrence=2026-01-24                | 2025-12-15 16:25:57 |\n",
      "| P. RAMLEE‚ÄôS MADU TIGA: ‚ÄòLIVE‚Äô IN CONCERT                              | DATE 24 Jan 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/p-ramlees-madu-tiga-live-in-concert/?occurrence=2026-01-24                                 | 2025-12-15 16:26:02 |\n",
      "| MOZART‚ÄôS 270TH BIRTHDAY!                                              | DATE 27 Jan 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/mozarts-270th-birthday/?occurrence=2026-01-27                                              | 2025-12-15 16:26:09 |\n",
      "| MEMOIRS OF A FLORAL FANTASY                                           | DATE 07 Feb 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/memoirs-of-a-floral-fantasy/?occurrence=2026-02-07                                         | 2025-12-15 16:26:14 |\n",
      "| HARRY POTTER AND THE PRISONER OF AZKABAN IN CONCERT                   | DATE 04 - 05 Apr 2026 | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/harry-potter-and-the-prisoner-of-azkaban-in-concert/?occurrence=2026-04-04&time=1775332800 | 2025-12-15 16:26:20 |\n",
      "| THE MPO AND BELLE SISOSKI: ETHNOSPHERE                                | DATE 11 Apr 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/the-mpo-and-belle-sisoski-ethnosphere/?occurrence=2026-04-11                               | 2025-12-15 16:26:25 |\n",
      "| GLAZUNOV: THE SEASONS IN SYMPHONY                                     | DATE 18 Apr 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/glazunov-the-seasons-in-symphony/?occurrence=2026-04-18                                    | 2025-12-15 16:26:31 |\n",
      "| AN EVENING OF VERDI                                                   | DATE 25 Apr 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/an-evening-of-verdi/?occurrence=2026-04-25                                                 | 2025-12-15 16:26:36 |\n",
      "| STAR WARS: A NEW HOPE IN CONCERT                                      | DATE 03 - 04 May 2026 | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/star-wars-a-new-hope-in-concert/?occurrence=2026-05-03&time=1777820400                     | 2025-12-15 16:26:42 |\n",
      "| RESONANCE SHILA AMZAH IN HARMONY: 25 YEARS OF MUSIC & MEMORIES        | DATE 09 May 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/resonance-shila-amzah-in-harmony-25-years-of-music-memories/?occurrence=2026-05-09         | 2025-12-15 16:26:47 |\n",
      "| CHARLES YANG RELOADED                                                 | DATE 16 May 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/charles-yang-reloaded/?occurrence=2026-05-16                                               | 2025-12-15 16:26:53 |\n",
      "| THE INCREDIBLE VOYAGE OF ALASDAIR MALLOY                              | DATE 23 May 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/the-incredible-voyage-of-alasdair-malloy/?occurrence=2026-05-23                            | 2025-12-15 16:26:58 |\n",
      "| THE PRODIGIES                                                         | DATE 18 Apr 2026      | LOCATION Aula Simfonia Jakarta Indonesia        | https://myticket.asia/events/the-prodigies/?occurrence=2026-04-18                                                       | 2025-12-15 16:27:04 |\n",
      "| SYMPHONIC GHIBLI II                                                   | DATE 03 - 04 Jul 2026 | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/symphonic-ghibli-ii/?occurrence=2026-07-03&time=1783108800                                 | 2025-12-15 16:27:10 |\n",
      "| FROM BROADWAY TO DISNEY: LEA SALONGA WITH THE MPO                     | DATE 26 Sep 2026      | LOCATION Dewan Filharmonik Petronas KLCC        | https://myticket.asia/events/from-broadway-to-disney-lea-salonga-with-the-mpo/?occurrence=2026-09-26                    | 2025-12-15 16:27:15 |\n",
      "\n",
      "Saved 42 unique rows ‚Üí all_events_KL.csv\n"
     ]
    }
   ],
   "source": [
    "# scrape_myticket_asia.py\n",
    "# pip install selenium pandas tabulate\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, time, re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException, StaleElementReferenceException, ElementClickInterceptedException\n",
    ")\n",
    "\n",
    "HOME = \"https://myticket.asia/\"\n",
    "OUTPUT_CSV = \"all_events_KL.csv\"\n",
    "WAIT = 25\n",
    "\n",
    "# ---------- driver ----------\n",
    "def make_driver() -> webdriver.Chrome:\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    opts.add_argument(\"--start-maximized\")\n",
    "    opts.add_argument(\"--disable-notifications\")\n",
    "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "    # Uncomment for headless:\n",
    "    # opts.add_argument(\"--headless=new\")\n",
    "    return webdriver.Chrome(options=opts)\n",
    "\n",
    "# ---------- utils ----------\n",
    "def slow_scroll_full_page(driver: webdriver.Chrome, step_px=500, nap=0.25, max_loops=200):\n",
    "    last = -1\n",
    "    for _ in range(max_loops):\n",
    "        driver.execute_script(\"window.scrollBy(0, arguments[0]);\", step_px)\n",
    "        time.sleep(nap)\n",
    "        y = driver.execute_script(\"return window.scrollY;\")\n",
    "        if y == last:\n",
    "            break\n",
    "        last = y\n",
    "\n",
    "def click_all_load_more(driver: webdriver.Chrome, max_clicks=50):\n",
    "    \"\"\"\n",
    "    Click all visible 'Load More' variants used by Modern Events Calendar.\n",
    "    Non-exact: CSS + text-normalization fallback.\n",
    "    \"\"\"\n",
    "    def visible(e): \n",
    "        try: return e.is_displayed() and e.is_enabled()\n",
    "        except: return False\n",
    "\n",
    "    clicks = 0\n",
    "    while clicks < max_clicks:\n",
    "        # Try common MEC 'load more' controls\n",
    "        buttons = []\n",
    "        buttons += driver.find_elements(By.CSS_SELECTOR, \"a.mec-load-more, button.mec-load-more\")\n",
    "        buttons += driver.find_elements(By.CSS_SELECTOR, \".mec-load-more a, .mec-load-more button\")\n",
    "        # Text fallback (case-insensitive contains 'load more')\n",
    "        buttons += [e for e in driver.find_elements(By.TAG_NAME, \"a\") if re.search(r'load\\s*more', (e.text or '').lower())]\n",
    "\n",
    "        # Dedup DOM elements\n",
    "        seen_ids = set()\n",
    "        uniq = []\n",
    "        for b in buttons:\n",
    "            try:\n",
    "                key = b.id\n",
    "                if key not in seen_ids:\n",
    "                    seen_ids.add(key)\n",
    "                    uniq.append(b)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        clicked_any = False\n",
    "        for b in uniq:\n",
    "            if not visible(b): \n",
    "                continue\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", b)\n",
    "                time.sleep(0.25)\n",
    "                b.click()\n",
    "                clicked_any = True\n",
    "                clicks += 1\n",
    "                time.sleep(1.2)  # allow new cards to render\n",
    "            except (ElementClickInterceptedException, StaleElementReferenceException):\n",
    "                continue\n",
    "            except Exception:\n",
    "                continue\n",
    "        if not clicked_any:\n",
    "            break\n",
    "\n",
    "def load_existing(path: str) -> pd.DataFrame:\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            return pd.read_csv(path)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return pd.DataFrame(columns=[\"Title\", \"Date\", \"Location\", \"Link\", \"TimeScraped\"])\n",
    "\n",
    "def save_append_dedup(df_new: pd.DataFrame, path: str):\n",
    "    old = load_existing(path)\n",
    "    all_df = pd.concat([old, df_new], ignore_index=True)\n",
    "    # primary dedup by Link; secondary by (Title, Date, Location)\n",
    "    all_df = all_df.drop_duplicates(subset=[\"Link\"], keep=\"first\")\n",
    "    all_df = all_df.drop_duplicates(subset=[\"Title\", \"Date\", \"Location\"], keep=\"first\")\n",
    "    all_df.to_csv(path, index=False)\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    return \" \".join((s or \"\").split())\n",
    "\n",
    "# ---------- flexible field extraction on detail pages (no exact XPaths) ----------\n",
    "def extract_title(driver: webdriver.Chrome) -> str:\n",
    "    # Prefer page H1 within main area, fallback to any h1\n",
    "    for css in [\"main h1\", \"#main-content h1\", \"h1\"]:\n",
    "        els = driver.find_elements(By.CSS_SELECTOR, css)\n",
    "        for el in els:\n",
    "            t = clean_text(el.text)\n",
    "            if t: return t\n",
    "    return \"\"\n",
    "\n",
    "def extract_date(driver: webdriver.Chrome) -> str:\n",
    "    # Try <time>, then common date classes, then definition list dd near date-like labels\n",
    "    # 1) time tags\n",
    "    for el in driver.find_elements(By.TAG_NAME, \"time\"):\n",
    "        t = clean_text(el.text)\n",
    "        if t: return t\n",
    "    # 2) common date-ish classes/labels\n",
    "    for css in [\"[class*='date']\", \"[class*='time']\"]:\n",
    "        for el in driver.find_elements(By.CSS_SELECTOR, css):\n",
    "            t = clean_text(el.text)\n",
    "            if t and len(t) > 2: return t\n",
    "    # 3) simple dl/dd extraction: pick the dd whose preceding label mentions date/time\n",
    "    dds = driver.find_elements(By.CSS_SELECTOR, \"dl dd\")\n",
    "    dts = driver.find_elements(By.CSS_SELECTOR, \"dl dt\")\n",
    "    for i, dt in enumerate(dts):\n",
    "        label = (dt.text or \"\").lower()\n",
    "        if any(k in label for k in [\"date\", \"time\", \"when\"]):\n",
    "            try:\n",
    "                t = clean_text(dds[i].text)\n",
    "                if t: return t\n",
    "            except Exception:\n",
    "                pass\n",
    "    # fallback: first dd with non-empty text\n",
    "    for el in dds:\n",
    "        t = clean_text(el.text)\n",
    "        if t: return t\n",
    "    return \"\"\n",
    "\n",
    "def extract_location(driver: webdriver.Chrome) -> str:\n",
    "    # Look for 'venue' or 'location' labels first\n",
    "    dds = driver.find_elements(By.CSS_SELECTOR, \"dl dd\")\n",
    "    dts = driver.find_elements(By.CSS_SELECTOR, \"dl dt\")\n",
    "    for i, dt in enumerate(dts):\n",
    "        label = (dt.text or \"\").lower()\n",
    "        if any(k in label for k in [\"venue\", \"location\", \"where\"]):\n",
    "            try:\n",
    "                t = clean_text(dds[i].text)\n",
    "                if t: return t\n",
    "            except Exception:\n",
    "                pass\n",
    "    # Generic class-based fallbacks\n",
    "    for css in [\"[class*='venue']\", \"[class*='location']\"]:\n",
    "        for el in driver.find_elements(By.CSS_SELECTOR, css):\n",
    "            t = clean_text(el.text)\n",
    "            if t and len(t) > 2: return t\n",
    "    # last resort: look for address-like blocks\n",
    "    for tag in [\"address\", \"p\", \"span\", \"div\"]:\n",
    "        for el in driver.find_elements(By.TAG_NAME, tag):\n",
    "            txt = clean_text(el.text)\n",
    "            if any(k in txt.lower() for k in [\"hall\", \"theatre\", \"theater\", \"ballroom\", \"arena\", \"convention\", \"stadium\", \"klcc\"]):\n",
    "                if len(txt) <= 120:\n",
    "                    return txt\n",
    "    return \"\"\n",
    "\n",
    "# ---------- main ----------\n",
    "def main():\n",
    "    driver = make_driver()\n",
    "    wait = WebDriverWait(driver, WAIT)\n",
    "    rows = []\n",
    "\n",
    "    try:\n",
    "        driver.get(HOME)\n",
    "        wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        time.sleep(1.0)\n",
    "\n",
    "        # Load everything we can see: scroll + load more + scroll again\n",
    "        slow_scroll_full_page(driver)\n",
    "        click_all_load_more(driver)\n",
    "        slow_scroll_full_page(driver)\n",
    "\n",
    "        # Collect ALL event links (no exact XPath):\n",
    "        # MEC usually wraps titles in h4 > a inside containers with id starting mec_skin_events_.\n",
    "        card_links = driver.find_elements(By.CSS_SELECTOR, \"div[id^='mec_skin_events_'] h4 a, section[id^='mec_skin_events_'] h4 a\")\n",
    "        # If homepage changes, also try generic h4 > a with myticket links\n",
    "        if not card_links:\n",
    "            card_links = [a for a in driver.find_elements(By.CSS_SELECTOR, \"h4 a\") if \"myticket.asia\" in (a.get_attribute(\"href\") or \"\")]\n",
    "\n",
    "        # Dedup by href\n",
    "        links = []\n",
    "        seen = set()\n",
    "        for el in card_links:\n",
    "            try:\n",
    "                href = (el.get_attribute(\"href\") or \"\").strip()\n",
    "                title_from_list = clean_text(el.text)\n",
    "                if href and href not in seen:\n",
    "                    seen.add(href)\n",
    "                    links.append((title_from_list, href))\n",
    "            except StaleElementReferenceException:\n",
    "                continue\n",
    "\n",
    "        if not links:\n",
    "            print(\"No event links found. Site structure may have changed.\")\n",
    "            return\n",
    "\n",
    "        # Visit each link in a new tab and extract fields\n",
    "        for title_from_list, href in links:\n",
    "            driver.execute_script(\"window.open(arguments[0], '_blank');\", href)\n",
    "            driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "            try:\n",
    "                wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "            except TimeoutException:\n",
    "                pass\n",
    "\n",
    "            name = extract_title(driver) or title_from_list\n",
    "            date = extract_date(driver)\n",
    "            location = extract_location(driver)\n",
    "\n",
    "            row = {\n",
    "                \"Title\": clean_text(name),\n",
    "                \"Date\": clean_text(date),\n",
    "                \"Location\": clean_text(location),\n",
    "                \"Link\": href,\n",
    "                \"TimeScraped\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "            # close tab and return\n",
    "            driver.close()\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "            time.sleep(0.4)\n",
    "\n",
    "        df = pd.DataFrame(rows)\n",
    "\n",
    "        # -------- NEW: show how many new events were added --------\n",
    "        old_df = load_existing(OUTPUT_CSV)\n",
    "        old_count = len(old_df)\n",
    "\n",
    "        save_append_dedup(df, OUTPUT_CSV)\n",
    "\n",
    "        updated_df = load_existing(OUTPUT_CSV)\n",
    "        new_total = len(updated_df)\n",
    "        added_count = new_total - old_count\n",
    "        print(f\"\\nüÜï {added_count} new event(s) added ({new_total} total now in file).\")\n",
    "        # -------- END NEW --------\n",
    "\n",
    "        from tabulate import tabulate\n",
    "        print(tabulate(df, headers=\"keys\", tablefmt=\"github\", showindex=False))\n",
    "        print(f\"\\nSaved {len(df)} unique rows ‚Üí {OUTPUT_CSV}\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577adaeb",
   "metadata": {},
   "source": [
    "# Jarkata (Completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b023b0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ Found 7 featured cards\n",
      "‚úÖ Megatix Indonesia (Featured): grabbed 2 new events\n",
      "\n",
      "‚úÖ TOTAL Unique Events: 9\n",
      "| Title                                       | Date             | Location          | Link                                                  | TimeScraped         |\n",
      "|---------------------------------------------|------------------|-------------------|-------------------------------------------------------|---------------------|\n",
      "| NORA EN PURE                                | Sat, 13 Dec 2025 | Savaya Bali       | https://megatix.co.id/events/nora-en-pure-1312        | 8/12/2025 10:21     |\n",
      "| NORA EN PURE                                | Sat, 13 Dec 2025 | Savaya Bali       | https://megatix.co.id/events/nora-en-pure-1312        | 8/12/2025 10:21     |\n",
      "| X-Clusive Presents: Bali NYE 2025 with Tyga | Wed, 31 Dec 2025 | The Stage         | https://megatix.co.id/events/nye-2025-with-tyga       | 8/12/2025 10:21     |\n",
      "| ANJUNADEEP                                  | Sun, 14 Dec 2025 | Savaya Bali       | https://megatix.co.id/events/anjunadeep-1412          | 8/12/2025 10:21     |\n",
      "| RUDIMENTAL                                  | Fri, 02 Jan 2026 | Atlas Beach Club  | https://megatix.co.id/events/rudimental-2026          | 8/12/2025 10:21     |\n",
      "| NEW YEAR'S EVE 2025 | MESA BALI             | Wed, 31 Dec 2025 | The Mesa Bali     | https://megatix.co.id/events/nye-mesa-2025            | 8/12/2025 10:21     |\n",
      "| NEW YEAR'S EVE PARTY AT W BALI - SEMINYAK   | Wed, 31 Dec 2025 | W Bali - Seminyak | https://megatix.co.id/events/NYE-W-BALI-SEMINYAK      | 8/12/2025 10:21     |\n",
      "| BEN B√ñHMER (LIVE)                           | Sat, 27 Dec 2025 | Savaya Bali       | https://megatix.co.id/events/ben-bohmer-2712          | 2025-12-15 16:27:27 |\n",
      "| ARTBAT & MISS MONIQUE                       | Mon, 29 Dec 2025 | Savaya Bali       | https://megatix.co.id/events/artbat-miss-monique-2912 | 2025-12-15 16:27:27 |\n",
      "\n",
      "üíæ CSV saved: all_jakarta.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Megatix Indonesia: Featured Events Scraper (deduplicated) ---\n",
    "# Output CSV: all_jakarta.csv\n",
    "# Works for https://megatix.co.id/\n",
    "# pip install selenium tabulate pandas\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime   # <-- added for timestamp\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "OUTPUT_CSV = \"all_jakarta.csv\"\n",
    "URL = \"https://megatix.co.id/\"\n",
    "\n",
    "# Load existing CSV (if any) to prevent duplicates\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    existing_df = pd.read_csv(OUTPUT_CSV)\n",
    "    seen_links = set(existing_df[\"Link\"].dropna())\n",
    "else:\n",
    "    existing_df = pd.DataFrame()\n",
    "    seen_links = set()\n",
    "\n",
    "# ---------------- DRIVER SETUP ----------------\n",
    "opts = webdriver.ChromeOptions()\n",
    "opts.add_argument(\"--start-maximized\")\n",
    "opts.add_argument(\"--disable-notifications\")\n",
    "driver = webdriver.Chrome(options=opts)\n",
    "driver.set_window_size(1400, 1000)\n",
    "all_rows = []\n",
    "\n",
    "# ---------------- SCRAPER ----------------\n",
    "try:\n",
    "    driver.get(URL)\n",
    "    WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='__nuxt']\")))\n",
    "    time.sleep(1.0)\n",
    "\n",
    "    FEATURED_CARD_X = \"//*[@id='__nuxt']/div/div[3]/div/main/section/section[2]/div[2]//article[.//h3]\"\n",
    "\n",
    "    # Scroll to load all cards\n",
    "    prev = -1\n",
    "    stable = 0\n",
    "    for _ in range(60):\n",
    "        cards = driver.find_elements(By.XPATH, FEATURED_CARD_X)\n",
    "        count = len(cards)\n",
    "        if count == prev:\n",
    "            stable += 1\n",
    "            if stable >= 2:\n",
    "                break\n",
    "        else:\n",
    "            stable = 0\n",
    "        prev = count\n",
    "        driver.execute_script(\"window.scrollBy(0, 900);\")\n",
    "        time.sleep(1.2)\n",
    "\n",
    "    cards = driver.find_elements(By.XPATH, FEATURED_CARD_X)\n",
    "    print(f\"üü¢ Found {len(cards)} featured cards\")\n",
    "\n",
    "    for ev in cards:\n",
    "        try:\n",
    "            title = ev.find_element(By.XPATH, \".//h3/span\").text.strip()\n",
    "        except:\n",
    "            title = \"No Title\"\n",
    "        try:\n",
    "            date = ev.find_element(By.XPATH, \".//div[1]/div[1]/span\").text.strip()\n",
    "        except:\n",
    "            date = \"No Date\"\n",
    "        try:\n",
    "            location = ev.find_element(By.XPATH, \".//div[1]/div[3]/span\").text.strip()\n",
    "        except:\n",
    "            location = \"Indonesia\"\n",
    "        try:\n",
    "            link = ev.find_element(By.XPATH, \".//ancestor::a\").get_attribute(\"href\") or \"No Link\"\n",
    "        except:\n",
    "            link = \"No Link\"\n",
    "\n",
    "        # Skip duplicates already in CSV\n",
    "        if link in seen_links:\n",
    "            continue\n",
    "\n",
    "        all_rows.append({\n",
    "            \"Title\": title,\n",
    "            \"Date\": date,\n",
    "            \"Location\": location,\n",
    "            \"Link\": link,\n",
    "            \"TimeScraped\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # <-- added line\n",
    "        })\n",
    "        seen_links.add(link)\n",
    "\n",
    "    print(f\"‚úÖ Megatix Indonesia (Featured): grabbed {len(all_rows)} new events\")\n",
    "\n",
    "except TimeoutException:\n",
    "    print(\"‚ö† Timeout waiting for Megatix Indonesia page\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "# ---------------- OUTPUT ----------------\n",
    "if all_rows:\n",
    "    new_df = pd.DataFrame(all_rows)\n",
    "    final_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "else:\n",
    "    final_df = existing_df\n",
    "\n",
    "print(f\"\\n‚úÖ TOTAL Unique Events: {len(final_df)}\")\n",
    "print(tabulate(final_df.tail(20), headers=\"keys\", tablefmt=\"github\", showindex=False))\n",
    "\n",
    "final_df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\nüíæ CSV saved: {OUTPUT_CSV}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e007c521",
   "metadata": {},
   "source": [
    "# Hong Kong (done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16fb98de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- HONG KONG: Live Nation only ---\n",
    "# # Output CSV: all_hong_kong.csv\n",
    "# # pip install selenium tabulate pandas\n",
    "\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "# from tabulate import tabulate\n",
    "# import pandas as pd\n",
    "# import time, os, unicodedata, shutil\n",
    "# from urllib.parse import urlparse\n",
    "# from datetime import datetime  # <-- added for timestamp\n",
    "\n",
    "# # =================== CONFIG ===================\n",
    "# OUTPUT_CSV = \"all_hong_kong.csv\"\n",
    "# BACKUP_CSV = OUTPUT_CSV + \".bak\"\n",
    "# LIVE_HOME = \"https://www.livenation.hk/en\"\n",
    "# WAIT = 25\n",
    "\n",
    "# # =================== DRIVER ===================\n",
    "# ua = (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "#       \"(KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\")\n",
    "# opts = webdriver.ChromeOptions()\n",
    "# opts.add_argument(f\"user-agent={ua}\")\n",
    "# opts.add_argument(\"--start-maximized\")\n",
    "# opts.add_argument(\"--disable-notifications\")\n",
    "# opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "# opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "# opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "# opts.page_load_strategy = \"eager\"\n",
    "\n",
    "# driver = webdriver.Chrome(options=opts)\n",
    "# driver.set_window_size(1400, 1000)\n",
    "# driver.set_page_load_timeout(60)\n",
    "# driver.set_script_timeout(60)\n",
    "\n",
    "# # =================== UTILS ===================\n",
    "# def accept_cookies(driver):\n",
    "#     for by, sel in [\n",
    "#         (By.ID, \"onetrust-accept-btn-handler\"),\n",
    "#         (By.XPATH, \"//*[@id='onetrust-accept-btn-handler']\"),\n",
    "#         (By.XPATH, \"//*[self::button or self::a][contains(.,'Accept All') or contains(.,'Accept')]\"),\n",
    "#     ]:\n",
    "#         try:\n",
    "#             el = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((by, sel)))\n",
    "#             driver.execute_script(\"arguments[0].click();\", el)\n",
    "#             time.sleep(0.2)\n",
    "#             break\n",
    "#         except Exception:\n",
    "#             continue\n",
    "\n",
    "# def safe_text(el):\n",
    "#     try:\n",
    "#         return unicodedata.normalize(\"NFKC\", \" \".join(el.text.split()))\n",
    "#     except Exception:\n",
    "#         return \"\"\n",
    "\n",
    "# def norm(s):\n",
    "#     return \" \".join(unicodedata.normalize(\"NFKC\", (s or \"\").strip().lower()).split())\n",
    "\n",
    "# def canonicalize_link(link):\n",
    "#     if not link:\n",
    "#         return \"\"\n",
    "#     p = urlparse(link)\n",
    "#     return f\"{(p.netloc or '').lower()}{p.path or ''}\"\n",
    "\n",
    "# def make_event_id(platform, link, title, date):\n",
    "#     canon = canonicalize_link(link)\n",
    "#     if canon:\n",
    "#         return f\"{platform}|{canon}\"\n",
    "#     return f\"{norm(platform)}|{norm(title)}|{norm(date)}\"\n",
    "\n",
    "# # =================== LOAD OLD CSV ===================\n",
    "# seen_ids_persisted = set()\n",
    "# old_df = None\n",
    "# column_order = [\"Platform\", \"Title\", \"Date\", \"Location\", \"Link\", \"TimeScraped\"]  # <-- added column here\n",
    "\n",
    "# if os.path.exists(OUTPUT_CSV):\n",
    "#     try:\n",
    "#         old_df = pd.read_csv(OUTPUT_CSV, dtype=str, keep_default_na=False)\n",
    "#         if list(old_df.columns):\n",
    "#             column_order = list(old_df.columns)\n",
    "#         for _, r in old_df.iterrows():\n",
    "#             seen_ids_persisted.add(\n",
    "#                 make_event_id(\n",
    "#                     str(r.get(\"Platform\",\"\")),\n",
    "#                     str(r.get(\"Link\",\"\")),\n",
    "#                     str(r.get(\"Title\",\"\")),\n",
    "#                     str(r.get(\"Date\",\"\")),\n",
    "#                 )\n",
    "#             )\n",
    "#         print(f\"‚Ü∫ Loaded {len(seen_ids_persisted)} previously-scraped rows\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ö† Could not read existing CSV: {e}\")\n",
    "\n",
    "# # --- ensure TimeScraped is part of the schema even if old CSV didn't have it ---\n",
    "# try:\n",
    "#     if old_df is not None and \"TimeScraped\" not in old_df.columns:\n",
    "#         old_df[\"TimeScraped\"] = \"\"\n",
    "#         column_order = list(old_df.columns)\n",
    "#     if \"TimeScraped\" not in column_order:\n",
    "#         column_order = list(column_order) + [\"TimeScraped\"]\n",
    "# except Exception:\n",
    "#     pass\n",
    "\n",
    "# all_rows = []\n",
    "# seen_links_this_run = set()\n",
    "\n",
    "# def append_row(platform, title, date_text, location, link):\n",
    "#     row = {\n",
    "#         \"Platform\": platform,\n",
    "#         \"Title\": title or \"No Title\",\n",
    "#         \"Date\": date_text or \"No Date\",\n",
    "#         \"Location\": location or \"Hong Kong\",\n",
    "#         \"Link\": link or \"No Link\",\n",
    "#         \"TimeScraped\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # <-- added timestamp\n",
    "#     }\n",
    "#     eid = make_event_id(row[\"Platform\"], row[\"Link\"], row[\"Title\"], row[\"Date\"])\n",
    "#     if eid in seen_ids_persisted:\n",
    "#         return\n",
    "#     canon = canonicalize_link(row[\"Link\"])\n",
    "#     if canon and canon in seen_links_this_run:\n",
    "#         return\n",
    "#     if canon:\n",
    "#         seen_links_this_run.add(canon)\n",
    "#     all_rows.append(row)\n",
    "\n",
    "# # =================== LIVE NATION HK ===================\n",
    "# def find_all_events_section():\n",
    "#     try:\n",
    "#         return WebDriverWait(driver, 12).until(\n",
    "#             EC.presence_of_element_located((By.XPATH, \"//*[self::h2 or self::h3][contains(.,'All Events')]/ancestor::section[1]\"))\n",
    "#         )\n",
    "#     except TimeoutException:\n",
    "#         return WebDriverWait(driver, 12).until(\n",
    "#             EC.presence_of_element_located((By.XPATH, \"//*[@id='main']/div/div[6]/section/div/div/div[2]\"))\n",
    "#         )\n",
    "\n",
    "# def scroll_deep(times=8, pause=0.45):\n",
    "#     for _ in range(times):\n",
    "#         driver.execute_script(\"window.scrollBy(0, 1000);\")\n",
    "#         time.sleep(pause)\n",
    "\n",
    "# def scrape_livenation_page(label=\"LiveNationHK\"):\n",
    "#     section = find_all_events_section()\n",
    "#     cards = section.find_elements(By.XPATH, \".//li[.//a[@href]]\")\n",
    "#     if not cards:\n",
    "#         cards = section.find_elements(By.XPATH, \".//ul[1]/li | .//div[contains(@class,'Card')]\")\n",
    "#     for li in cards:\n",
    "#         link, title, location, date_text = \"\", \"\", \"Hong Kong\", \"\"\n",
    "#         try:\n",
    "#             a = li.find_element(By.XPATH, \".//a[@href]\")\n",
    "#             link = a.get_attribute(\"href\") or \"\"\n",
    "#         except NoSuchElementException:\n",
    "#             pass\n",
    "#         for xp in [\".//p[contains(@class,'title')][1]\", \".//p[1]\", \".//*[self::h3 or self::h2 or self::p][1]\"]:\n",
    "#             try:\n",
    "#                 title = safe_text(li.find_element(By.XPATH, xp))\n",
    "#                 if title: break\n",
    "#             except NoSuchElementException:\n",
    "#                 continue\n",
    "#         for xp in [\".//p[contains(@class,'location')]\", \".//p[2]\"]:\n",
    "#             try:\n",
    "#                 l = safe_text(li.find_element(By.XPATH, xp))\n",
    "#                 if l: location = l; break\n",
    "#             except NoSuchElementException:\n",
    "#                 continue\n",
    "#         for xp in [\".//time\", \".//p[contains(@class,'date')][1]\", \".//p[2]\"]:\n",
    "#             try:\n",
    "#                 d = safe_text(li.find_element(By.XPATH, xp))\n",
    "#                 if d and d.lower() != location.lower():\n",
    "#                     date_text = d; break\n",
    "#             except NoSuchElementException:\n",
    "#                 continue\n",
    "#         append_row(label, title, date_text, location, link)\n",
    "\n",
    "# def run_livenation():\n",
    "#     driver.get(LIVE_HOME)\n",
    "#     accept_cookies(driver)\n",
    "#     scroll_deep(8, 0.45)\n",
    "#     scrape_livenation_page(\"LiveNationHK\")\n",
    "#     # Try \"Page 2\"\n",
    "#     try:\n",
    "#         nxt = WebDriverWait(driver, 6).until(\n",
    "#             EC.element_to_be_clickable((By.XPATH, \"//*[@id='main']/div/div[6]//nav//li[3]/button\"))\n",
    "#         )\n",
    "#         driver.execute_script(\"arguments[0].click();\", nxt)\n",
    "#         time.sleep(1.0)\n",
    "#         scroll_deep(8, 0.45)\n",
    "#         scrape_livenation_page(\"LiveNationHK\")\n",
    "#     except TimeoutException:\n",
    "#         pass\n",
    "\n",
    "# # =================== RUN ===================\n",
    "# try:\n",
    "#     run_livenation()\n",
    "# finally:\n",
    "#     try:\n",
    "#         driver.quit()\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "# # =================== SAVE (append-only) ===================\n",
    "# new_df = pd.DataFrame(all_rows, dtype=str).reset_index(drop=True)\n",
    "\n",
    "# if os.path.exists(OUTPUT_CSV) and old_df is not None:\n",
    "#     if not new_df.empty:\n",
    "#         new_df[\"__id\"] = new_df.apply(\n",
    "#             lambda r: make_event_id(r.get(\"Platform\",\"\"), r.get(\"Link\",\"\"), r.get(\"Title\",\"\"), r.get(\"Date\",\"\")),\n",
    "#             axis=1\n",
    "#         )\n",
    "#         new_df = new_df[~new_df[\"__id\"].isin(seen_ids_persisted)].drop(columns=\"__id\")\n",
    "#     for col in column_order:\n",
    "#         if col not in new_df.columns:\n",
    "#             new_df[col] = \"\"\n",
    "#     new_df = new_df[column_order]\n",
    "#     final_df = pd.concat([old_df, new_df], ignore_index=True)\n",
    "# else:\n",
    "#     for col in column_order:\n",
    "#         if col not in new_df.columns:\n",
    "#             new_df[col] = \"\"\n",
    "#     new_df = new_df[column_order]\n",
    "#     final_df = new_df\n",
    "\n",
    "# # Fill missing TimeScraped for older rows\n",
    "# if \"TimeScraped\" in final_df.columns:\n",
    "#     final_df[\"TimeScraped\"] = final_df[\"TimeScraped\"].replace(\"\", pd.NA).fillna(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "# added = len(new_df)\n",
    "# print(f\"\\n‚úÖ New rows appended this run: {added}\")\n",
    "# print(f\"üì¶ Total rows in file: {len(final_df)}\")\n",
    "# print(tabulate(final_df.tail(max(1, min(added, 20))), headers=\"keys\", tablefmt=\"github\", showindex=False))\n",
    "\n",
    "# tmp = OUTPUT_CSV + \".tmp\"\n",
    "# final_df.to_csv(tmp, index=False, encoding=\"utf-8-sig\")\n",
    "# if os.path.exists(OUTPUT_CSV):\n",
    "#     try:\n",
    "#         shutil.copy2(OUTPUT_CSV, BACKUP_CSV)\n",
    "#     except Exception:\n",
    "#         pass\n",
    "# os.replace(tmp, OUTPUT_CSV)\n",
    "# print(f\"\\nüíæ CSV updated: {OUTPUT_CSV}\")\n",
    "# if os.path.exists(BACKUP_CSV):\n",
    "#     print(f\"üßØ Backup saved: {BACKUP_CSV}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "247965ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö† Timeout while loading or scraping: Message: \n",
      "\n",
      "‚ö† Saved debug artifacts: fatal_timeout_screenshot.png, fatal_timeout_source.html\n",
      "\n",
      "‚úÖ TOTAL Unique Events: 28\n",
      "| Platform     | Title                                                                | Date                                                       | Location                                                                            | Link                                                                                                                   | TimeScraped      |\n",
      "|--------------|----------------------------------------------------------------------|------------------------------------------------------------|-------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------|------------------|\n",
      "| LiveNationHK | TWICE <THIS IS FOR> WORLD TOUR IN HONG KONG                          | SUN 07 DEC                                                 | Hong Kong | Kai Tak Stadium                                                         | https://www.livenation.hk/en/event/twice-this-is-for-world-tour-in-hong-kong-hong-kong-tickets-edp1621711              | 12/11/2025 16:11 |\n",
      "| LiveNationHK | Ali Abdaal - How to Make 2026 The Best Year of Your Life             | MON 08 DEC                                                 | Hong Kong | Hong Kong Jockey Club Amphitheatre, HKAPA                               | https://www.livenation.hk/en/event/ali-abdaal-how-to-make-2026-the-best-year-of-your-life-hong-kong-tickets-edp1625852 | 12/11/2025 16:11 |\n",
      "| LiveNationHK | Fly By Midnight - The Fastest Time Of Our Lives Tour                 | TUE 13 JAN                                                 | Hong Kong | Kitty Woo Stadium, Tung Po                                              | https://www.livenation.hk/en/event/fly-by-midnight-the-fastest-time-of-our-lives-tour-hong-kong-tickets-edp1621734     | 12/11/2025 16:11 |\n",
      "| LiveNationHK | DAY6 10th Anniversary Tour <The DECADE> in HONG KONG                 | SAT 17 JAN                                                 | Hong Kong | AsiaWorld-Arena                                                         | https://www.livenation.hk/en/event/day6-10th-anniversary-tour-the-decade-in-hong-kong-hong-kong-tickets-edp1626774     | 12/11/2025 16:11 |\n",
      "| LiveNationHK | BLACKPINK WORLD TOUR <DEADLINE> IN HONG KONG                         | SAT 24 JAN                                                 | Hong Kong | Kai Tak Stadium                                                         | https://www.livenation.hk/en/event/blackpink-world-tour-deadline-in-hong-kong-hong-kong-tickets-edp1601705             | 12/11/2025 16:11 |\n",
      "| LiveNationHK | BLACKPINK WORLD TOUR <DEADLINE> IN HONG KONG                         | SUN 25 JAN                                                 | Hong Kong | Kai Tak Stadium                                                         | https://www.livenation.hk/en/event/blackpink-world-tour-deadline-in-hong-kong-hong-kong-tickets-edp1601721             | 12/11/2025 16:11 |\n",
      "| LiveNationHK | BLACKPINK WORLD TOUR <DEADLINE> IN HONG KONG                         | MON 26 JAN                                                 | Hong Kong | Kai Tak Stadium                                                         | https://www.livenation.hk/en/event/blackpink-world-tour-deadline-in-hong-kong-hong-kong-tickets-edp1620999             | 12/11/2025 16:11 |\n",
      "| LiveNationHK | 2025-26 aespa LIVE TOUR - SYNK: aeXIS LINE - in HONG KONG            | SAT 07 FEB                                                 | Hong Kong | AsiaWorld-Arena                                                         | https://www.livenation.hk/en/event/2025-26-aespa-live-tour-synk-aexis-line-in-hong-kong-hong-kong-tickets-edp1620467   | 12/11/2025 16:11 |\n",
      "| LiveNationHK | 2025-26 aespa LIVE TOUR - SYNK: aeXIS LINE - in HONG KONG            | SUN 08 FEB                                                 | Hong Kong | AsiaWorld-Arena                                                         | https://www.livenation.hk/en/event/2025-26-aespa-live-tour-synk-aexis-line-in-hong-kong-hong-kong-tickets-edp1620483   | 12/11/2025 16:11 |\n",
      "| LiveNationHK | ONEREPUBLIC ‚ÄúFrom Asia, With Love‚Äù 2026                              | SAT 21 FEB                                                 | Hong Kong | AsiaWorld-Arena                                                         | https://www.livenation.hk/en/event/onerepublic-from-asia-with-love-2026-hong-kong-tickets-edp1626588                   | 12/11/2025 16:11 |\n",
      "| HKTicketing  | ATTACK ON TITAN - Beyond the Walls World Tour - The Official Concert | Displaying results: 1-3 of 3                               | HATSUNE MIKU EXPO 2025 in Hong Kong                                                 | https://premier.hkticketing.com/shows/show.aspx?sh=ATTAC1125                                                           | 12/11/2025 16:11 |\n",
      "| HKTicketing  | HK TICKETING - Calum Scott The Avenoir Tour 2026 in Hong Kong        | Date:\n",
      "Thu 05 Feb 2026 8:00PM                                                            | Venue:\n",
      "Runway 11, AsiaWorld-Expo, Lantau                                                                                     | https://premier.hkticketing.com/shows/show.aspx?sh=CALUM0226                                                           | 12/11/2025 16:11 |\n",
      "| HKTicketing  | HK TICKETING - HATSUNE MIKU EXPO 2025 in Hong Kong                   | Date:\n",
      "    ---- Select Date ----\n",
      "       \n",
      "      Sat 8 Nov 2025 8:00pm - ‰ºÅ‰Ωç(‰∏çË®≠ÂäÉ‰Ωç)Free Standing\n",
      "    \n",
      "       \n",
      "      Sat 8 Nov 2025 8:00pm - ÊåáÂÆöÂ∫ß‰Ωç/Marked Seating                                                            | Venue:\n",
      "AsiaWorld-Arena, AsiaWorld-Expo, Lantau                                                                                     | https://premier.hkticketing.com/shows/show.aspx?sh=HATSU1125                                                           | 12/11/2025 16:11 |\n",
      "| HKTicketing  | HK TICKETING - Disney On Ice presents Magic In the Stars             | Date:\n",
      "    ---- Select Date ----\n",
      "       \n",
      "      Fri 23 Jan 2026 3:30PM\n",
      "    \n",
      "       \n",
      "      Fri 23 Jan 2026 7:30PM\n",
      "    \n",
      "       \n",
      "      Sat 24 Jan 2026 11:30AM\n",
      "    \n",
      "       \n",
      "      Sat 24 Jan 2026 3:30PM\n",
      "    \n",
      "       \n",
      "      Sat 24 Jan 2026 7:30PM\n",
      "    \n",
      "       \n",
      "      Sun 25 Jan 2026 11:30AM\n",
      "    \n",
      "       \n",
      "      Sun 25 Jan 2026 3:30PM\n",
      "    \n",
      "       \n",
      "      Sun 25 Jan 2026 7:30PM                                                            | Venue:\n",
      "Kai Tak Arena, Kai Tak Sports Park, Kowloon                                                                                     | https://premier.hkticketing.com/shows/show.aspx?sh=DISNE0126                                                           | 12/11/2025 16:11 |\n",
      "| HKTicketing  | TRAVIS SCOTT CIRCUS MAXIMUS 2025 Êæ≥Èó®Á´ô                              | ¬∑ A maximum of 4 tickets can be purchased per Access Code. | 8 Oct 2025 (Wed) from 12:00 PM to 11:59 PM, or until pre-sale tickets are sold out. | https://hkt.hkticketing.com/en/#/allEvents/detail?projectId=50000000795004&noredirect=true                             | 12/11/2025 16:11 |\n",
      "| LiveNationHK | DAY6 10th Anniversary Tour <The DECADE> in HONG KONG                 | SUN 18 JAN                                                 | DAY6                                                                                | https://www.livenation.hk/en/event/day6-10th-anniversary-tour-the-decade-in-hong-kong-hong-kong-tickets-edp1630128     | 12/11/2025 16:11 |\n",
      "| LiveNationHK | Blue 25th Anniversary Tour                                           | SAT 07 FEB                                                 | Blue                                                                                | https://www.livenation.hk/en/event/blue-25th-anniversary-tour-hong-kong-tickets-edp1630645                             | 12/11/2025 16:11 |\n",
      "| LiveNationHK | 2025-26 TREASURE TOUR [PULSE ON] IN MACAO                            | FRI 06 MAR                                                 | TREASURE                                                                            | https://www.livenation.hk/en/event/2025-26-treasure-tour-pulse-on-in-macao-macao-tickets-edp1629039                    | 12/11/2025 16:11 |\n",
      "| LiveNationHK | Acraze in HK                                                         | SAT 29 NOV 10:00 PM                                        | Acraze                                                                              | https://www.livenation.hk/en/event/acraze-in-hk-hong-kong-tickets-edp1633367                                           | 12/11/2025 16:11 |\n",
      "| LiveNationHK | Keinemusik                                                           | SUN 05 APR                                                 | Keinemusik                                                                          | https://www.livenation.hk/en/event/keinemusik-hong-kong-tickets-edp1632981                                             | 12/11/2025 16:11 |\n",
      "\n",
      "üíæ CSV saved: all_hong_kong.csv\n"
     ]
    }
   ],
   "source": [
    "# --- HKTicketing Scraper (XPath-only; no fallbacks) ---\n",
    "# Site: https://premier.hkticketing.com/\n",
    "# Output CSV: all_hong_kong.csv (Platform, Title, Date, Location, Link)\n",
    "# pip install selenium tabulate pandas\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import time, os, math, sys\n",
    "from datetime import datetime  # <-- added import for timestamp\n",
    "\n",
    "HOME = \"https://premier.hkticketing.com/\"\n",
    "OUTPUT_CSV = \"all_hong_kong.csv\"\n",
    "\n",
    "# Tunables\n",
    "WAIT = 25\n",
    "PAGELOAD_TIMEOUT = 60\n",
    "SCRIPT_TIMEOUT = 60\n",
    "NAV_RETRIES = 4\n",
    "\n",
    "# Load previous results (for de-dup by Link)\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    try:\n",
    "        existing_df = pd.read_csv(OUTPUT_CSV)\n",
    "        if \"Link\" in existing_df.columns:\n",
    "            seen_links = set(existing_df[\"Link\"].dropna().astype(str))\n",
    "        else:\n",
    "            seen_links = set()\n",
    "    except Exception:\n",
    "        existing_df = pd.DataFrame(columns=[\"Platform\", \"Title\", \"Date\", \"Location\", \"Link\"])\n",
    "        seen_links = set()\n",
    "else:\n",
    "    existing_df = pd.DataFrame(columns=[\"Platform\", \"Title\", \"Date\", \"Location\", \"Link\"])\n",
    "    seen_links = set()\n",
    "\n",
    "# ------------- Driver -------------\n",
    "opts = webdriver.ChromeOptions()\n",
    "opts.add_argument(\"--start-maximized\")\n",
    "opts.add_argument(\"--disable-notifications\")\n",
    "opts.add_argument(\"--disable-popup-blocking\")\n",
    "opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "driver = webdriver.Chrome(options=opts)\n",
    "driver.set_window_size(1400, 1000)\n",
    "driver.set_page_load_timeout(PAGELOAD_TIMEOUT)\n",
    "driver.set_script_timeout(SCRIPT_TIMEOUT)\n",
    "actions = ActionChains(driver)\n",
    "\n",
    "def save_debug(prefix: str):\n",
    "    try:\n",
    "        png = f\"{prefix}_screenshot.png\"\n",
    "        html = f\"{prefix}_source.html\"\n",
    "        driver.save_screenshot(png)\n",
    "        with open(html, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(driver.page_source)\n",
    "        print(f\"‚ö† Saved debug artifacts: {png}, {html}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def js_ready() -> bool:\n",
    "    try:\n",
    "        return driver.execute_script(\"return document.readyState\") == \"complete\"\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def wait_for_home_ready(timeout=WAIT):\n",
    "    end = time.time() + timeout\n",
    "    while time.time() < end:\n",
    "        if js_ready():\n",
    "            try:\n",
    "                if driver.find_elements(By.XPATH, '//*[@id=\"moreEventsSection\"]') or \\\n",
    "                   driver.find_elements(By.XPATH, '//*[@id=\"heroNavPaging\"]'):\n",
    "                    return True\n",
    "            except Exception:\n",
    "                pass\n",
    "        time.sleep(0.3)\n",
    "    return False\n",
    "\n",
    "def get_with_retries(url: str, attempts=NAV_RETRIES) -> None:\n",
    "    last_err = None\n",
    "    for i in range(1, attempts + 1):\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            driver.execute_script(\"window.scrollTo(0, 50);\")\n",
    "            time.sleep(0.3)\n",
    "            if wait_for_home_ready(timeout=WAIT + 5):\n",
    "                return\n",
    "            else:\n",
    "                raise TimeoutException(\"Home not ready within wait window.\")\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            save_debug(f\"load_attempt_{i}\")\n",
    "            sleep_s = min(2 * i, 8)\n",
    "            print(f\"‚ö† Load attempt {i}/{attempts} failed: {e}. Retrying in {sleep_s}s...\")\n",
    "            time.sleep(sleep_s)\n",
    "    raise TimeoutException(f\"Failed to load {url} after {attempts} attempts: {last_err}\")\n",
    "\n",
    "def safe_get_text(by, xp, timeout=WAIT):\n",
    "    el = WebDriverWait(driver, timeout).until(EC.presence_of_element_located((by, xp)))\n",
    "    return el.text.strip()\n",
    "\n",
    "def safe_click(by, xp, timeout=WAIT):\n",
    "    el = WebDriverWait(driver, timeout).until(EC.element_to_be_clickable((by, xp)))\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", el)\n",
    "    time.sleep(0.15)\n",
    "    driver.execute_script(\"arguments[0].click();\", el)\n",
    "\n",
    "rows = []\n",
    "\n",
    "try:\n",
    "    get_with_retries(HOME, attempts=NAV_RETRIES)\n",
    "\n",
    "    idx = 1\n",
    "    while True:\n",
    "        tile_xp = f'//*[@id=\"moreEventsSection\"]/div[{idx}]/div[2]/a'\n",
    "        title_click_xp = f'//*[@id=\"moreEventsSection\"]/div[{idx}]/div[2]/a/div[2]/strong'\n",
    "\n",
    "        try:\n",
    "            tile = WebDriverWait(driver, 4).until(EC.presence_of_element_located((By.XPATH, tile_xp)))\n",
    "        except TimeoutException:\n",
    "            break\n",
    "\n",
    "        link = tile.get_attribute(\"href\") or \"\"\n",
    "        safe_click(By.XPATH, title_click_xp)\n",
    "        time.sleep(0.6)\n",
    "\n",
    "        venue_xp = '//*[@id=\"ctl00_ctl00_uiBodyMain_uiBodyRight_uiPerfSelector_uiPerfSelectorUpdatePanel\"]/div[2]/div[3]/p[1]'\n",
    "        date_xp  = '//*[@id=\"ctl00_ctl00_uiBodyMain_uiBodyRight_uiPerfSelector_uiPerfSelectorUpdatePanel\"]/div[2]/div[3]/p[2]'\n",
    "\n",
    "        try:\n",
    "            venue = safe_get_text(By.XPATH, venue_xp, timeout=WAIT)\n",
    "        except Exception:\n",
    "            venue = \"\"\n",
    "\n",
    "        try:\n",
    "            date_time = safe_get_text(By.XPATH, date_xp, timeout=WAIT)\n",
    "        except Exception:\n",
    "            date_time = \"\"\n",
    "\n",
    "        try:\n",
    "            page_title = driver.title.strip()\n",
    "        except Exception:\n",
    "            page_title = \"\"\n",
    "        title_val = page_title if page_title else f\"Event {idx}\"\n",
    "\n",
    "        if not link:\n",
    "            try:\n",
    "                link = driver.current_url\n",
    "            except Exception:\n",
    "                link = \"\"\n",
    "\n",
    "        if link and link not in seen_links:\n",
    "            rows.append({\n",
    "                \"Platform\": \"HKTicketing\",\n",
    "                \"Title\": title_val,\n",
    "                \"Date\": date_time,\n",
    "                \"Location\": venue,\n",
    "                \"Link\": link,\n",
    "                \"TimeScraped\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # <-- added line\n",
    "            })\n",
    "            seen_links.add(link)\n",
    "\n",
    "        driver.back()\n",
    "        if not wait_for_home_ready(timeout=WAIT + 5):\n",
    "            print(\"‚Ñπ Re-loading home after back navigation...\")\n",
    "            get_with_retries(HOME, attempts=2)\n",
    "        time.sleep(0.4)\n",
    "        idx += 1\n",
    "\n",
    "    safe_click(By.XPATH, '//*[@id=\"heroNavPaging\"]/a[1]')\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    hero_name = safe_get_text(By.XPATH, '//*[@id=\"heroModuleInner\"]/div[2]/div[2]/div[1]/h2/a')\n",
    "\n",
    "    safe_click(By.XPATH, '//*[@id=\"heroModuleInner\"]/div[3]/div[2]/div[2]/a')\n",
    "    WebDriverWait(driver, WAIT).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//*[@id=\"app-scroll-container\"]'))\n",
    "    )\n",
    "    time.sleep(0.6)\n",
    "\n",
    "    hero_venue = safe_get_text(By.XPATH, '//*[@id=\"app-scroll-container\"]/div[1]/div[2]/div/div[2]/div/div/div[2]/div[2]/p[5]')\n",
    "    hero_datetime = safe_get_text(By.XPATH, '//*[@id=\"app-scroll-container\"]/div[1]/div[2]/div/div[2]/div/div/div[2]/div[2]/p[6]')\n",
    "\n",
    "    try:\n",
    "        hero_link = driver.current_url\n",
    "    except Exception:\n",
    "        hero_link = \"\"\n",
    "\n",
    "    if hero_link and hero_link not in seen_links:\n",
    "        rows.append({\n",
    "            \"Platform\": \"HKTicketing\",\n",
    "            \"Title\": hero_name,\n",
    "            \"Date\": hero_datetime,\n",
    "            \"Location\": hero_venue,\n",
    "            \"Link\": hero_link,\n",
    "            \"TimeScraped\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # <-- added line\n",
    "        })\n",
    "        seen_links.add(hero_link)\n",
    "\n",
    "except TimeoutException as e:\n",
    "    print(f\"‚ö† Timeout while loading or scraping: {e}\")\n",
    "    save_debug(\"fatal_timeout\")\n",
    "except NoSuchElementException as e:\n",
    "    print(f\"‚ö† Missing element via provided XPath: {e}\")\n",
    "    save_debug(\"fatal_no_such_element\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Unexpected error: {e}\")\n",
    "    save_debug(\"fatal_error\")\n",
    "finally:\n",
    "    try:\n",
    "        driver.quit()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ---------------- OUTPUT ----------------\n",
    "if rows:\n",
    "    new_df = pd.DataFrame(rows)\n",
    "    cols = [\"Platform\", \"Title\", \"Date\", \"Location\", \"Link\", \"TimeScraped\"]  # <-- ensure column order includes TimeScraped\n",
    "    for c in cols:\n",
    "        if c not in new_df.columns:\n",
    "            new_df[c] = \"\"\n",
    "    new_df = new_df[cols]\n",
    "\n",
    "    if set(cols) - set(existing_df.columns):\n",
    "        for c in cols:\n",
    "            if c not in existing_df.columns:\n",
    "                existing_df[c] = \"\"\n",
    "    existing_df = existing_df[cols]\n",
    "\n",
    "    final_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "else:\n",
    "    final_df = existing_df.copy()\n",
    "\n",
    "if not final_df.empty:\n",
    "    final_df[\"Link\"] = final_df[\"Link\"].astype(str).str.strip()\n",
    "    final_df = final_df.drop_duplicates(subset=[\"Link\"], keep=\"first\")\n",
    "\n",
    "print(f\"\\n‚úÖ TOTAL Unique Events: {len(final_df)}\")\n",
    "print(tabulate(final_df.tail(20), headers=\"keys\", tablefmt=\"github\", showindex=False))\n",
    "\n",
    "final_df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\nüíæ CSV saved: {OUTPUT_CSV}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbf37f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import csv\n",
    "# import time\n",
    "# from datetime import datetime, timezone\n",
    "\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "# URL = \"https://www.macstadiumhkpa.com/en/%E6%9C%80%E6%96%B0%E6%B4%BB%E5%8B%95\"\n",
    "# CSV_FILE = \"all_events_hk.csv\"\n",
    "# EVENT_LOCATION = \"MacPherson Stadium Hong Kong\"\n",
    "\n",
    "\n",
    "# def build_driver(headless: bool = True):\n",
    "#     chrome_options = Options()\n",
    "#     if headless:\n",
    "#         chrome_options.add_argument(\"--headless=new\")\n",
    "#     chrome_options.add_argument(\"--disable-gpu\")\n",
    "#     chrome_options.add_argument(\"--no-sandbox\")\n",
    "#     chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "#     chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "#     driver = webdriver.Chrome(options=chrome_options)\n",
    "#     return driver\n",
    "\n",
    "\n",
    "# def wait_for_page_and_lazy_load(driver):\n",
    "#     wait = WebDriverWait(driver, 20)\n",
    "\n",
    "#     # Wait until some event name element is present (using the pattern of the ID you gave)\n",
    "#     wait.until(\n",
    "#         EC.presence_of_element_located(\n",
    "#             (\n",
    "#                 By.XPATH,\n",
    "#                 '//*[starts-with(@id, \"comp-kycrouab1__item-\")]/p/span'\n",
    "#             )\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     # Lazy-load: scroll until the bottom height stops changing\n",
    "#     last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#     while True:\n",
    "#         driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#         time.sleep(2)  # give time for Wix/lazy content to load\n",
    "#         new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#         if new_height == last_height:\n",
    "#             break\n",
    "#         last_height = new_height\n",
    "\n",
    "\n",
    "# def load_existing_name_url_keys(csv_path):\n",
    "#     \"\"\"\n",
    "#     Returns a set of (event_name, event_url) pairs that already exist in the CSV.\n",
    "#     Works even if older CSV rows don't have an event_url column.\n",
    "#     \"\"\"\n",
    "#     existing_keys = set()\n",
    "#     if not os.path.exists(csv_path):\n",
    "#         return existing_keys\n",
    "\n",
    "#     with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "#         reader = csv.DictReader(f)\n",
    "#         for row in reader:\n",
    "#             name = (row.get(\"event_name\") or \"\").strip()\n",
    "#             url = (row.get(\"event_url\") or \"\").strip()\n",
    "#             if name or url:\n",
    "#                 existing_keys.add((name, url))\n",
    "#     return existing_keys\n",
    "\n",
    "\n",
    "# def save_events_to_csv(csv_path, events):\n",
    "#     \"\"\"\n",
    "#     Append new events to the CSV, ensuring:\n",
    "#     - Header is written if file does not exist.\n",
    "#     - Existing rows are kept.\n",
    "#     \"\"\"\n",
    "#     file_exists = os.path.exists(csv_path)\n",
    "#     fieldnames = [\n",
    "#         \"event_name\",\n",
    "#         \"event_date\",\n",
    "#         \"event_location\",\n",
    "#         \"event_url\",\n",
    "#         \"time_scraped_utc\",\n",
    "#     ]\n",
    "\n",
    "#     with open(csv_path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "#         writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "#         if not file_exists:\n",
    "#             writer.writeheader()\n",
    "#         for ev in events:\n",
    "#             writer.writerow(ev)\n",
    "\n",
    "\n",
    "# def scrape_events(driver):\n",
    "#     driver.get(URL)\n",
    "#     wait_for_page_and_lazy_load(driver)\n",
    "\n",
    "#     # Event names\n",
    "#     name_elements = driver.find_elements(\n",
    "#         By.XPATH,\n",
    "#         '//*[starts-with(@id, \"comp-kycrouab1__item-\")]/p/span'\n",
    "#     )\n",
    "\n",
    "#     # Event dates\n",
    "#     date_elements = driver.find_elements(\n",
    "#         By.XPATH,\n",
    "#         '//*[starts-with(@id, \"comp-kycrouav__item-\")]/p/span/span/span/span/span'\n",
    "#     )\n",
    "\n",
    "#     # Event URLs: \"Details\" links\n",
    "#     url_elements = driver.find_elements(\n",
    "#         By.XPATH,\n",
    "#         '//a[contains(normalize-space(text()), \"Details\")]'\n",
    "#     )\n",
    "\n",
    "#     event_names = [el.text.strip() for el in name_elements if el.text.strip()]\n",
    "#     event_dates = [el.text.strip() for el in date_elements if el.text.strip()]\n",
    "#     event_urls = [el.get_attribute(\"href\").strip() for el in url_elements if el.get_attribute(\"href\")]\n",
    "\n",
    "#     # Make sure we only pair up to the shortest list length\n",
    "#     n = min(len(event_names), len(event_dates), len(event_urls))\n",
    "#     event_names = event_names[:n]\n",
    "#     event_dates = event_dates[:n]\n",
    "#     event_urls = event_urls[:n]\n",
    "\n",
    "#     # Existing (name, url) combos from CSV\n",
    "#     existing_keys = load_existing_name_url_keys(CSV_FILE)\n",
    "\n",
    "#     new_events = []\n",
    "#     current_utc = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "#     # Also de-dupe within this run\n",
    "#     seen_this_run = set()\n",
    "\n",
    "#     for name, date_str, url in zip(event_names, event_dates, event_urls):\n",
    "#         norm_name = name.strip()\n",
    "#         norm_url = url.strip()\n",
    "\n",
    "#         key = (norm_name, norm_url)\n",
    "\n",
    "#         # Skip if already in CSV or already added this run\n",
    "#         if key in existing_keys or key in seen_this_run:\n",
    "#             continue\n",
    "\n",
    "#         seen_this_run.add(key)\n",
    "\n",
    "#         new_events.append(\n",
    "#             {\n",
    "#                 \"event_name\": norm_name,\n",
    "#                 \"event_date\": date_str.strip(),\n",
    "#                 \"event_location\": EVENT_LOCATION,\n",
    "#                 \"event_url\": norm_url,\n",
    "#                 \"time_scraped_utc\": current_utc,\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#     return new_events\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     driver = build_driver(headless=True)\n",
    "#     try:\n",
    "#         new_events = scrape_events(driver)\n",
    "#         if new_events:\n",
    "#             save_events_to_csv(CSV_FILE, new_events)\n",
    "#             print(f\"Added {len(new_events)} new events to {CSV_FILE}.\")\n",
    "#         else:\n",
    "#             print(\"No new events found. CSV unchanged.\")\n",
    "#     finally:\n",
    "#         driver.quit()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438694ca",
   "metadata": {},
   "source": [
    "# Jarkata (done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38fcd55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 7 new events to all_jakarta.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "\n",
    "URL = \"https://megatix.co.id/madaboutcomedy\"\n",
    "CSV_FILE = \"all_jakarta.csv\"\n",
    "\n",
    "\n",
    "def build_driver(headless: bool = False):  # show browser window so site ‚Äúpops up‚Äù\n",
    "    chrome_options = Options()\n",
    "\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless=new\")\n",
    "\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--window-size=1400,1000\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def wait_for_page_and_lazy_load(driver):\n",
    "    \"\"\"\n",
    "    Wait for at least one event name to appear, then scroll\n",
    "    to allow lazy-loaded content to load.\n",
    "    \"\"\"\n",
    "    wait = WebDriverWait(driver, 25)\n",
    "\n",
    "    try:\n",
    "        # Wait for any event name element, not a super-deep static path\n",
    "        wait.until(\n",
    "            EC.presence_of_element_located(\n",
    "                (\n",
    "                    By.XPATH,\n",
    "                    '//*[@id=\"megatix\"]/div/main/div/div[2]/div[1]'\n",
    "                    '//div/div[2]/div[2]/div'\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        print(\"Warning: event name element did not load in time.\")\n",
    "        return\n",
    "\n",
    "    # Lazy-load scroll loop\n",
    "    last_height = 0\n",
    "    for _ in range(5):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1.5)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "\n",
    "def load_existing_event_names(csv_path):\n",
    "    \"\"\"\n",
    "    Load existing event names from CSV to avoid duplicates on re-runs.\n",
    "    \"\"\"\n",
    "    existing_names = set()\n",
    "    if not os.path.exists(csv_path):\n",
    "        return existing_names\n",
    "\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            name = (row.get(\"event_name\") or \"\").strip()\n",
    "            if name:\n",
    "                existing_names.add(name)\n",
    "    return existing_names\n",
    "\n",
    "\n",
    "def save_events_to_csv(csv_path, events):\n",
    "    \"\"\"\n",
    "    Append new events to CSV, preserving existing rows.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.exists(csv_path)\n",
    "    fieldnames = [\"event_name\", \"event_date\", \"event_location\", \"time_scraped_utc\"]\n",
    "\n",
    "    with open(csv_path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        for ev in events:\n",
    "            writer.writerow(ev)\n",
    "\n",
    "\n",
    "def scrape_events(driver):\n",
    "    driver.get(URL)\n",
    "    wait_for_page_and_lazy_load(driver)\n",
    "\n",
    "    # Find ALL event name elements on the page (not just one card)\n",
    "    name_elements = driver.find_elements(\n",
    "        By.XPATH,\n",
    "        '//*[@id=\"megatix\"]/div/main/div/div[2]/div[1]'\n",
    "        '//div/div[2]/div[2]/div'\n",
    "    )\n",
    "\n",
    "    existing_names = load_existing_event_names(CSV_FILE)\n",
    "    seen_this_run = set()\n",
    "    new_events = []\n",
    "    current_utc = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    for name_elem in name_elements:\n",
    "        try:\n",
    "            event_name = name_elem.text.strip()\n",
    "        except Exception:\n",
    "            event_name = \"\"\n",
    "\n",
    "        if not event_name:\n",
    "            continue\n",
    "\n",
    "        # Skip duplicates based on event_name\n",
    "        if event_name in existing_names or event_name in seen_this_run:\n",
    "            continue\n",
    "\n",
    "        # From the name element, get the info container (div[2] that holds date/name/location)\n",
    "        # name is at: card/div[2]/div[2]/div\n",
    "        # so container is: ../..  (div[2])\n",
    "        try:\n",
    "            info_container = name_elem.find_element(By.XPATH, './../..')\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # Date: ./div[1]/span inside info_container\n",
    "        try:\n",
    "            date_elem = info_container.find_element(By.XPATH, './div[1]/span')\n",
    "            event_date = date_elem.text.strip()\n",
    "        except Exception:\n",
    "            event_date = \"\"\n",
    "\n",
    "        # Location: ./div[3]/span inside info_container\n",
    "        try:\n",
    "            loc_elem = info_container.find_element(By.XPATH, './div[3]/span')\n",
    "            event_location = loc_elem.text.strip()\n",
    "        except Exception:\n",
    "            event_location = \"\"\n",
    "\n",
    "        seen_this_run.add(event_name)\n",
    "\n",
    "        new_events.append(\n",
    "            {\n",
    "                \"event_name\": event_name,\n",
    "                \"event_date\": event_date,\n",
    "                \"event_location\": event_location,\n",
    "                \"time_scraped_utc\": current_utc,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return new_events\n",
    "\n",
    "\n",
    "def main():\n",
    "    driver = build_driver(headless=False)  # visible window (popup)\n",
    "    try:\n",
    "        new_events = scrape_events(driver)\n",
    "        if new_events:\n",
    "            save_events_to_csv(CSV_FILE, new_events)\n",
    "            print(f\"Added {len(new_events)} new events to {CSV_FILE}.\")\n",
    "        else:\n",
    "            print(\"No new events found. CSV unchanged.\")\n",
    "\n",
    "        # Optional: keep browser open briefly so you can see it\n",
    "        time.sleep(5)\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b7d7054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ Found 7 featured cards\n",
      "‚úÖ Megatix Indonesia (Featured): grabbed 7 events\n",
      "\n",
      "‚Ñπ No new events added since last run.\n",
      "\n",
      "‚úÖ TOTAL Raw Events: 7\n",
      "| Title                                       | Date             | Location          | Link                                                  | TimeScraped         |\n",
      "|---------------------------------------------|------------------|-------------------|-------------------------------------------------------|---------------------|\n",
      "| BEN B√ñHMER (LIVE)                           | Sat, 27 Dec 2025 | Savaya Bali       | https://megatix.co.id/events/ben-bohmer-2712          | 2025-12-15 16:29:04 |\n",
      "| BEN B√ñHMER (LIVE)                           | Sat, 27 Dec 2025 | Savaya Bali       | https://megatix.co.id/events/ben-bohmer-2712          | 2025-12-15 16:29:04 |\n",
      "| X-Clusive Presents: Bali NYE 2025 with Tyga | Wed, 31 Dec 2025 | The Stage         | https://megatix.co.id/events/nye-2025-with-tyga       | 2025-12-15 16:29:04 |\n",
      "| ARTBAT & MISS MONIQUE                       | Mon, 29 Dec 2025 | Savaya Bali       | https://megatix.co.id/events/artbat-miss-monique-2912 | 2025-12-15 16:29:05 |\n",
      "| RUDIMENTAL                                  | Fri, 02 Jan 2026 | Atlas Beach Club  | https://megatix.co.id/events/rudimental-2026          | 2025-12-15 16:29:05 |\n",
      "| NEW YEAR'S EVE 2025 | MESA BALI             | Wed, 31 Dec 2025 | The Mesa Bali     | https://megatix.co.id/events/nye-mesa-2025            | 2025-12-15 16:29:05 |\n",
      "| NEW YEAR'S EVE PARTY AT W BALI - SEMINYAK   | Wed, 31 Dec 2025 | W Bali - Seminyak | https://megatix.co.id/events/NYE-W-BALI-SEMINYAK      | 2025-12-15 16:29:05 |\n",
      "\n",
      "üíæ CSV saved: all_jakarta.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Megatix Indonesia: Featured Events Scraper ---\n",
    "# Output CSV: all_jakarta.csv\n",
    "# Works for https://megatix.co.id/\n",
    "# pip install selenium tabulate pandas\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import time\n",
    "import os  # <-- added to read existing CSV for \"new events\" comparison\n",
    "from datetime import datetime  # <-- added for timestamp\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "OUTPUT_CSV = \"all_jakarta.csv\"\n",
    "URL = \"https://megatix.co.id/\"\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.set_window_size(1400, 1000)\n",
    "all_rows = []\n",
    "\n",
    "# ---------------- SCRAPER ----------------\n",
    "try:\n",
    "    driver.get(URL)\n",
    "\n",
    "    # Wait for base element\n",
    "    WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='__nuxt']\")))\n",
    "    time.sleep(1.0)\n",
    "\n",
    "    FEATURED_WRAP_X = \"//*[@id='__nuxt']/div/div[3]/div/main/section/section[2]/div[2]/div\"\n",
    "    FEATURED_CARD_X = \"//*[@id='__nuxt']/div/div[3]/div/main/section/section[2]/div[2]//article[.//h3]\"\n",
    "\n",
    "    # Scroll and load all featured cards\n",
    "    prev = -1\n",
    "    stable = 0\n",
    "    for _ in range(60):\n",
    "        cards = driver.find_elements(By.XPATH, FEATURED_CARD_X)\n",
    "        count = len(cards)\n",
    "        if count == prev:\n",
    "            stable += 1\n",
    "            if stable >= 2:\n",
    "                break\n",
    "        else:\n",
    "            stable = 0\n",
    "        prev = count\n",
    "        driver.execute_script(\"window.scrollBy(0, 900);\")\n",
    "        time.sleep(1.2)\n",
    "\n",
    "    print(f\"üü¢ Found {len(driver.find_elements(By.XPATH, FEATURED_CARD_X))} featured cards\")\n",
    "\n",
    "    for ev in driver.find_elements(By.XPATH, FEATURED_CARD_X):\n",
    "        try:\n",
    "            title = ev.find_element(By.XPATH, \".//h3/span\").text.strip()\n",
    "        except:\n",
    "            title = \"No Title\"\n",
    "        try:\n",
    "            date = ev.find_element(By.XPATH, \".//div[1]/div[1]/span\").text.strip()\n",
    "        except:\n",
    "            date = \"No Date\"\n",
    "        try:\n",
    "            location = ev.find_element(By.XPATH, \".//div[1]/div[3]/span\").text.strip()\n",
    "        except:\n",
    "            location = \"Indonesia\"\n",
    "        try:\n",
    "            link = ev.find_element(By.XPATH, \".//ancestor::a\").get_attribute(\"href\") or \"No Link\"\n",
    "        except:\n",
    "            link = \"No Link\"\n",
    "\n",
    "        all_rows.append({\n",
    "            \"Title\": title,\n",
    "            \"Date\": date,\n",
    "            \"Location\": location,\n",
    "            \"Link\": link,\n",
    "            \"TimeScraped\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # <-- added line\n",
    "        })\n",
    "\n",
    "    print(f\"‚úÖ Megatix Indonesia (Featured): grabbed {len(all_rows)} events\")\n",
    "\n",
    "except TimeoutException:\n",
    "    print(\"‚ö† Timeout waiting for Megatix Indonesia page\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "# ---------------- OUTPUT ----------------\n",
    "df = pd.DataFrame(all_rows).reset_index(drop=True)\n",
    "\n",
    "# --- NEW: Compare with existing CSV and print any newly added events ---\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    try:\n",
    "        old_df = pd.read_csv(OUTPUT_CSV)\n",
    "        old_links = set(old_df.get(\"Link\", pd.Series(dtype=str)).astype(str).str.strip())\n",
    "    except Exception:\n",
    "        old_links = set()\n",
    "else:\n",
    "    old_links = set()\n",
    "\n",
    "# Normalize links for comparison and detect new ones\n",
    "if not df.empty:\n",
    "    df[\"Link\"] = df[\"Link\"].astype(str).str.strip()\n",
    "    new_mask = ~df[\"Link\"].isin(old_links)\n",
    "    new_events = df[new_mask].copy()\n",
    "    if len(new_events) > 0:\n",
    "        print(f\"\\nüÜï {len(new_events)} new event(s) since last run:\")\n",
    "        for _, r in new_events.iterrows():\n",
    "            # TimeScraped will also show here automatically since it's in the row\n",
    "            print(f\"  ‚Ä¢ {r['Title']} | {r['Date']} | {r['Location']} | {r['Link']} | {r['TimeScraped']}\")\n",
    "    else:\n",
    "        print(\"\\n‚Ñπ No new events added since last run.\")\n",
    "else:\n",
    "    print(\"\\n‚Ñπ No events scraped this run (empty listing).\")\n",
    "# --- END NEW ---\n",
    "\n",
    "print(f\"\\n‚úÖ TOTAL Raw Events: {len(df)}\")\n",
    "print(tabulate(df, headers=\"keys\", tablefmt=\"github\", showindex=False))\n",
    "\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\nüíæ CSV saved: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3321c2b6",
   "metadata": {},
   "source": [
    "# Bangkok (done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cc41d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # thai ticket major\n",
    "# # pip install selenium pandas\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "# from datetime import datetime\n",
    "# import pandas as pd\n",
    "# import time\n",
    "\n",
    "# HOME = \"https://www.thaiticketmajor.com/concert/\"\n",
    "\n",
    "# # ---- Your detail-page XPaths (kept as-is) ----\n",
    "# NAME_X  = \"/html/body/div[1]/main/div[1]/div[2]/div/div[2]/div/div[1]/div/h1/font/font\"\n",
    "# VENUE_X = \"/html/body/div[1]/main/div[1]/div[2]/div/div[2]/div/div[2]/div[1]/ul/li[2]/p/span/font/font\"\n",
    "# PRICE_X = \"/html/body/div[1]/main/div[1]/div[2]/div/div[2]/div/div[2]/div[2]/ul/li[2]/div\"\n",
    "\n",
    "# # Listing: we‚Äôll collect all anchors in section[2] (your click path lives under here)\n",
    "# # This is more reliable than clicking the nested /font/font node.\n",
    "# LIST_ANCHORS_X = \"//body/div[1]/main/section[2]//a[@href]\"\n",
    "\n",
    "# # ================== APPEND+DEDUP ADD-ON (no changes to your code below) ==================\n",
    "# import atexit, os, re\n",
    "\n",
    "# _OUTPUT_CSV_PATH = \"all_bangkok.csv\"\n",
    "\n",
    "# def _normalize_link(u: str) -> str:\n",
    "#     u = (u or \"\").strip().lower()\n",
    "#     u = re.sub(r\"#.*$\", \"\", u)           # drop fragments\n",
    "#     u = re.sub(r\"\\?.*$\", \"\", u)          # drop query params\n",
    "#     return u.rstrip(\"/\")\n",
    "\n",
    "# try:\n",
    "#     _old_df_ttm = pd.read_csv(_OUTPUT_CSV_PATH)\n",
    "# except Exception:\n",
    "#     _old_df_ttm = pd.DataFrame()\n",
    "\n",
    "# def _merge_back_ttm():\n",
    "#     \"\"\"Runs AFTER your script's own to_csv.\n",
    "#        - Restores previous rows\n",
    "#        - Removes duplicates\n",
    "#        - Prints which events are newly added this run (or none)\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         new_df = pd.read_csv(_OUTPUT_CSV_PATH)\n",
    "#     except Exception:\n",
    "#         # If the fresh write failed, nothing to merge\n",
    "#         return\n",
    "\n",
    "#     # Ensure consistent columns even if earlier files differ\n",
    "#     need_cols = [\"Event Name\", \"Venue\", \"Ticket Prices\", \"Link\", \"Scraped At (UTC)\"]\n",
    "#     for df in (_old_df_ttm, new_df):\n",
    "#         for c in need_cols:\n",
    "#             if c not in df.columns:\n",
    "#                 df[c] = \"\"\n",
    "\n",
    "#     # Normalize links for strong dedup\n",
    "#     _old_df_ttm[\"LinkNorm\"] = _old_df_ttm[\"Link\"].map(_normalize_link)\n",
    "#     new_df[\"LinkNorm\"]      = new_df[\"Link\"].map(_normalize_link)\n",
    "\n",
    "#     # --- Identify which rows are NEW vs the old file (before merging) ---\n",
    "#     old_keys = set(_old_df_ttm[\"LinkNorm\"].astype(str))\n",
    "#     # For rows with empty LinkNorm, fallback to (Event Name, Venue)\n",
    "#     if \"\" in old_keys:\n",
    "#         old_fallback = set(zip(\n",
    "#             _old_df_ttm[\"Event Name\"].astype(str).str.strip().str.lower(),\n",
    "#             _old_df_ttm[\"Venue\"].astype(str).str.strip().str.lower()\n",
    "#         ))\n",
    "#     else:\n",
    "#         old_fallback = set()\n",
    "\n",
    "#     newly_added_rows = []\n",
    "#     for _, r in new_df.iterrows():\n",
    "#         ln = str(r.get(\"LinkNorm\", \"\") or \"\")\n",
    "#         if ln:\n",
    "#             if ln not in old_keys:\n",
    "#                 newly_added_rows.append(r)\n",
    "#         else:\n",
    "#             key = (\n",
    "#                 str(r.get(\"Event Name\",\"\")).strip().lower(),\n",
    "#                 str(r.get(\"Venue\",\"\")).strip().lower()\n",
    "#             )\n",
    "#             if key not in old_fallback:\n",
    "#                 newly_added_rows.append(r)\n",
    "\n",
    "#     # Merge and dedup\n",
    "#     merged = pd.concat([_old_df_ttm, new_df], ignore_index=True)\n",
    "\n",
    "#     # Primary dedup by normalized link (keep first = preserve earliest row)\n",
    "#     merged = merged.drop_duplicates(subset=[\"LinkNorm\"], keep=\"first\")\n",
    "\n",
    "#     # Secondary guard when links are missing/unstable\n",
    "#     merged = merged.drop_duplicates(subset=[\"Event Name\", \"Venue\"], keep=\"first\")\n",
    "\n",
    "#     # Persist without helper column\n",
    "#     merged = merged.drop(columns=[\"LinkNorm\"], errors=\"ignore\")\n",
    "#     merged.to_csv(_OUTPUT_CSV_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "#     # --- Print summary of new events ---\n",
    "#     if newly_added_rows:\n",
    "#         print(f\"\\nüÜï {len(newly_added_rows)} new event(s) added this run:\")\n",
    "#         for r in newly_added_rows:\n",
    "#             title = str(r.get(\"Event Name\",\"\")).strip()\n",
    "#             venue = str(r.get(\"Venue\",\"\")).strip()\n",
    "#             link  = str(r.get(\"Link\",\"\")).strip()\n",
    "#             date  = str(r.get(\"Scraped At (UTC)\",\"\")).strip()\n",
    "#             print(f\"  ‚Ä¢ {title} | {venue} | {link} | scraped {date}\")\n",
    "#     else:\n",
    "#         print(\"\\n‚Ñπ No new events added this run.\")\n",
    "\n",
    "# # Ensure merge runs after your script finishes writing the CSV\n",
    "# atexit.register(_merge_back_ttm)\n",
    "# # ================== END ADD-ON ==================\n",
    "\n",
    "# def build_driver():\n",
    "#     opts = webdriver.ChromeOptions()\n",
    "#     opts.add_argument(\"--start-maximized\")\n",
    "#     opts.add_argument(\"--disable-notifications\")\n",
    "#     opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "#     opts.page_load_strategy = \"eager\"\n",
    "#     d = webdriver.Chrome(options=opts)\n",
    "#     d.set_page_load_timeout(90)\n",
    "#     return d\n",
    "\n",
    "# def wait(drv, xp, sec=25, clickable=False):\n",
    "#     cond = EC.element_to_be_clickable if clickable else EC.presence_of_element_located\n",
    "#     return WebDriverWait(drv, sec).until(cond((By.XPATH, xp)))\n",
    "\n",
    "# def t(drv, xp):\n",
    "#     try:\n",
    "#         el = drv.find_element(By.XPATH, xp)\n",
    "#         # textContent is safer with nested <font> etc.\n",
    "#         return \" \".join((el.get_attribute(\"textContent\") or \"\").split())\n",
    "#     except:\n",
    "#         return \"\"\n",
    "\n",
    "# def main():\n",
    "#     driver = build_driver()\n",
    "#     rows = []\n",
    "#     try:\n",
    "#         driver.get(HOME)\n",
    "#         # Wait for listing anchors to exist\n",
    "#         wait(driver, LIST_ANCHORS_X)\n",
    "#         time.sleep(0.5)\n",
    "\n",
    "#         anchors = driver.find_elements(By.XPATH, LIST_ANCHORS_X)\n",
    "#         hrefs = []\n",
    "#         for a in anchors:\n",
    "#             href = a.get_attribute(\"href\")\n",
    "#             # Keep only event-detail-like links (most are /event/... or /concert/...)\n",
    "#             if href and \"thaiticketmajor.com\" in href and href not in hrefs:\n",
    "#                 hrefs.append(href)\n",
    "\n",
    "#         if not hrefs:\n",
    "#             print(\"No events found on the listing.\")\n",
    "#             return\n",
    "\n",
    "#         for i, href in enumerate(hrefs, 1):\n",
    "#             try:\n",
    "#                 driver.get(href)\n",
    "#                 # Wait for name node to appear\n",
    "#                 wait(driver, NAME_X, sec=20)\n",
    "\n",
    "#                 name  = t(driver, NAME_X)\n",
    "#                 venue = t(driver, VENUE_X)\n",
    "#                 price = t(driver, PRICE_X)\n",
    "\n",
    "#                 rows.append({\n",
    "#                     \"Event Name\": name,\n",
    "#                     \"Venue\": venue,\n",
    "#                     \"Ticket Prices\": price,\n",
    "#                     \"Link\": href,\n",
    "#                     \"Scraped At (UTC)\": datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "#                 })\n",
    "\n",
    "#                 # If you strictly want to ‚Äúgo back and click the rest‚Äù, uncomment these two lines:\n",
    "#                 # driver.back()\n",
    "#                 # wait(driver, LIST_ANCHORS_X)\n",
    "\n",
    "#             except (TimeoutException, StaleElementReferenceException):\n",
    "#                 rows.append({\n",
    "#                     \"Event Name\": \"\",\n",
    "#                     \"Venue\": \"\",\n",
    "#                     \"Ticket Prices\": \"\",\n",
    "#                     \"Link\": href,\n",
    "#                     \"Scraped At (UTC)\": datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "#                 })\n",
    "#                 # Try to return to list if something failed\n",
    "#                 try:\n",
    "#                     driver.get(HOME)\n",
    "#                     wait(driver, LIST_ANCHORS_X)\n",
    "#                 except TimeoutException:\n",
    "#                     pass\n",
    "\n",
    "#         pd.DataFrame(rows).to_csv(\"all_bangkok.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "#         print(f\"Saved {len(rows)} rows to all_bangkok.csv\")\n",
    "\n",
    "#     finally:\n",
    "#         try: driver.quit()\n",
    "#         except: pass\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "\n",
    "\n",
    "# issue with scraping\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c18524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ticketmelon (all asia)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e98151d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4 rows to all_bangkok.csv\n",
      "\n",
      "üÜï 3 new event(s) added this run:\n",
      "  ‚Ä¢ nan | nan | nan | https://www.livenationtero.co.th/en/event/babymonster-love-monsters-asia-fan-concert-2025-in-bangkok-bangkok-tickets-edp1628752 | scraped 2025-12-15 08:29:31\n",
      "  ‚Ä¢ nan | nan | nan | https://www.livenationtero.co.th/en/event/giv%C4%93on-dear-beloved-the-tour-bangkok-tickets-edp1636493 | scraped 2025-12-15 08:29:42\n",
      "  ‚Ä¢ nan | nan | üèüÔ∏è: Lido Connect 3, Thailand | https://www.livenationtero.co.th/en/event/pryvt-back-to-reality-world-tour-in-bangkok-bangkok-tickets-edp1637655 | scraped 2025-12-15 08:29:46\n",
      "\n",
      "üìä Summary: old=307, scraped_this_run=4, deduped_total=305, duplicates_removed=6\n",
      "‚ö† Warning: merged row count is less than previous file. (This should not happen.)\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "URL = \"https://www.livenationtero.co.th/en?utm_source=chatgpt.com\"\n",
    "\n",
    "LIST_X   = \"//*[@id='upcoming-shows']/div/ul\"\n",
    "ITEM_X   = LIST_X + \"/li[{i}]\"\n",
    "ANCHOR_X = ITEM_X + \"/a\"\n",
    "NAME_X   = ITEM_X + \"/a/p[1]\"      # event name\n",
    "DATE_X   = ITEM_X + \"/a/small\"     # event date  ‚úÖ\n",
    "\n",
    "VENUE_X_PRIMARY = \"//*[@id='main']/div/div[1]/div/div[2]/div/p[4]\"\n",
    "VENUE_FALLBACKS = [\n",
    "    \"//*[@id='main']//p[contains(., 'Venue')]/following-sibling::p[1]\",\n",
    "    \"//*[@id='main']//div[contains(@class,'event') or contains(@class,'details')]//p[contains(@class,'venue')]\",\n",
    "    \"//*[contains(@class,'EventDetails')]//p[contains(@class,'venue')]\",\n",
    "]\n",
    "\n",
    "# ================== APPEND + DEDUP ADD-ON (ONLY ADDITIONS BELOW) ==================\n",
    "import os, re, atexit\n",
    "\n",
    "_OUTPUT_CSV_PATH = \"all_bangkok.csv\"\n",
    "\n",
    "def _normalize_link(u: str) -> str:\n",
    "    u = (u or \"\").strip().lower()\n",
    "    u = re.sub(r\"#.*$\", \"\", u)          # drop fragments\n",
    "    u = re.sub(r\"\\?.*$\", \"\", u)         # drop query params\n",
    "    return u.rstrip(\"/\")\n",
    "\n",
    "# Preload old CSV (if any) so we can merge it back after your own to_csv runs\n",
    "try:\n",
    "    _old_df_lnt = pd.read_csv(_OUTPUT_CSV_PATH)\n",
    "except Exception:\n",
    "    _old_df_lnt = pd.DataFrame()\n",
    "\n",
    "def _merge_back_lnt():\n",
    "    \"\"\"Runs AFTER your script writes all_bangkok.csv.\n",
    "       Merges old + new, removes duplicates, saves back to file,\n",
    "       and prints which events are newly added (or none).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        new_df = pd.read_csv(_OUTPUT_CSV_PATH)\n",
    "    except Exception:\n",
    "        return  # nothing written, nothing to merge\n",
    "\n",
    "    # Ensure the expected schema exists on both sides\n",
    "    need_cols = [\"Event Name\", \"Event Date\", \"Venue\", \"Link\", \"Scraped At (UTC)\"]\n",
    "    for df in (_old_df_lnt, new_df):\n",
    "        for c in need_cols:\n",
    "            if c not in df.columns:\n",
    "                df[c] = \"\"\n",
    "\n",
    "    # Normalize links for robust dedup (handles tracking params, trailing slashes)\n",
    "    _old_df_lnt[\"LinkNorm\"] = _old_df_lnt[\"Link\"].map(_normalize_link)\n",
    "    new_df[\"LinkNorm\"]      = new_df[\"Link\"].map(_normalize_link)\n",
    "\n",
    "    # ---- Determine which rows are NEW compared to old file ----\n",
    "    old_linknorms = set(_old_df_lnt[\"LinkNorm\"].astype(str))\n",
    "    # Fallback identity if Link is blank/unstable: (Event Name, Venue)\n",
    "    old_fallback_keys = set(zip(\n",
    "        _old_df_lnt[\"Event Name\"].astype(str).str.strip().str.lower(),\n",
    "        _old_df_lnt[\"Venue\"].astype(str).str.strip().str.lower()\n",
    "    ))\n",
    "\n",
    "    newly_added_rows = []\n",
    "    for _, r in new_df.iterrows():\n",
    "        ln = str(r.get(\"LinkNorm\", \"\") or \"\")\n",
    "        if ln:\n",
    "            if ln not in old_linknorms:\n",
    "                newly_added_rows.append(r)\n",
    "        else:\n",
    "            key = (\n",
    "                str(r.get(\"Event Name\",\"\")).strip().lower(),\n",
    "                str(r.get(\"Venue\",\"\")).strip().lower()\n",
    "            )\n",
    "            if key not in old_fallback_keys:\n",
    "                newly_added_rows.append(r)\n",
    "\n",
    "    # ---- Merge & deduplicate globally ----\n",
    "    merged = pd.concat([_old_df_lnt, new_df], ignore_index=True)\n",
    "    merged = merged.drop_duplicates(subset=[\"LinkNorm\"], keep=\"first\")\n",
    "    merged = merged.drop_duplicates(subset=[\"Event Name\", \"Venue\"], keep=\"first\")\n",
    "\n",
    "    # ---- Summary & guarantees ----\n",
    "    old_count = len(_old_df_lnt)\n",
    "    new_raw_count = len(new_df)\n",
    "    merged_count = len(merged)\n",
    "    dups_dropped = (old_count + new_raw_count) - merged_count\n",
    "\n",
    "    # Backfill missing timestamps for older rows that didn't have it\n",
    "    if \"Scraped At (UTC)\" in merged.columns:\n",
    "        merged[\"Scraped At (UTC)\"] = (\n",
    "            merged[\"Scraped At (UTC)\"]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .replace({\"nan\": \"\", \"None\": \"\"})\n",
    "        )\n",
    "        merged[\"Scraped At (UTC)\"] = merged[\"Scraped At (UTC)\"].mask(\n",
    "            merged[\"Scraped At (UTC)\"] == \"\",\n",
    "            datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        )\n",
    "\n",
    "    # Persist without helper column\n",
    "    merged = merged.drop(columns=[\"LinkNorm\"], errors=\"ignore\")\n",
    "    merged.to_csv(_OUTPUT_CSV_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # ---- Print summary of new events (or none) ----\n",
    "    if newly_added_rows:\n",
    "        print(f\"\\nüÜï {len(newly_added_rows)} new event(s) added this run:\")\n",
    "        for r in newly_added_rows:\n",
    "            title = str(r.get(\"Event Name\",\"\")).strip()\n",
    "            date  = str(r.get(\"Event Date\",\"\")).strip()\n",
    "            venue = str(r.get(\"Venue\",\"\")).strip()\n",
    "            link  = str(r.get(\"Link\",\"\")).strip()\n",
    "            when  = str(r.get(\"Scraped At (UTC)\",\"\")).strip()\n",
    "            print(f\"  ‚Ä¢ {title} | {date} | {venue} | {link} | scraped {when}\")\n",
    "    else:\n",
    "        print(\"\\n‚Ñπ No new events added this run.\")\n",
    "\n",
    "    print(f\"\\nüìä Summary: old={old_count}, scraped_this_run={new_raw_count}, \"\n",
    "          f\"deduped_total={merged_count}, duplicates_removed={dups_dropped}\")\n",
    "\n",
    "    if merged_count < old_count:\n",
    "        print(\"‚ö† Warning: merged row count is less than previous file. (This should not happen.)\")\n",
    "    else:\n",
    "        print(\"‚úÖ No previously saved events were removed. Duplicates were removed correctly.\")\n",
    "\n",
    "# Register the merge so it runs automatically at program exit\n",
    "atexit.register(_merge_back_lnt)\n",
    "# ================== END ADD-ON ==================\n",
    "\n",
    "def build_driver():\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    opts.add_argument(\"--start-maximized\")\n",
    "    opts.add_argument(\"--disable-notifications\")\n",
    "    return webdriver.Chrome(options=opts)\n",
    "\n",
    "def wait_xpath(drv, xpath, sec=25):\n",
    "    return WebDriverWait(drv, sec).until(EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "\n",
    "def wait_clickable(drv, xpath, sec=25):\n",
    "    return WebDriverWait(drv, sec).until(EC.element_to_be_clickable((By.XPATH, xpath)))\n",
    "\n",
    "def node_text(drv, xpath):\n",
    "    try:\n",
    "        el = drv.find_element(By.XPATH, xpath)\n",
    "        # textContent catches nested spans / line breaks better than .text sometimes\n",
    "        txt = el.get_attribute(\"textContent\") or \"\"\n",
    "        return \" \".join(txt.split())\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def get_venue_with_fallbacks(drv, timeout=8):\n",
    "    end = time.time() + timeout\n",
    "    try:\n",
    "        el = WebDriverWait(drv, min(4, timeout)).until(\n",
    "            EC.presence_of_element_located((By.XPATH, VENUE_X_PRIMARY))\n",
    "        )\n",
    "        txt = el.text.strip()\n",
    "        if txt: return txt\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "    for xp in VENUE_FALLBACKS:\n",
    "        if time.time() > end: break\n",
    "        try:\n",
    "            el = WebDriverWait(drv, max(1, int(end - time.time()))).until(\n",
    "                EC.presence_of_element_located((By.XPATH, xp))\n",
    "            )\n",
    "            txt = el.text.strip()\n",
    "            if txt: return txt\n",
    "        except TimeoutException:\n",
    "            continue\n",
    "    return \"\"\n",
    "\n",
    "def main():\n",
    "    driver = build_driver()\n",
    "    rows = []\n",
    "    try:\n",
    "        driver.get(URL)\n",
    "        wait_xpath(driver, LIST_X)\n",
    "        items = driver.find_elements(By.XPATH, LIST_X + \"/li\")\n",
    "        n = len(items)\n",
    "        if n == 0:\n",
    "            print(\"No events found under #upcoming-shows.\")\n",
    "            return\n",
    "\n",
    "        for i in range(1, n + 1):\n",
    "            try:\n",
    "                li = driver.find_element(By.XPATH, ITEM_X.format(i=i))\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", li)\n",
    "                time.sleep(0.2)\n",
    "\n",
    "                event_name = node_text(driver, NAME_X.format(i=i))\n",
    "                event_date = node_text(driver, DATE_X.format(i=i))   # ‚úÖ now using <small>\n",
    "\n",
    "                a = wait_clickable(driver, ANCHOR_X.format(i=i))\n",
    "                href = a.get_attribute(\"href\")\n",
    "                a.click()\n",
    "\n",
    "                venue = get_venue_with_fallbacks(driver, timeout=8)\n",
    "\n",
    "                rows.append({\n",
    "                    \"Event Name\": event_name,\n",
    "                    \"Event Date\": event_date,\n",
    "                    \"Venue\": venue,\n",
    "                    \"Link\": href or driver.current_url,\n",
    "                    \"Scraped At (UTC)\": datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                })\n",
    "\n",
    "            except (TimeoutException, StaleElementReferenceException):\n",
    "                rows.append({\n",
    "                    \"Event Name\": event_name if 'event_name' in locals() else \"\",\n",
    "                    \"Event Date\": event_date if 'event_date' in locals() else \"\",\n",
    "                    \"Venue\": \"\",\n",
    "                    \"Link\": href if 'href' in locals() else \"\",\n",
    "                    \"Scraped At (UTC)\": datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                })\n",
    "            finally:\n",
    "                try:\n",
    "                    driver.back()\n",
    "                    wait_xpath(driver, LIST_X)\n",
    "                except TimeoutException:\n",
    "                    driver.get(URL)\n",
    "                    wait_xpath(driver, LIST_X)\n",
    "\n",
    "        if rows:\n",
    "            pd.DataFrame(rows).to_csv(\"all_bangkok.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "            print(f\"Saved {len(rows)} rows to all_bangkok.csv\")\n",
    "            _merge_back_lnt()  # <-- ensure immediate merge so previous data is preserved\n",
    "        else:\n",
    "            print(\"No rows collected.\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f344993d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 256 cards.\n",
      "üÜï Added 102 new event(s). Duplicates removed this run: 2.\n",
      "üì¶ Total rows now: 405 (previously had 305).\n",
      "‚úÖ No previously saved events were removed.\n",
      "\n",
      "Newly added:\n",
      "  ‚Ä¢ matr√´shka x CIZHKA | –ë–∞—Ä –ö—É—Ä–∞–∂ | Courage | https://megatix.in.th/events/matrshka-x-cizhka?source=home | scraped 2025-12-15T08:31:07Z\n",
      "  ‚Ä¢ New Year's Eve Dinner Buffet | DoubleTree by Hilton Bangkok Ploenchit | DoubleTree by Hilton Bangkok Ploenchit | https://megatix.in.th/events/new-years-eve-dinner-buffet-doubletree-by-hilton-bangkok-ploenchit?source=home | scraped 2025-12-15T08:31:08Z\n",
      "  ‚Ä¢ New Year‚Äôs Eve Special Dinner on 31 December 2025 | Cape Dara Resort | https://megatix.in.th/events/new-year-special-dinner-on-31-december-2025?source=home | scraped 2025-12-15T08:31:08Z\n",
      "  ‚Ä¢ 2026 NEW YEAR PARTY by the Beach | Baba Beach Club Natai Luxury Pool Villa Hotel | https://megatix.in.th/events/new-year-party-by-the-beach?source=home | scraped 2025-12-15T08:31:08Z\n",
      "  ‚Ä¢ SEEN & SIN New Year's Eve 2025 | AVANI+ Riverside Bangkok Hotel | AVANI+ Riverside Bangkok Hotel | https://megatix.in.th/events/seen-sin-new-years-eve-avani-riverside-bangkok-hotel?source=home | scraped 2025-12-15T08:31:08Z\n",
      "  ‚Ä¢ NYE - To New Beginnings at the Rooftop Bar at Penthouse Bar + Grill | Penthouse Bar+Grill | https://megatix.in.th/events/nye-to-new-beginnings-at-the-rooftop-bar-at-penthouse-bar-grill?source=home | scraped 2025-12-15T08:31:09Z\n",
      "  ‚Ä¢ New Year's Eve Gala Dinner Party 2025 | THE SUKHOTHAI BANGKOK | The Sukhothai Bangkok | https://megatix.in.th/events/new-years-gala-dinner-party-2025?source=home | scraped 2025-12-15T08:31:09Z\n",
      "  ‚Ä¢ Festive Celebration at Hilton Garden Inn Bangkok Silom | Hilton Garden Inn Bangkok Silom | https://megatix.in.th/events/festive-celebration-at-hilton-garden-inn-bangkok-silom?source=home | scraped 2025-12-15T08:31:09Z\n",
      "  ‚Ä¢ ZERAPHINA Party Bangkok | Above Eleven | https://megatix.in.th/events/ZERAPHINA?source=home | scraped 2025-12-15T08:31:09Z\n",
      "  ‚Ä¢ Moonbeach party15.12 | Pai Moonbeach | https://megatix.in.th/events/moonbeach-party1512?source=home | scraped 2025-12-15T08:31:10Z\n",
      "  ‚Ä¢ The Level Up Comedy Club - Stand-Up Open Mic @ P.J. O'Briens - Dec 16th | The Level Up Comedy Club @ P.J. O'Brien's Bangkok | https://megatix.in.th/events/the-level-up-comedy-club-stand-up-open-mic-pj-obriens-dec-16th?source=home | scraped 2025-12-15T08:31:10Z\n",
      "  ‚Ä¢ ùôÜùôäùôÉ ùôèùòºùôä ùôÅùôÑùôÇùôÉùôè ùôâùôÑùôÇùôÉùôè ùôàùôêùòºùôî ùôèùôÉùòºùôÑ | Koh Tao Boxing Stadium | https://megatix.in.th/events/KTMT171225?source=home | scraped 2025-12-15T08:31:10Z\n",
      "  ‚Ä¢ SEASIDE BEACH PARTY | Seaside Sunset Bar | https://megatix.in.th/events/seaside-beach-party181225?source=home | scraped 2025-12-15T08:31:11Z\n",
      "  ‚Ä¢ Mustache Takeover x Apt101 | House - Tech House - Melodic | APT 101 Club | https://megatix.in.th/events/mustache-takeover-x-apt101-house-tech-house-melodic?source=home | scraped 2025-12-15T08:31:11Z\n",
      "  ‚Ä¢ HYPNOTICA Friday 19 December @ Sabaii Bay Beach Club | Sabaii Bay Beach Club Koh Phangan | https://megatix.in.th/events/hypnotica-friday-19-december-sabaii-bay-beach-club?source=home | scraped 2025-12-15T08:31:11Z\n",
      "  ‚Ä¢ TOM ENZY at Illuzion Phuket | FRI 19 DEC | Illuzion Phuket | https://megatix.in.th/events/tom-enzy-at-illuzion-phuket-fri-19-dec?source=home | scraped 2025-12-15T08:31:12Z\n",
      "  ‚Ä¢ Red: Danny Rampling | ReD CNX | https://megatix.in.th/events/red-danny-rampling?source=home | scraped 2025-12-15T08:31:12Z\n",
      "  ‚Ä¢ HARD TECHNO with: MARI FERRARI !!! (our biggest booking ever) | SUBWERK, by Rave Times | SUBWERK Club | https://megatix.in.th/events/hard-techno-witch-mari-ferrari-our-biggest-booking-ever-subwerk-by-rave-times?source=home | scraped 2025-12-15T08:31:12Z\n",
      "  ‚Ä¢ THE MATCHA PARTY at KROMO Bangkok | KROMO Bangkok, Curio Collection by Hilton | https://megatix.in.th/events/matcha-party?source=home | scraped 2025-12-15T08:31:13Z\n",
      "  ‚Ä¢ Kolour Pattaya | Legend Siam Pattaya | https://megatix.in.th/events/kolour-legend?source=home | scraped 2025-12-15T08:31:13Z\n",
      "  ‚Ä¢ Danny Rampling Live in Hua Hin - Garden Party at Irie Tingz | Irie Tingz (Hua Hin) | https://megatix.in.th/events/danny-rampling-live-in-hua-hin-garden-party-at-irie-tingz?source=home | scraped 2025-12-15T08:31:14Z\n",
      "  ‚Ä¢ Santa Con 2025 | Tony's | https://megatix.in.th/events/santa-con-2025?source=home | scraped 2025-12-15T08:31:14Z\n",
      "  ‚Ä¢ Impact Fireworks Boat Party | Ratchawong pier | https://megatix.in.th/events/impact-fireworks-boat-party?source=home | scraped 2025-12-15T08:31:14Z\n",
      "  ‚Ä¢ Impact fireworks boat party | Ratchawong pier | https://megatix.in.th/events/impact-fireworks-boat-partyy?source=home | scraped 2025-12-15T08:31:14Z\n",
      "  ‚Ä¢ –í–∞–Ω—è –£—Å–æ–≤–∏—á: –°–æ–ª—å–Ω—ã–π –°—Ç–µ–Ω–¥–∞–ø –ö–æ–Ω—Ü–µ—Ä—Ç –≤ –ë–∞–Ω–≥–∫–æ–∫–µ - SIAM@SIAM DESIGN HOTEL BANGKOK | Siam@Siam Design Hotel Bangkok | https://megatix.in.th/events/UsovichBkkOLDe88?source=home | scraped 2025-12-15T08:31:15Z\n",
      "  ‚Ä¢ STANTON WARRIORS, TNTKLZ, LINDZ @ SUB @ Courage Phuket 20.12.2025 | Courage | https://megatix.in.th/events/stanton-warriors-sub-2012?source=home | scraped 2025-12-15T08:31:15Z\n",
      "  ‚Ä¢ CRUSY at Illuzion Phuket | SAT 20 DEC | Illuzion Phuket | https://megatix.in.th/events/crusy-at-illuzion-phuket-sat-20-dec?source=home | scraped 2025-12-15T08:31:15Z\n",
      "  ‚Ä¢ Petanque Tournament at Blue Parrot #4 | Blue Parrot Bkk | https://megatix.in.th/events/petanque-tournament-at-blue-parrot-3?source=home | scraped 2025-12-15T08:31:15Z\n",
      "  ‚Ä¢ Maya 3rd Anniversary | Maya Beach Club Phuket | 21.12.25 | Maya Beach Club Phuket | https://megatix.in.th/events/maya-anniversary-maya-beach-club-phuket-211225?source=home | scraped 2025-12-15T08:31:16Z\n",
      "  ‚Ä¢ SUNDAY SESSION | 499/1 Rama III Rd | https://megatix.in.th/events/sunday-session-bangkokislands?source=home | scraped 2025-12-15T08:31:16Z\n",
      "  ‚Ä¢ Moon mountain Pre-Party x Bangkok Island | 499/1 Rama III Rd | https://megatix.in.th/events/moon-mountain-pre-party-x-bangkok-island?source=home | scraped 2025-12-15T08:31:16Z\n",
      "  ‚Ä¢ Secret Rave Hard Techno Retro Mountain Club TUE 23.12.25 - Copy | LOST IN RETRO MOUNTAIN | https://megatix.in.th/events/secret-rave-hard-techno-retro-mountain-club-tue-231225-copy?source=home | scraped 2025-12-15T08:31:16Z\n",
      "  ‚Ä¢ SING SING THEATRE @Caf√© del Mar Phuket | 24.12.25 | Cafe Del Mar Phuket | https://megatix.in.th/events/sing-sing-theatre-cafe-del-mar-phuket-241225?source=home | scraped 2025-12-15T08:31:17Z\n",
      "  ‚Ä¢ Retro Mountain Club - 3 Stages - Holy Rave | LOST IN RETRO MOUNTAIN | https://megatix.in.th/events/Retro-Mountain-3-Stages-Holy-Rave?source=home | scraped 2025-12-15T08:31:18Z\n",
      "  ‚Ä¢ Short film talent | 499/1 Rama III Rd | https://megatix.in.th/events/short-film-talent?source=home | scraped 2025-12-15T08:31:18Z\n",
      "  ‚Ä¢ SHAPESHIFTERS X APT101 DEC 2025 | APT 101 Club | https://megatix.in.th/events/shapeshifters-x-apt101-dec-2025?source=home | scraped 2025-12-15T08:31:19Z\n",
      "  ‚Ä¢ Tayllor | Maya Beach Club Phuket | 27.12.25 | Maya Beach Club Phuket | https://megatix.in.th/events/tayllor-maya-beach-club-phuket-271225?source=home | scraped 2025-12-15T08:31:19Z\n",
      "  ‚Ä¢ Onyx Bangkok presents. D-Block & S-Te-Fan | Onyx Bangkok | https://megatix.in.th/events/onyx-bangkok-presents-dbstf?source=home | scraped 2025-12-15T08:31:20Z\n",
      "  ‚Ä¢ BASS SABOTAGE RAVE | COURAGE | –ö–£–†–ê–ñ | https://megatix.in.th/events/bass-sabotage-rave?source=home | scraped 2025-12-15T08:31:20Z\n",
      "  ‚Ä¢ VORTEXXX pres. CARNIVAL NIGHT with NATASHA WAX & SONY VIBE | Abundance Modern Fusion | https://megatix.in.th/events/vortexxx-pres-carnival-night-natasha-wax-sony-vibe?source=home | scraped 2025-12-15T08:31:20Z\n",
      "  ‚Ä¢ SPECTRUM ROOFTOP - NEW YEAR EVE CELEBRATION | Hyatt Regency Bangkok Sukhumvit | https://megatix.in.th/events/spectrum-rooftop-new-year-eve-celebration?source=home | scraped 2025-12-15T08:31:21Z\n",
      "  ‚Ä¢ Ecstatica New Years 2026 | AUM Sound Healing Center | https://megatix.in.th/events/ecstatica-new-years-2026?source=home | scraped 2025-12-15T08:31:21Z\n",
      "  ‚Ä¢ New Year's Eve Celebration | The Ritz-Carlton, Koh Samui | The Ritz-Carlton, Koh Samui | https://megatix.in.th/events/new-years-eve-celebration?source=home | scraped 2025-12-15T08:31:22Z\n",
      "  ‚Ä¢ Celebrate New Year's Eve Outdoor Pool Terrace Package | Hilton Garden Inn Bangkok Riverside (Together & Co) | https://megatix.in.th/events/hiltongardeninnbangkokriverside?source=home | scraped 2025-12-15T08:31:22Z\n",
      "  ‚Ä¢ THE TECHNO COUNTDOWN ‚Äì BANGKOK NYE 2025 | The River CLub | https://megatix.in.th/events/chatgpt-said-the-techno-countdown-bangkok-nye-2025?source=home | scraped 2025-12-15T08:31:22Z\n",
      "  ‚Ä¢ Moonbeach New year party | Moon Beach Pai | https://megatix.in.th/events/moonbeach-new-year-party2026?source=home | scraped 2025-12-15T08:31:23Z\n",
      "  ‚Ä¢ New Year Celebrations at Royal Orchid Sheraton Riverside Hotel Bangkok | Royal Orchid Sheraton Riverside Hotel Bangkok | https://megatix.in.th/events/new-year-celebrations-at-royal-orchid-sheraton-riverside-hotel-bangkok?source=home | scraped 2025-12-15T08:31:23Z\n",
      "  ‚Ä¢ IN THE SKY - EPIC NYE CELEBRATION | W District - Helipad | https://megatix.in.th/events/in-the-sky-epic-nye-celebration?source=home | scraped 2025-12-15T08:31:23Z\n",
      "  ‚Ä¢ FIRA NYE 2025 - Countdown Party | FIRA PHUKET | https://megatix.in.th/events/fira-nye-2025-countdown-party?source=home | scraped 2025-12-15T08:31:24Z\n",
      "  ‚Ä¢ Sunset to Starlight countdown 2026 - Escape | ESCAPE BANGKOK- The Emquartier Building B, 8th Floor | https://megatix.in.th/events/sunset-to-starlight-countdown-2026-Escape?source=home | scraped 2025-12-15T08:31:24Z\n",
      "  ‚Ä¢ New Year's Eve 2025 at Banyan Tree Bangkok | Banyan Tree Bangkok | https://megatix.in.th/events/new-years-eve-2025-at-banyan-tree-bangkok?source=home | scraped 2025-12-15T08:31:24Z\n",
      "  ‚Ä¢ [Early Bird 15% Off] Riverside New Year‚Äôs Eve Holiday Staycation 2026 | The Salil Hotel Riverside - Bangkok | The Salil Hotel Riverside - Bangkok | https://megatix.in.th/events/SLR25-NYE?source=home | scraped 2025-12-15T08:31:25Z\n",
      "  ‚Ä¢ New Year‚Äôs Epicurean Journey - CHAR'D Grill | Avista Grande Phuket Karon - MGallery Collection | Avista Grande Phuket Karon | https://megatix.in.th/events/CHARD-NYEs2025?source=home | scraped 2025-12-15T08:31:25Z\n",
      "  ‚Ä¢ NYE Rooftop Revelry | Sizzle Rooftop Restaurant | https://megatix.in.th/events/nye-rooftop-revelry?source=home | scraped 2025-12-15T08:31:25Z\n",
      "  ‚Ä¢ New Year's Eve Dinner & Countdown | 3 Uncles & a Barman | https://megatix.in.th/events/new-years-eve-dinner-countdown?source=home | scraped 2025-12-15T08:31:25Z\n",
      "  ‚Ä¢ ‚ÄòChaophraya Maha Mongkol‚Äô Dinner Buffet Package ‚ÄìTHB 990 per person | Kalanan Riverside Resort | https://megatix.in.th/events/chaophraya-maha-mongkol-dinner-buffet-package-thb-990-per-person?source=home | scraped 2025-12-15T08:31:25Z\n",
      "  ‚Ä¢ New Year Celebrations at Colette Bangkok | Colette Bangkok | https://megatix.in.th/events/countdown-at-colette?source=home | scraped 2025-12-15T08:31:25Z\n",
      "  ‚Ä¢ Jungle Feasts to City Peaks at PRIME | SKYVIEW Hotel Bangkok | Prime at SKYVIEW Hotel Bangkok Sukhumvit 24 | https://megatix.in.th/events/jungle-feasts-to-city-peaks-at-prime-skyview-hotel-bangkok?source=home | scraped 2025-12-15T08:31:26Z\n",
      "  ‚Ä¢ New Year‚Äôs Eve Gala Dinner | Vista - Avista Hideaway Phuket Patong | https://megatix.in.th/events/new-years-eve-gala-dinner?source=home | scraped 2025-12-15T08:31:26Z\n",
      "  ‚Ä¢ Gatsby & Mask New Year's Eve 2025 at ZOOM Sky Bar & Restaurant | ZOOM Skybar & Restaurant (JC Kevin Sathorn Bangkok Hotel, 40th Floor) | https://megatix.in.th/events/newyearseve2025-zoomskybar?source=home | scraped 2025-12-15T08:31:26Z\n",
      "  ‚Ä¢ New Year's Eve Celebration Dinner | Open Kitchen restaurant | https://megatix.in.th/events/new-years-eve-celebration-dinner?source=home | scraped 2025-12-15T08:31:26Z\n",
      "  ‚Ä¢ ELECTRIC CARNIVAL: SKY-HIGH COUNTDOWN ABOVE BANGKOK | Sato San Rooftop Bar | https://megatix.in.th/events/electric-carnival-satosanrooftopbar?source=home | scraped 2025-12-15T08:31:27Z\n",
      "  ‚Ä¢ Alexa Beach Club Pattaya NYE PARTY 2026 | Alexa Beach Club Pattaya | https://megatix.in.th/events/alexa-beach-club-pattaya-nye-2026?source=home | scraped 2025-12-15T08:31:27Z\n",
      "  ‚Ä¢ RAYNUE COUNTDOWN COLLECTIVE | Raynue (Gaysorn Amarin) | https://megatix.in.th/events/raynue-new-years-countdown-2026?source=home | scraped 2025-12-15T08:31:27Z\n",
      "  ‚Ä¢ Riviera New Year‚Äôs Eve at Iris & Wild Iris | 31st December 2025 | SO BANGKOK | https://megatix.in.th/events/riviera-new-years-eve-at-iris-wild-iris-31st-december-2025?source=home | scraped 2025-12-15T08:31:28Z\n",
      "  ‚Ä¢ Maison Midnight New Year‚Äôs Eve Club de l‚ÄôEau at The Water Club | 31st December 2025 | SO BANGKOK | https://megatix.in.th/events/maison-midnight-new-years-eve-club-de-leau-at-the-water-club-31st-december-2025?source=home | scraped 2025-12-15T08:31:28Z\n",
      "  ‚Ä¢ Ring in the New Year with a vibrant celebration at Caf√© Mozu by lebua | Caf√© Mozu, M Floor by the pool | https://megatix.in.th/events/new-year-celebration-at-cafe-mozu-by-lebua?source=home | scraped 2025-12-15T08:31:28Z\n",
      "  ‚Ä¢ Beachfront Countdown to 2026 | Cabanas Koh Samui Beach Club | https://megatix.in.th/events/beachfront-countdown-to-2026?source=home | scraped 2025-12-15T08:31:28Z\n",
      "  ‚Ä¢ Sunset to Starlight countdown 2026 - Tribe | TRIBE Sky Beach Club | https://megatix.in.th/events/sunset-to-starlight-countdown-2026-Tribe?source=home | scraped 2025-12-15T08:31:28Z\n",
      "  ‚Ä¢ New Year @ HGI Phuket | Hilton Garden Inn Phuket Bang Tao | https://megatix.in.th/events/new-year-hgi-phuket?source=home | scraped 2025-12-15T08:31:28Z\n",
      "  ‚Ä¢ New Years Eve free-flow@79 Beach Club Samui | 79 Beach Club Samui | https://megatix.in.th/events/new-years-eve79-beach-club-samui?source=home | scraped 2025-12-15T08:31:28Z\n",
      "  ‚Ä¢ NEW YEAR‚ÄôS EVE PARTY AT THE MESH | The Mesh, Four Point by Sheraton Sukhumvit 22 | https://megatix.in.th/events/new-years-eve-party-at-the-mesh?source=home | scraped 2025-12-15T08:31:29Z\n",
      "  ‚Ä¢ PULLMAN BANGKOK COUNTDOWN - SKYLINE SPARKLES @BALLROOM38 | Pullman Bangkok Hotel G | https://megatix.in.th/events/pullman-countdown-2025?source=home | scraped 2025-12-15T08:31:29Z\n",
      "  ‚Ä¢ Yao Rooftop NYE - Midnight in Shanghai | Yao Rooftop Bar | https://megatix.in.th/events/yao-rooftop-nye-midnight-in-shanghai?source=home | scraped 2025-12-15T08:31:30Z\n",
      "  ‚Ä¢ Rooftop Bar RedSquare Moonlight New Year‚Äôs Eve Party | RedSquare Rooftop Bar | https://megatix.in.th/events/moonlight-new-years-eve-party-redsquare?source=home | scraped 2025-12-15T08:31:30Z\n",
      "  ‚Ä¢ NEW YEAR EVE ROOFTOP PARTY üî¥ ANOTHER WORLD | 31 DEC 2025 | The Westin Grande Sukhumvit, Bangkok | https://megatix.in.th/events/madstash-westin-nye2025?source=home | scraped 2025-12-15T08:31:30Z\n",
      "  ‚Ä¢ Nobu Bangkok | New Year's Eve Starlit Celebration | Nobu Bangkok | https://megatix.in.th/events/nobu-bangkok-new-years-eve-celebration?source=home | scraped 2025-12-15T08:31:30Z\n",
      "  ‚Ä¢ Le Du Kaan | New Year's Eve White Party | Le Du Kaan | https://megatix.in.th/events/early-bird-le-du-kaan-new-years-eve-party?source=home | scraped 2025-12-15T08:31:30Z\n",
      "  ‚Ä¢ New Year Eve's Countdown Party | The House on Sathorn | https://megatix.in.th/events/new-year-eves-countdown-party?source=home | scraped 2025-12-15T08:31:31Z\n",
      "  ‚Ä¢ Countdown Pool Party | Pullman Bangkok King Power Hotel | https://megatix.in.th/events/countdown-pool-party?source=home | scraped 2025-12-15T08:31:31Z\n",
      "  ‚Ä¢ Hollystone NYE 2026 | Hollystone Phangan | https://megatix.in.th/events/hollystone-nye-2026?source=home | scraped 2025-12-15T08:31:32Z\n",
      "  ‚Ä¢ Bangkok‚Äôs Wildest NYE - Fire, Pole, Lights and Chaos | Aces Night Club | https://megatix.in.th/events/aces-bangkok-countdown-2026-neon-nights?source=home | scraped 2025-12-15T08:31:32Z\n",
      "  ‚Ä¢ The Ultimate Rooftop Fiesta to 2026! at MOJJO | SKYVIEW Hotel Bangkok | Mojjo Rooftop Lounge & Bar | https://megatix.in.th/events/The-ultimate-rooftop-fiesta-to-2026-at-mojjo-skyview-hotel-bangkok?source=home | scraped 2025-12-15T08:31:32Z\n",
      "  ‚Ä¢ Stellar Ascent Countdown 2026 at VANILLA SKY ROOFTOP BAR | SKYVIEW Hotel Bangkok | Vanilla Sky Rooftop | https://megatix.in.th/events/cosmic-skyline-countdown-at-vanilla-sky-rooftop-bar-skyview-hotel-bangkok?source=home | scraped 2025-12-15T08:31:33Z\n",
      "  ‚Ä¢ ODYSSEY @Caf√© del Mar Phuket | 06.01.26 | Cafe Del Mar Phuket | https://megatix.in.th/events/odyssey-cafe-del-mar-phuket-060126?source=home | scraped 2025-12-15T08:31:34Z\n",
      "  ‚Ä¢ Stand Up –ù—É—Ä–ª–∞–Ω–∞ –°–∞–±—É—Ä–æ–≤–∞ –Ω–∞ –ü—Ö—É–∫–µ—Ç–µ | Royal Phuket City Hotel | https://megatix.in.th/events/nurlan-saburov?source=home | scraped 2025-12-15T08:31:35Z\n",
      "  ‚Ä¢ CHIFORCE, CENTREFORCE883 x CHI SAMUI | Chi Samui Beach Club | https://megatix.in.th/events/chiforce?source=home | scraped 2025-12-15T08:31:35Z\n",
      "  ‚Ä¢ OXA - Jungle Party - Berlin Calling Edition [09.01.26] | Oxa Koh Phangan | https://megatix.in.th/events/oxa-jungle-party-090126?source=home | scraped 2025-12-15T08:31:35Z\n",
      "  ‚Ä¢ Sunset Splash X Innerbloom Pool Party - 10 Jan 2026 | W Bangkok | https://megatix.in.th/events/sunset-splash-x-innerbloom-pool-party-06-december-2025-copy?source=home | scraped 2025-12-15T08:31:36Z\n",
      "  ‚Ä¢ Jungle Jam BKK: 30 years of PLAYAZ [DnB] | Jungle Jam BKK (Soi Sukhumvit 11) | https://megatix.in.th/events/junglejambkk-playaz-dnb?source=home | scraped 2025-12-15T08:31:36Z\n",
      "  ‚Ä¢ Escobar KohTao - Jungle Light üî• üå¥ 17.1.25 | Escobar | https://megatix.in.th/events/escobar-kohtao-jungle-light-17125?source=home | scraped 2025-12-15T08:31:37Z\n",
      "  ‚Ä¢ HVYWGHT x ONEFORTYASIA with GOTH TRAD and DEEP TEMPO | Speakerbox | https://megatix.in.th/events/hvywght-x-onefortyasia?source=home | scraped 2025-12-15T08:31:38Z\n",
      "  ‚Ä¢ Escobar KohTao - Primal Moon üî• üå¥ 25.1.25 | Escobar | https://megatix.in.th/events/escobar-kohtao-primal-moon-25125?source=home | scraped 2025-12-15T08:31:39Z\n",
      "  ‚Ä¢ Escobar KohTao - Jungle Cakes üî• üå¥ 29.1.25 | Escobar | https://megatix.in.th/events/escobar-kohtao-jungle-cakes-29125?source=home | scraped 2025-12-15T08:31:39Z\n",
      "  ‚Ä¢ Jungle Experience Festival Feb 1st 2026 | Jungle Experience | https://megatix.in.th/events/jungle-experience-festival-feb-1st-2026?source=home | scraped 2025-12-15T08:31:40Z\n",
      "  ‚Ä¢ Escobar KohTao - Mystic Jungleüî• üå¥ 5.2.25 | Escobar | https://megatix.in.th/events/escobar-kohtao-mystic-jungle-5225?source=home | scraped 2025-12-15T08:31:40Z\n",
      "  ‚Ä¢ Escobar KohTao - Wild Tribeüî• üå¥ 9.2.25 | Escobar | https://megatix.in.th/events/escobar-kohtao-wild-tribe-9225?source=home | scraped 2025-12-15T08:31:42Z\n",
      "  ‚Ä¢ Escobar KohTao - The Dragon‚Äôs Jungleüî• üå¥ 15.2.25 | Escobar | https://megatix.in.th/events/escobar-kohtao-the-dragons-jungle-15225?source=home | scraped 2025-12-15T08:31:42Z\n",
      "  ‚Ä¢ Escobar KohTao - Forbidden Forestüî• üå¥ 23.2.25 | Escobar | https://megatix.in.th/events/escobar-kohtao-forbidden-forest-23225?source=home | scraped 2025-12-15T08:31:43Z\n",
      "  ‚Ä¢ OXA - Jungle Party [27.02.26] | Oxa Koh Phangan | https://megatix.in.th/events/oxa-jungle-party-270226?source=home | scraped 2025-12-15T08:31:43Z\n",
      "  ‚Ä¢ Escobar KohTao - Wild Eclipseüî• üå¥ 27.2.25 | Escobar | https://megatix.in.th/events/escobar-kohtao-wild-eclipse-27225?source=home | scraped 2025-12-15T08:31:43Z\n",
      "  ‚Ä¢ OXA - Jungle Party - Berlin Calling Edition [06.03.26] | Oxa Koh Phangan | https://megatix.in.th/events/oxa-jungle-party-berlin-calling-060326?source=home | scraped 2025-12-15T08:31:43Z\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Megatix Thailand (category=31) scraper ‚Äî INCREMENTAL & NON-DELETING\n",
    "- Appends only new events to OUT_CSV; existing rows are preserved.\n",
    "- Dedup key: uid = norm(Event Name) + norm(Link)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException, NoSuchElementException,\n",
    "    ElementClickInterceptedException, StaleElementReferenceException\n",
    ")\n",
    "\n",
    "URL = \"https://megatix.in.th/events?category=31\"\n",
    "OUT_CSV = \"all_bangkok.csv\"\n",
    "\n",
    "# ---- Your XPaths ----\n",
    "X_SECTION = '//*[@id=\"browse-events-heading\"]/div[2]'\n",
    "X_LOAD_MORE = '//*[@id=\"browse-events-heading\"]/div[2]/div/div[3]/button/span'\n",
    "X_CARD_BASE = '//*[@id=\"browse-events-heading\"]/div[2]/div/div[2]/div/div'\n",
    "\n",
    "def x_name(i):  return f'//*[@id=\"browse-events-heading\"]/div[2]/div/div[2]/div/div[{i}]/a/div/div/div[1]/p[2]'\n",
    "def x_venue(i): return f'//*[@id=\"browse-events-heading\"]/div[2]/div/div[2]/div/div[{i}]/a/div/div/div[1]/p[3]'\n",
    "def x_link(i):  return f'//*[@id=\"browse-events-heading\"]/div[2]/div/div[2]/div/div[{i}]/a'\n",
    "\n",
    "def utc_now_iso():\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    \"\"\"Lowercase, strip, collapse whitespace for stable matching.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    return \" \".join(s.strip().lower().split())\n",
    "\n",
    "def build_uid(name: str, link: str) -> str:\n",
    "    return f\"{norm(name)}|{norm(link)}\"\n",
    "\n",
    "def slow_scroll_to_bottom(driver, step=600, pause=0.4, max_passes=3):\n",
    "    for _ in range(max_passes):\n",
    "        last_h = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "        y = 0\n",
    "        while y < last_h:\n",
    "            y += step\n",
    "            driver.execute_script(f\"window.scrollTo(0, {y});\")\n",
    "            time.sleep(pause)\n",
    "        time.sleep(0.8)\n",
    "        new_h = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "        if new_h == last_h:\n",
    "            break\n",
    "\n",
    "def click_load_more_until_done(driver, wait, max_clicks=200):\n",
    "    clicks = 0\n",
    "    while clicks < max_clicks:\n",
    "        try:\n",
    "            btn_span = wait.until(EC.presence_of_element_located((By.XPATH, X_LOAD_MORE)))\n",
    "            button = driver.find_element(By.XPATH, X_LOAD_MORE + \"/..\")\n",
    "            if not button.is_enabled() or not button.is_displayed():\n",
    "                break\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", button)\n",
    "            time.sleep(0.4)\n",
    "            button.click()\n",
    "            clicks += 1\n",
    "            time.sleep(1.2)\n",
    "            slow_scroll_to_bottom(driver, step=700, pause=0.35, max_passes=1)\n",
    "        except (TimeoutException, NoSuchElementException):\n",
    "            break\n",
    "        except (ElementClickInterceptedException, StaleElementReferenceException):\n",
    "            time.sleep(0.8)\n",
    "            try:\n",
    "                button = driver.find_element(By.XPATH, X_LOAD_MORE + \"/..\")\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", button)\n",
    "                time.sleep(0.5)\n",
    "                button.click()\n",
    "                clicks += 1\n",
    "                time.sleep(1.2)\n",
    "            except Exception:\n",
    "                break\n",
    "\n",
    "def get_card_count(driver):\n",
    "    return len(driver.find_elements(By.XPATH, X_CARD_BASE))\n",
    "\n",
    "def scrape_events():\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    opts.add_argument(\"--start-maximized\")\n",
    "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--window-size=1400,1000\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opts.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "    rows = []\n",
    "\n",
    "    try:\n",
    "        driver.get(URL)\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, X_SECTION)))\n",
    "        slow_scroll_to_bottom(driver, step=700, pause=0.35, max_passes=2)\n",
    "        click_load_more_until_done(driver, wait)\n",
    "        slow_scroll_to_bottom(driver, step=800, pause=0.4, max_passes=2)\n",
    "\n",
    "        total = get_card_count(driver)\n",
    "        print(f\"Found {total} cards.\")\n",
    "\n",
    "        for i in range(1, total + 1):\n",
    "            try:\n",
    "                name = driver.find_element(By.XPATH, x_name(i)).text.strip()\n",
    "                venue = driver.find_element(By.XPATH, x_venue(i)).text.strip()\n",
    "                link = driver.find_element(By.XPATH, x_link(i)).get_attribute(\"href\") or \"\"\n",
    "                rows.append({\n",
    "                    \"Event Name\": name,\n",
    "                    \"Event Venue\": venue,\n",
    "                    \"Link\": link,\n",
    "                    \"Source\": \"Megatix TH\",\n",
    "                    \"ScrapedAtUTC\": utc_now_iso(),  # ‚Üê time scraped included\n",
    "                    \"uid\": build_uid(name, link)\n",
    "                })\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "            except StaleElementReferenceException:\n",
    "                time.sleep(0.2)\n",
    "                try:\n",
    "                    name = driver.find_element(By.XPATH, x_name(i)).text.strip()\n",
    "                    venue = driver.find_element(By.XPATH, x_venue(i)).text.strip()\n",
    "                    link = driver.find_element(By.XPATH, x_link(i)).get_attribute(\"href\") or \"\"\n",
    "                    rows.append({\n",
    "                        \"Event Name\": name,\n",
    "                        \"Event Venue\": venue,\n",
    "                        \"Link\": link,\n",
    "                        \"Source\": \"Megatix TH\",\n",
    "                        \"ScrapedAtUTC\": utc_now_iso(),  # ‚Üê time scraped included\n",
    "                        \"uid\": build_uid(name, link)\n",
    "                    })\n",
    "                except Exception:\n",
    "                    continue\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return pd.DataFrame(rows) if rows else pd.DataFrame(columns=[\n",
    "        \"Event Name\",\"Event Venue\",\"Link\",\"Source\",\"ScrapedAtUTC\",\"uid\"\n",
    "    ])\n",
    "\n",
    "def append_only_new(df_new: pd.DataFrame, out_csv: str):\n",
    "    \"\"\"\n",
    "    Append only rows whose uid is NOT already in out_csv.\n",
    "    Never deletes existing rows. Keeps their original ScrapedAtUTC.\n",
    "    \"\"\"\n",
    "    if os.path.exists(out_csv):\n",
    "        df_old = pd.read_csv(out_csv)\n",
    "        if \"uid\" not in df_old.columns:\n",
    "            # Backfill uids for legacy files\n",
    "            df_old[\"uid\"] = (df_old[\"Event Name\"].fillna(\"\")\n",
    "                             .apply(norm) + \"|\" + df_old[\"Link\"].fillna(\"\").apply(norm))\n",
    "    else:\n",
    "        df_old = pd.DataFrame(columns=df_new.columns)\n",
    "\n",
    "    # --- BEGIN ADDITIONS: summary + no-duplicate/no-removal guarantees ---\n",
    "    old_uids_before = set(df_old.get(\"uid\", pd.Series(dtype=str)).astype(str))\n",
    "    old_count_before = len(df_old)\n",
    "\n",
    "    # Backfill missing ScrapedAtUTC for old rows (keeps original timestamps if present)\n",
    "    if \"ScrapedAtUTC\" in df_old.columns:\n",
    "        df_old[\"ScrapedAtUTC\"] = (\n",
    "            df_old[\"ScrapedAtUTC\"].astype(str).str.strip().replace({\"nan\": \"\", \"None\": \"\"})\n",
    "        )\n",
    "        df_old.loc[df_old[\"ScrapedAtUTC\"] == \"\", \"ScrapedAtUTC\"] = utc_now_iso()\n",
    "    # --- END ADDITIONS ---\n",
    "\n",
    "    old_uids = set(df_old.get(\"uid\", pd.Series(dtype=str)).astype(str))\n",
    "    df_new = df_new.copy()\n",
    "    df_new[\"uid\"] = df_new[\"uid\"].astype(str)\n",
    "\n",
    "    # Keep only truly new rows\n",
    "    df_to_add = df_new[~df_new[\"uid\"].isin(old_uids)]\n",
    "\n",
    "    if not df_to_add.empty:\n",
    "        # Concatenate without dropping any existing rows\n",
    "        df_out_pre_dedup = pd.concat([df_old, df_to_add], ignore_index=True)\n",
    "\n",
    "        # --- BEGIN ADDITIONS: enforce no duplicates by uid (keep first), and summarize ---\n",
    "        df_out = df_out_pre_dedup.drop_duplicates(subset=[\"uid\"], keep=\"first\").reset_index(drop=True)\n",
    "        duplicates_removed = len(df_out_pre_dedup) - len(df_out)\n",
    "\n",
    "        # Sanity: ensure all previously saved uids are still present (no removals)\n",
    "        previously_saved_still_present = old_uids_before.issubset(set(df_out[\"uid\"].astype(str)))\n",
    "\n",
    "        # Optional: ensure column order\n",
    "        cols = [\"Event Name\",\"Event Venue\",\"Link\",\"Source\",\"ScrapedAtUTC\",\"uid\"]\n",
    "        df_out = df_out[[c for c in cols if c in df_out.columns] + [c for c in df_out.columns if c not in cols]]\n",
    "\n",
    "        # Save\n",
    "        df_out.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        # Print detailed summary\n",
    "        actually_added_count = len(df_to_add)\n",
    "        print(f\"üÜï Added {actually_added_count} new event(s). Duplicates removed this run: {duplicates_removed}.\")\n",
    "        print(f\"üì¶ Total rows now: {len(df_out)} (previously had {old_count_before}).\")\n",
    "        if previously_saved_still_present:\n",
    "            print(\"‚úÖ No previously saved events were removed.\")\n",
    "        else:\n",
    "            print(\"‚ö† Warning: some previously saved events are missing (unexpected).\")\n",
    "\n",
    "        # List the newly added events (with time scraped)\n",
    "        if actually_added_count > 0:\n",
    "            print(\"\\nNewly added:\")\n",
    "            for _, r in df_to_add.iterrows():\n",
    "                print(f\"  ‚Ä¢ {r['Event Name']} | {r['Event Venue']} | {r['Link']} | scraped {r['ScrapedAtUTC']}\")\n",
    "        # --- END ADDITIONS ---\n",
    "\n",
    "    else:\n",
    "        print(\"No new events to append. Existing file left unchanged.\")\n",
    "\n",
    "def main():\n",
    "    df_new = scrape_events()\n",
    "    if df_new.empty:\n",
    "        print(\"No rows scraped ‚Äî page structure may have changed or content blocked.\")\n",
    "        return\n",
    "    append_only_new(df_new, OUT_CSV)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89ed574",
   "metadata": {},
   "source": [
    "# Australia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96bbcb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 140 existing rows from all_australia.csv\n",
      "‚úÖ Saved 145 total rows to all_australia.csv\n",
      "üÜï Added 5 new rows.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# moshtix_sections_scraper.py\n",
    "# Scrapes events ONLY from:\n",
    "#   - //*[@id=\"col_main\"]/section[4]\n",
    "#   - //*[@id=\"col_main\"]/section[5]\n",
    "#\n",
    "# Extracted fields per ticket row:\n",
    "#   Event Name | Date | Location | Ticket Name | Ticket Price | Event Link\n",
    "#\n",
    "# Behavior:\n",
    "#   - Keeps previous data in all_australia.csv\n",
    "#   - Appends only new rows (no duplicates)\n",
    "#   - Prints how many new records were added\n",
    "#\n",
    "# Usage:\n",
    "#   pip install selenium pandas\n",
    "#   python moshtix_sections_scraper.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    StaleElementReferenceException,\n",
    "    ElementClickInterceptedException,\n",
    ")\n",
    "\n",
    "HOME = \"https://www.moshtix.com.au/v2/\"\n",
    "OUTPUT_CSV = \"all_australia.csv\"\n",
    "\n",
    "WAIT_SEC = 25\n",
    "\n",
    "def make_driver():\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    opts.add_argument(\"--start-maximized\")\n",
    "    opts.add_argument(\"--disable-notifications\")\n",
    "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    return webdriver.Chrome(options=opts)\n",
    "\n",
    "def wait_for(drv, by, selector, sec=WAIT_SEC):\n",
    "    return WebDriverWait(drv, sec).until(EC.presence_of_element_located((by, selector)))\n",
    "\n",
    "def smooth_scroll_to(drv, element):\n",
    "    try:\n",
    "        drv.execute_script(\"arguments[0].scrollIntoView({block:'start', inline:'nearest'});\", element)\n",
    "        time.sleep(0.5)\n",
    "        drv.execute_script(\"window.scrollBy(0, -120);\")\n",
    "        time.sleep(0.4)\n",
    "        drv.execute_script(\"window.scrollBy(0, 240);\")\n",
    "        time.sleep(0.6)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def dismiss_banners(drv):\n",
    "    candidates = [\n",
    "        \"//button[contains(translate(., 'ACEPTI', 'acepti'), 'accept')]\",\n",
    "        \"//button[contains(., 'Accept')]\",\n",
    "        \"//button[contains(., 'I agree')]\",\n",
    "        \"//button[contains(., 'Got it')]\",\n",
    "        \"//button[contains(., 'Close')]\",\n",
    "    ]\n",
    "    for xp in candidates:\n",
    "        try:\n",
    "            btn = WebDriverWait(drv, 3).until(EC.element_to_be_clickable((By.XPATH, xp)))\n",
    "            btn.click()\n",
    "            time.sleep(0.3)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def collect_event_links_in_section(drv, section_xpath):\n",
    "    links = []\n",
    "    try:\n",
    "        sec_el = wait_for(drv, By.XPATH, section_xpath)\n",
    "        smooth_scroll_to(drv, sec_el)\n",
    "        anchors = sec_el.find_elements(By.XPATH, \".//a[contains(@href, '/v2/event/')]\")\n",
    "        seen = set()\n",
    "        for a in anchors:\n",
    "            try:\n",
    "                href = a.get_attribute(\"href\")\n",
    "                if href and \"/v2/event/\" in href and href not in seen:\n",
    "                    seen.add(href)\n",
    "                    links.append((a, href))\n",
    "            except StaleElementReferenceException:\n",
    "                continue\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "    return links\n",
    "\n",
    "def extract_event_core(drv):\n",
    "    def safe_text(by, sel):\n",
    "        try:\n",
    "            el = wait_for(drv, by, sel, sec=8)\n",
    "            return el.text.strip()\n",
    "        except TimeoutException:\n",
    "            return \"\"\n",
    "    name = safe_text(By.CSS_SELECTOR, \"#event-summary-title > span\")\n",
    "    date = safe_text(By.CSS_SELECTOR, \"#event-summary-date\")\n",
    "    venue = safe_text(By.CSS_SELECTOR, \"#event-summary-venue > span\")\n",
    "    return name, date, venue\n",
    "\n",
    "def extract_tickets(drv):\n",
    "    tickets = []\n",
    "    try:\n",
    "        form = wait_for(drv, By.CSS_SELECTOR, \"#event-tickets-form\", sec=10)\n",
    "        name_els = form.find_elements(By.CSS_SELECTOR, \"[id^='ticket-type-name-']\")\n",
    "        for n in name_els:\n",
    "            try:\n",
    "                n_id = n.get_attribute(\"id\")\n",
    "                suffix = n_id.split(\"-\")[-1]\n",
    "                tname = n.text.strip()\n",
    "                price_sel = f\"#ticket-type-total-{suffix}\"\n",
    "                try:\n",
    "                    price_el = form.find_element(By.CSS_SELECTOR, price_sel)\n",
    "                    tprice = price_el.text.strip()\n",
    "                except NoSuchElementException:\n",
    "                    tprice = \"\"\n",
    "                if tname:\n",
    "                    tickets.append((tname, tprice))\n",
    "            except Exception:\n",
    "                continue\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "    return tickets\n",
    "\n",
    "\n",
    "def load_existing_data(path):\n",
    "    \"\"\"### NEW: Load existing CSV and return as DataFrame\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            print(f\"Loaded {len(df)} existing rows from {path}\")\n",
    "            return df\n",
    "        except Exception:\n",
    "            print(\"‚ö†Ô∏è Could not read existing CSV; starting fresh.\")\n",
    "    return pd.DataFrame(columns=[\"Event Name\", \"Date\", \"Location\", \"Ticket Name\", \"Ticket Price\", \"Link\"])\n",
    "\n",
    "\n",
    "def merge_and_save(existing_df, new_rows, output_csv):\n",
    "    \"\"\"### NEW: Merge new data, drop duplicates, save CSV, and show stats\"\"\"\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    combined = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "    combined_before = len(combined)\n",
    "    combined = combined.drop_duplicates(\n",
    "        subset=[\"Event Name\", \"Ticket Name\", \"Ticket Price\"], keep=\"first\"\n",
    "    )\n",
    "    added = len(combined) - len(existing_df)\n",
    "    combined.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"‚úÖ Saved {len(combined)} total rows to {output_csv}\")\n",
    "    print(f\"üÜï Added {added} new rows.\\n\")\n",
    "    return added\n",
    "\n",
    "\n",
    "def main():\n",
    "    driver = make_driver()\n",
    "    rows = []\n",
    "    existing = load_existing_data(OUTPUT_CSV)\n",
    "\n",
    "    try:\n",
    "        driver.get(HOME)\n",
    "        wait_for(driver, By.ID, \"col_main\")\n",
    "        time.sleep(1.2)\n",
    "        dismiss_banners(driver)\n",
    "\n",
    "        for _ in range(4):\n",
    "            driver.execute_script(\"window.scrollBy(0, 800);\")\n",
    "            time.sleep(0.6)\n",
    "\n",
    "        section_xpaths = [\n",
    "            \"//*[@id='col_main']/section[4]\",\n",
    "            \"//*[@id='col_main']/section[5]\",\n",
    "        ]\n",
    "\n",
    "        # Scrape timestamp for this run\n",
    "        scraped_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        for sec_xp in section_xpaths:\n",
    "            links = collect_event_links_in_section(driver, sec_xp)\n",
    "            for idx, (elem, href) in enumerate(links, start=1):\n",
    "                try:\n",
    "                    try:\n",
    "                        smooth_scroll_to(driver, elem)\n",
    "                        elem.click()\n",
    "                    except (ElementClickInterceptedException, StaleElementReferenceException):\n",
    "                        driver.get(href)\n",
    "\n",
    "                    wait_for(driver, By.CSS_SELECTOR, \"#event-summary-title > span\", sec=WAIT_SEC)\n",
    "                    ev_name, ev_date, ev_venue = extract_event_core(driver)\n",
    "                    ev_link = driver.current_url\n",
    "\n",
    "                    tickets = extract_tickets(driver)\n",
    "                    if tickets:\n",
    "                        for tname, tprice in tickets:\n",
    "                            rows.append({\n",
    "                                \"Event Name\": ev_name,\n",
    "                                \"Date\": ev_date,\n",
    "                                \"Location\": ev_venue,\n",
    "                                \"Ticket Name\": tname,\n",
    "                                \"Ticket Price\": tprice,\n",
    "                                \"Link\": ev_link,\n",
    "                                \"Scraped Time\": scraped_time\n",
    "                            })\n",
    "                    else:\n",
    "                        rows.append({\n",
    "                            \"Event Name\": ev_name,\n",
    "                            \"Date\": ev_date,\n",
    "                            \"Location\": ev_venue,\n",
    "                            \"Ticket Name\": \"\",\n",
    "                            \"Ticket Price\": \"\",\n",
    "                            \"Link\": ev_link,\n",
    "                            \"Scraped Time\": scraped_time\n",
    "                        })\n",
    "\n",
    "                    driver.back()\n",
    "                    wait_for(driver, By.ID, \"col_main\")\n",
    "                    time.sleep(0.8)\n",
    "                    dismiss_banners(driver)\n",
    "\n",
    "                except TimeoutException:\n",
    "                    try:\n",
    "                        driver.back()\n",
    "                        wait_for(driver, By.ID, \"col_main\")\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    continue\n",
    "\n",
    "        # --- Merge and Save ---\n",
    "        merge_and_save(existing, rows, OUTPUT_CSV)\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11df2046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Ticketmaster AU ‚Äî Dance/Electronic (date-filtered) scraper\n",
    "# - Source URL (example): https://www.ticketmaster.com.au/browse/dance-electronic-catid-201/music-rid-10001#pageInfo?startDate=2025-10-27&endDate=2025-12-31\n",
    "# - Columns: Title, Date, Location, Link, Source, ScrapedAtUTC\n",
    "\n",
    "# Usage (VS Code / terminal):\n",
    "#     pip install selenium pandas\n",
    "#     python scrape_ticketmaster_au_elec.py\n",
    "# \"\"\"\n",
    "\n",
    "# import time\n",
    "# import sys\n",
    "# from datetime import datetime, timezone\n",
    "\n",
    "# import pandas as pd\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.common.exceptions import (\n",
    "#     TimeoutException,\n",
    "#     NoSuchElementException,\n",
    "#     StaleElementReferenceException,\n",
    "# )\n",
    "\n",
    "# # ===================== CONFIG =====================\n",
    "# START_DATE = \"2025-10-27\"\n",
    "# END_DATE   = \"2025-12-31\"\n",
    "\n",
    "# # Your provided (category) page + date filters.\n",
    "# # NOTE: Put the \"#pageInfo\" anchor before the query string as per your link.\n",
    "# BASE_URL = (\n",
    "#     \"https://www.ticketmaster.com.au/browse/dance-electronic-catid-201/music-rid-10001\"\n",
    "#     \"#pageInfo\"\n",
    "#     f\"?startDate={START_DATE}&endDate={END_DATE}\"\n",
    "# )\n",
    "\n",
    "# OUTPUT_CSV = \"all_australia.csv\"\n",
    "# WAIT_SEC   = 25\n",
    "\n",
    "# # Your provided XPaths\n",
    "# X_SECTION  = \"//*[@id='pageInfo']/div[1]/div/div[2]/div[2]/div[1]\"\n",
    "# X_LIST     = X_SECTION + \"/div[2]/div[2]/ul\"\n",
    "# X_ITEMS    = X_LIST + \"/li\"\n",
    "\n",
    "# # Relative XPaths from each <li> item (based on your example for li[1])\n",
    "# X_NAME_REL      = \".//div[1]/div/div[2]/div[2]/span[1]/span\"\n",
    "# X_LOCATION_REL  = \".//div[1]/div/div[2]/div[2]/span[2]/span[2]\"\n",
    "# X_DATE_REL      = \".//div[1]/div/div[2]/div[1]/span/span[2]\"\n",
    "# # Link: try to grab an <a> within the card\n",
    "# X_LINK_REL      = \".//a[@href]\"\n",
    "\n",
    "# # ===================== DRIVER =====================\n",
    "# def build_driver(headless: bool = True) -> webdriver.Chrome:\n",
    "#     ua = (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "#           \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\")\n",
    "#     opts = webdriver.ChromeOptions()\n",
    "#     opts.add_argument(f\"user-agent={ua}\")\n",
    "#     if headless:\n",
    "#         opts.add_argument(\"--headless=new\")\n",
    "#     opts.add_argument(\"--disable-notifications\")\n",
    "#     opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "#     opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "#     opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "#     # Selenium Manager will fetch the correct driver automatically (Selenium 4.6+).\n",
    "#     return webdriver.Chrome(options=opts)\n",
    "\n",
    "# # ===================== HELPERS =====================\n",
    "# def wait_for(xpath: str, driver: webdriver.Chrome, timeout: int = WAIT_SEC):\n",
    "#     return WebDriverWait(driver, timeout).until(\n",
    "#         EC.presence_of_element_located((By.XPATH, xpath))\n",
    "#     )\n",
    "\n",
    "# def scroll_lazy(driver: webdriver.Chrome, max_rounds: int = 18, pause: float = 0.75):\n",
    "#     \"\"\"\n",
    "#     Scroll down repeatedly to trigger lazy loading.\n",
    "#     Stops early if page height no longer grows.\n",
    "#     \"\"\"\n",
    "#     last_h = 0\n",
    "#     for i in range(max_rounds):\n",
    "#         driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#         time.sleep(pause)\n",
    "#         h = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "#         if h == last_h:\n",
    "#             break\n",
    "#         last_h = h\n",
    "\n",
    "# def get_text_safe(node, rel_xpath: str) -> str:\n",
    "#     try:\n",
    "#         return node.find_element(By.XPATH, rel_xpath).text.strip()\n",
    "#     except NoSuchElementException:\n",
    "#         return \"\"\n",
    "#     except StaleElementReferenceException:\n",
    "#         try:\n",
    "#             return node.find_element(By.XPATH, rel_xpath).text.strip()\n",
    "#         except Exception:\n",
    "#             return \"\"\n",
    "\n",
    "# def get_link_safe(node, rel_xpath: str) -> str:\n",
    "#     try:\n",
    "#         a = node.find_element(By.XPATH, rel_xpath)\n",
    "#         href = a.get_attribute(\"href\") or \"\"\n",
    "#         return href.strip()\n",
    "#     except NoSuchElementException:\n",
    "#         return \"\"\n",
    "#     except StaleElementReferenceException:\n",
    "#         try:\n",
    "#             a = node.find_element(By.XPATH, rel_xpath)\n",
    "#             href = a.get_attribute(\"href\") or \"\"\n",
    "#             return href.strip()\n",
    "#         except Exception:\n",
    "#             return \"\"\n",
    "\n",
    "# # ===================== MAIN SCRAPER =====================\n",
    "# def scrape_ticketmaster_au(driver: webdriver.Chrome) -> pd.DataFrame:\n",
    "#     print(f\"Opening: {BASE_URL}\")\n",
    "#     driver.get(BASE_URL)\n",
    "\n",
    "#     # Wait until the section & list exist\n",
    "#     try:\n",
    "#         wait_for(X_SECTION, driver)\n",
    "#         wait_for(X_LIST, driver)\n",
    "#     except TimeoutException:\n",
    "#         raise TimeoutException(\"Could not find the events section on the page (XPaths may have changed).\")\n",
    "\n",
    "#     # Scroll to load all items\n",
    "#     scroll_lazy(driver, max_rounds=20, pause=0.8)\n",
    "\n",
    "#     # Grab items\n",
    "#     try:\n",
    "#         items = driver.find_elements(By.XPATH, X_ITEMS)\n",
    "#     except NoSuchElementException:\n",
    "#         items = []\n",
    "\n",
    "#     print(f\"Found {len(items)} event cards (raw).\")\n",
    "\n",
    "#     rows = []\n",
    "#     scraped_at = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "#     source = \"Ticketmaster AU\"\n",
    "\n",
    "#     for idx, li in enumerate(items, start=1):\n",
    "#         try:\n",
    "#             title    = get_text_safe(li, X_NAME_REL)\n",
    "#             location = get_text_safe(li, X_LOCATION_REL)\n",
    "#             date_str = get_text_safe(li, X_DATE_REL)\n",
    "#             link     = get_link_safe(li, X_LINK_REL)\n",
    "\n",
    "#             # Skip empty rows with no title\n",
    "#             if not title:\n",
    "#                 continue\n",
    "\n",
    "#             rows.append({\n",
    "#                 \"Title\": title,\n",
    "#                 \"Date\": date_str,\n",
    "#                 \"Location\": location,\n",
    "#                 \"Link\": link,\n",
    "#                 \"Source\": source,\n",
    "#                 \"ScrapedAtUTC\": scraped_at,\n",
    "#             })\n",
    "#         except Exception as e:\n",
    "#             # Keep going if one card explodes\n",
    "#             print(f\"[warn] Skipped li #{idx} due to error: {e}\")\n",
    "#             continue\n",
    "\n",
    "#     df = pd.DataFrame(rows, columns=[\"Title\", \"Date\", \"Location\", \"Link\", \"Source\", \"ScrapedAtUTC\"])\n",
    "#     # Deduplicate by Link, then by (Title, Date, Location)\n",
    "#     if not df.empty:\n",
    "#         before = len(df)\n",
    "#         df = df.drop_duplicates(subset=[\"Link\"]).copy()\n",
    "#         df = df.drop_duplicates(subset=[\"Title\", \"Date\", \"Location\"]).copy()\n",
    "#         after = len(df)\n",
    "#         print(f\"De-duplicated: {before} -> {after}\")\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def merge_with_existing(df_new: pd.DataFrame, path: str) -> pd.DataFrame:\n",
    "#     try:\n",
    "#         df_old = pd.read_csv(path)\n",
    "#         merged = pd.concat([df_old, df_new], ignore_index=True)\n",
    "#         # Global dedupe again\n",
    "#         merged = merged.drop_duplicates(subset=[\"Link\"]).drop_duplicates(subset=[\"Title\", \"Date\", \"Location\"])\n",
    "#         return merged\n",
    "#     except FileNotFoundError:\n",
    "#         return df_new\n",
    "\n",
    "# def main():\n",
    "#     driver = build_driver(headless=True)\n",
    "#     try:\n",
    "#         df = scrape_ticketmaster_au(driver)\n",
    "#     finally:\n",
    "#         try:\n",
    "#             driver.quit()\n",
    "#         except Exception:\n",
    "#             pass\n",
    "\n",
    "#     if df.empty:\n",
    "#         print(\"No events found (empty DataFrame).\")\n",
    "#         return\n",
    "\n",
    "#     out = merge_with_existing(df, OUTPUT_CSV)\n",
    "#     out.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "#     print(f\"\\nSaved {len(out)} total rows -> {OUTPUT_CSV}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         main()\n",
    "#     except Exception as e:\n",
    "#         print(f\"[fatal] {e}\")\n",
    "#         sys.exit(1)\n",
    "\n",
    "\n",
    "# # need to add code to show how many new events were added\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abfc69de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# megatix_section_scraper_hardened.py\n",
    "# has to wait because redirect too many times\n",
    "\n",
    "# import os\n",
    "# import re\n",
    "# import time\n",
    "# import pandas as pd\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.common.exceptions import (\n",
    "#     TimeoutException, NoSuchElementException, StaleElementReferenceException,\n",
    "#     ElementClickInterceptedException\n",
    "# )\n",
    "\n",
    "# # ----------------- CONFIG -----------------\n",
    "# START_URL = \"https://megatix.com/\"   # Megatix root (may redirect regionally)\n",
    "# TARGET_LIST_XP = \"//*[@id='__nuxt']/div/div[3]/div/main/section/section[2]/div[2]/div\"\n",
    "# FALLBACK_ALL_EVENTS_LINK_CSS = \"a[href*='/events']\"  # fallback to an All Events view\n",
    "# OUTPUT_CSV = \"all_austraila.csv\"\n",
    "# WAIT = 40\n",
    "\n",
    "# # Event page fields (your exact XPaths)\n",
    "# EV_NAME_XP  = \"//*[@id='megatix']/div/main/div[1]/div[3]/div[2]/div[2]/div[1]/div[1]/div[1]/h1\"\n",
    "# EV_DATE_XP  = \"//*[@id='megatix']/div/main/div[1]/div[3]/div[2]/div[2]/div[1]/div[2]/div[1]\"\n",
    "# EV_VENUE_XP = \"//*[@id='megatix']/div/main/div[1]/div[3]/div[2]/div[2]/div[1]/div[2]/div[2]/a\"\n",
    "# TICKETS_SECTION_XP = \"//*[@id='megatix']/div/main/div[1]/div[3]/div[2]/div[1]/div[2]/div\"\n",
    "\n",
    "# # ----------------- DRIVER -----------------\n",
    "# def driver_make():\n",
    "#     opts = webdriver.ChromeOptions()\n",
    "#     opts.add_argument(\"--start-maximized\")\n",
    "#     opts.add_argument(\"--disable-notifications\")\n",
    "#     opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "#     opts.add_argument(\"--lang=en-US,en\")\n",
    "#     # Optional: reduce automation detectability\n",
    "#     opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "#     opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "#     # Faster load behavior\n",
    "#     opts.page_load_strategy = \"eager\"\n",
    "#     drv = webdriver.Chrome(options=opts)\n",
    "\n",
    "#     drv.delete_all_cookies()   # ‚úÖ Clear cookies/cache to avoid redirect loops\n",
    "\n",
    "#     try:\n",
    "#         drv.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n",
    "#             \"source\": \"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\"\n",
    "#         })\n",
    "#     except Exception:\n",
    "#         pass\n",
    "#     return drv\n",
    "\n",
    "# # ----------------- UTILS -----------------\n",
    "# def wait_for(drv, by, sel, sec=WAIT):\n",
    "#     return WebDriverWait(drv, sec).until(EC.presence_of_element_located((by, sel)))\n",
    "\n",
    "# def wait_any(drv, candidates, sec=WAIT):\n",
    "#     \"\"\"Wait for the first present among multiple (by, sel) tuples.\"\"\"\n",
    "#     end = time.time() + sec\n",
    "#     last_err = None\n",
    "#     while time.time() < end:\n",
    "#         for by, sel in candidates:\n",
    "#             try:\n",
    "#                 el = drv.find_element(by, sel)\n",
    "#                 if el:\n",
    "#                     return el\n",
    "#             except Exception as e:\n",
    "#                 last_err = e\n",
    "#         time.sleep(0.25)\n",
    "#     raise TimeoutException(f\"None of the candidates appeared within {sec}s: {candidates}\") from last_err\n",
    "\n",
    "# def click_when_clickable(drv, by, sel, sec=WAIT):\n",
    "#     el = WebDriverWait(drv, sec).until(EC.element_to_be_clickable((by, sel)))\n",
    "#     el.click()\n",
    "#     return el\n",
    "\n",
    "# def smooth_scroll(drv, px=700, steps=12, pause=0.5):\n",
    "#     for _ in range(steps):\n",
    "#         drv.execute_script(f\"window.scrollBy(0,{px});\")\n",
    "#         time.sleep(pause)\n",
    "\n",
    "# def gentle_to(drv, el):\n",
    "#     try:\n",
    "#         drv.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", el)\n",
    "#         time.sleep(0.4)\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "# def dismiss_overlays(drv):\n",
    "#     # Cookies / consent / promo modals best-effort\n",
    "#     xps = [\n",
    "#         \"//button[contains(translate(.,'ACCEPT','accept'),'accept')]\",\n",
    "#         \"//button[contains(.,'Accept')]\",\n",
    "#         \"//button[contains(.,'Got it')]\",\n",
    "#         \"//button[contains(.,'Close')]\",\n",
    "#         \"//div[@role='dialog']//button\",\n",
    "#     ]\n",
    "#     for xp in xps:\n",
    "#         try:\n",
    "#             btn = WebDriverWait(drv, 2).until(EC.element_to_be_clickable((By.XPATH, xp)))\n",
    "#             btn.click()\n",
    "#             time.sleep(0.2)\n",
    "#         except Exception:\n",
    "#             pass\n",
    "\n",
    "# def wait_cloudflare(drv, max_sec=20):\n",
    "#     \"\"\"If Cloudflare 'Checking your browser...' appears, wait it out.\"\"\"\n",
    "#     start = time.time()\n",
    "#     while time.time() - start < max_sec:\n",
    "#         html = drv.page_source.lower()\n",
    "#         if \"checking your browser\" in html or \"just a moment\" in html:\n",
    "#             time.sleep(1.0)\n",
    "#         else:\n",
    "#             return\n",
    "#     # continue anyway; not fatal\n",
    "\n",
    "# def text_or_empty(drv, by, sel, sec=10):\n",
    "#     try:\n",
    "#         el = wait_for(drv, by, sel, sec=sec)\n",
    "#         return el.text.strip()\n",
    "#     except TimeoutException:\n",
    "#         return \"\"\n",
    "\n",
    "# CURRENCY_RE = re.compile(r\"([$‡∏ø‚Ç´‚Ç±‚Ç¨¬£‚Çπ]|AUD|SGD|MYR|THB|IDR|PHP|USD|EUR|GBP)\", re.I)\n",
    "\n",
    "# def extract_tickets_from_container(container_el):\n",
    "#     \"\"\"Collect ticket (name, price) by scanning anchor blocks inside tickets container.\"\"\"\n",
    "#     pairs = []\n",
    "#     links = container_el.find_elements(By.XPATH, \".//a\")\n",
    "#     for a in links:\n",
    "#         try:\n",
    "#             txt = a.text.strip()\n",
    "#             if not txt:\n",
    "#                 continue\n",
    "#             lines = [s.strip() for s in txt.splitlines() if s.strip()]\n",
    "#             name_guess, price_guess = \"\", \"\"\n",
    "#             for s in lines:\n",
    "#                 if not price_guess and (CURRENCY_RE.search(s) or re.search(r\"\\d\", s)):\n",
    "#                     price_guess = s\n",
    "#                 elif not name_guess:\n",
    "#                     name_guess = s\n",
    "#             if not name_guess and lines:\n",
    "#                 name_guess = lines[0]\n",
    "#             if not price_guess and len(lines) > 1:\n",
    "#                 price_guess = lines[-1]\n",
    "#             if name_guess or price_guess:\n",
    "#                 pairs.append((name_guess, price_guess))\n",
    "#         except StaleElementReferenceException:\n",
    "#             continue\n",
    "#     # De-dup (name, price)\n",
    "#     seen, out = set(), []\n",
    "#     for n, p in pairs:\n",
    "#         k = (n, p)\n",
    "#         if k not in seen:\n",
    "#             seen.add(k)\n",
    "#             out.append((n, p))\n",
    "#     return out\n",
    "\n",
    "# # ----------------- MAIN -----------------\n",
    "# def main():\n",
    "#     drv = driver_make()\n",
    "\n",
    "#     # Load existing CSV so we append but don't remove previous data\n",
    "#     existing = pd.DataFrame(columns=[\"Event Name\", \"Date\", \"Location\", \"Ticket Name\", \"Ticket Price\", \"Link\"])\n",
    "#     if os.path.exists(OUTPUT_CSV):\n",
    "#         try:\n",
    "#             existing = pd.read_csv(OUTPUT_CSV)\n",
    "#         except Exception:\n",
    "#             pass\n",
    "\n",
    "#     rows = []\n",
    "\n",
    "#     try:\n",
    "#         drv.get(START_URL)\n",
    "#         # Wait for a plausible app root: either Nuxt or Megatix root node or at least <main>\n",
    "#         try:\n",
    "#             wait_cloudflare(drv, max_sec=25)\n",
    "#             wait_any(\n",
    "#                 drv,\n",
    "#                 [\n",
    "#                     (By.CSS_SELECTOR, \"#__nuxt\"),\n",
    "#                     (By.CSS_SELECTOR, \"#megatix\"),\n",
    "#                     (By.TAG_NAME, \"main\"),\n",
    "#                     (By.XPATH, \"//*[@id='__nuxt']\"),\n",
    "#                 ],\n",
    "#                 sec=WAIT,\n",
    "#             )\n",
    "#         except TimeoutException:\n",
    "#             # As a last resort, continue; some builds render later\n",
    "#             pass\n",
    "\n",
    "#         dismiss_overlays(drv)\n",
    "#         smooth_scroll(drv, px=700, steps=8, pause=0.6)\n",
    "\n",
    "#         # Try to find the target list; if missing, try a fallback ‚ÄúAll Events‚Äù link then wait again\n",
    "#         def get_list_element(retries=6):\n",
    "#             for i in range(retries):\n",
    "#                 try:\n",
    "#                     lst = drv.find_element(By.XPATH, TARGET_LIST_XP)\n",
    "#                     if lst.is_displayed():\n",
    "#                         return lst\n",
    "#                 except Exception:\n",
    "#                     pass\n",
    "#                 # try clicking an /events link to land on a full listing\n",
    "#                 try:\n",
    "#                     a = drv.find_element(By.CSS_SELECTOR, FALLBACK_ALL_EVENTS_LINK_CSS)\n",
    "#                     a.click()\n",
    "#                     wait_cloudflare(drv, max_sec=20)\n",
    "#                     time.sleep(1.2)\n",
    "#                 except Exception:\n",
    "#                     pass\n",
    "#                 smooth_scroll(drv, px=900, steps=2, pause=0.7)\n",
    "#             return None\n",
    "\n",
    "#         lst = get_list_element()\n",
    "#         if not lst:\n",
    "#             # Still not found: search any section that contains many <article> cards under main\n",
    "#             try:\n",
    "#                 lst = WebDriverWait(drv, 15).until(\n",
    "#                     EC.presence_of_element_located(\n",
    "#                         (By.XPATH, \"//main//section[.//article]/section/div[2]/div | //main//section//div[.//article]\")\n",
    "#                     )\n",
    "#                 )\n",
    "#             except TimeoutException:\n",
    "#                 raise TimeoutException(\"Could not locate the listing container; site layout likely different/region-gated.\")\n",
    "\n",
    "#         # Ensure as many cards as possible are loaded\n",
    "#         prev_count = -1\n",
    "#         for _ in range(12):\n",
    "#             cards = lst.find_elements(By.XPATH, \".//article\")\n",
    "#             if len(cards) == prev_count:\n",
    "#                 break\n",
    "#             prev_count = len(cards)\n",
    "#             try:\n",
    "#                 drv.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight;\", lst)\n",
    "#             except Exception:\n",
    "#                 pass\n",
    "#             drv.execute_script(\"window.scrollBy(0, 1000);\")\n",
    "#             time.sleep(0.8)\n",
    "\n",
    "#         # Collect event hrefs (don‚Äôt click yet‚Äîavoid stale issues on back())\n",
    "#         cards = lst.find_elements(By.XPATH, \".//article\")\n",
    "#         hrefs = []\n",
    "#         for c in cards:\n",
    "#             try:\n",
    "#                 a = c.find_element(By.XPATH, \".//a[.//div or .//h3 or .//h2]\")\n",
    "#                 href = a.get_attribute(\"href\")\n",
    "#                 if href and href.startswith(\"http\"):\n",
    "#                     hrefs.append(href)\n",
    "#             except NoSuchElementException:\n",
    "#                 continue\n",
    "\n",
    "#         # Visit each event page and extract info\n",
    "#         for href in hrefs:\n",
    "#             try:\n",
    "#                 drv.get(href)\n",
    "#                 wait_cloudflare(drv, max_sec=20)\n",
    "#                 # Wait for event summary (some pages lazy-load)\n",
    "#                 try:\n",
    "#                     WebDriverWait(drv, 20).until(\n",
    "#                         EC.presence_of_element_located((By.XPATH, EV_NAME_XP))\n",
    "#                     )\n",
    "#                 except TimeoutException:\n",
    "#                     # try a small scroll to trigger hydration\n",
    "#                     smooth_scroll(drv, px=600, steps=2, pause=0.5)\n",
    "#                     wait_for(drv, By.XPATH, EV_NAME_XP, sec=15)\n",
    "\n",
    "#                 ev_name  = text_or_empty(drv, By.XPATH, EV_NAME_XP, 12)\n",
    "#                 ev_date  = text_or_empty(drv, By.XPATH, EV_DATE_XP, 12)\n",
    "#                 ev_venue = text_or_empty(drv, By.XPATH, EV_VENUE_XP, 12)\n",
    "#                 ev_link  = drv.current_url\n",
    "\n",
    "#                 tickets = []\n",
    "#                 try:\n",
    "#                     tsec = wait_for(drv, By.XPATH, TICKETS_SECTION_XP, sec=12)\n",
    "#                     tickets = extract_tickets_from_container(tsec)\n",
    "#                 except TimeoutException:\n",
    "#                     tickets = []\n",
    "\n",
    "#                 if tickets:\n",
    "#                     for tname, tprice in tickets:\n",
    "#                         rows.append({\n",
    "#                             \"Event Name\": ev_name,\n",
    "#                             \"Date\": ev_date,\n",
    "#                             \"Location\": ev_venue,\n",
    "#                             \"Ticket Name\": tname,\n",
    "#                             \"Ticket Price\": tprice,\n",
    "#                             \"Link\": ev_link\n",
    "#                         })\n",
    "#                 else:\n",
    "#                     rows.append({\n",
    "#                         \"Event Name\": ev_name,\n",
    "#                         \"Date\": ev_date,\n",
    "#                         \"Location\": ev_venue,\n",
    "#                         \"Ticket Name\": \"\",\n",
    "#                         \"Ticket Price\": \"\",\n",
    "#                         \"Link\": ev_link\n",
    "#                     })\n",
    "\n",
    "#             except TimeoutException:\n",
    "#                 # Skip and continue\n",
    "#                 continue\n",
    "#             except ElementClickInterceptedException:\n",
    "#                 continue\n",
    "\n",
    "#         # Save (append w/ de-dup, don‚Äôt remove older rows)\n",
    "#         new_df = pd.DataFrame(rows, columns=[\"Event Name\", \"Date\", \"Location\", \"Ticket Name\", \"Ticket Price\", \"Link\"])\n",
    "#         combined = pd.concat([existing, new_df], ignore_index=True)\n",
    "#         combined[\"__k\"] = (\n",
    "#             combined[\"Link\"].astype(str).fillna(\"\")\n",
    "#             + \" | \" + combined[\"Ticket Name\"].astype(str).fillna(\"\")\n",
    "#             + \" | \" + combined[\"Ticket Price\"].astype(str).fillna(\"\")\n",
    "#         )\n",
    "#         combined = combined.drop_duplicates(subset=\"__k\").drop(columns=\"__k\")\n",
    "#         combined.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "#         print(f\"Saved {len(combined)} total rows to {OUTPUT_CSV} \"\n",
    "#               f\"(added {len(combined) - len(existing)} new rows).\")\n",
    "\n",
    "#     finally:\n",
    "#         try:\n",
    "#             drv.quit()\n",
    "#         except Exception:\n",
    "#             pass\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4638ff9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 159\u001b[39m\n\u001b[32m    155\u001b[39m         driver.quit()\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 148\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    146\u001b[39m driver = build_driver(headless=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     new_events = scrape_events(driver)\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m new_events:\n\u001b[32m    150\u001b[39m         save_events_to_csv(CSV_FILE, new_events)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36mscrape_events\u001b[39m\u001b[34m(driver)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscrape_events\u001b[39m(driver):\n\u001b[32m     95\u001b[39m     driver.get(URL)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     wait_for_page_and_lazy_load(driver)\n\u001b[32m     98\u001b[39m     \u001b[38;5;66;03m# Select all event <article> elements in the list\u001b[39;00m\n\u001b[32m     99\u001b[39m     articles = driver.find_elements(\n\u001b[32m    100\u001b[39m         By.XPATH,\n\u001b[32m    101\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m//*[@id=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwhats-on__ajax-wrapper\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m]/section/div[2]/div/article\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    102\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mwait_for_page_and_lazy_load\u001b[39m\u001b[34m(driver)\u001b[39m\n\u001b[32m     36\u001b[39m wait = WebDriverWait(driver, \u001b[32m20\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Wait until at least one event article is present\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m wait.until(\n\u001b[32m     40\u001b[39m     EC.presence_of_element_located(\n\u001b[32m     41\u001b[39m         (\n\u001b[32m     42\u001b[39m             By.XPATH,\n\u001b[32m     43\u001b[39m             \u001b[33m'\u001b[39m\u001b[33m//*[@id=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwhats-on__ajax-wrapper\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m]/section/div[2]/div/article[1]\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     44\u001b[39m         )\n\u001b[32m     45\u001b[39m     )\n\u001b[32m     46\u001b[39m )\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Simple scroll loop in case of lazy loading\u001b[39;00m\n\u001b[32m     49\u001b[39m last_height = driver.execute_script(\u001b[33m\"\u001b[39m\u001b[33mreturn document.body.scrollHeight\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saiva\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:129\u001b[39m, in \u001b[36mWebDriverWait.until\u001b[39m\u001b[34m(self, method, message)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m         value = method(\u001b[38;5;28mself\u001b[39m._driver)\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m value:\n\u001b[32m    131\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saiva\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\support\\expected_conditions.py:104\u001b[39m, in \u001b[36mpresence_of_element_located.<locals>._predicate\u001b[39m\u001b[34m(driver)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_predicate\u001b[39m(driver: WebDriverOrWebElement):\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m driver.find_element(*locator)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saiva\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:926\u001b[39m, in \u001b[36mWebDriver.find_element\u001b[39m\u001b[34m(self, by, value)\u001b[39m\n\u001b[32m    923\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m NoSuchElementException(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot locate relative element with: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mby.root\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m elements[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.execute(Command.FIND_ELEMENT, {\u001b[33m\"\u001b[39m\u001b[33musing\u001b[39m\u001b[33m\"\u001b[39m: by, \u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m: value})[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saiva\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:455\u001b[39m, in \u001b[36mWebDriver.execute\u001b[39m\u001b[34m(self, driver_command, params)\u001b[39m\n\u001b[32m    452\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msessionId\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[32m    453\u001b[39m         params[\u001b[33m\"\u001b[39m\u001b[33msessionId\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.session_id\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m response = cast(RemoteConnection, \u001b[38;5;28mself\u001b[39m.command_executor).execute(driver_command, params)\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28mself\u001b[39m.error_handler.check_response(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saiva\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:405\u001b[39m, in \u001b[36mRemoteConnection.execute\u001b[39m\u001b[34m(self, command, params)\u001b[39m\n\u001b[32m    403\u001b[39m trimmed = \u001b[38;5;28mself\u001b[39m._trim_large_entries(params)\n\u001b[32m    404\u001b[39m LOGGER.debug(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, command_info[\u001b[32m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(command_info[\u001b[32m0\u001b[39m], url, body=data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saiva\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:429\u001b[39m, in \u001b[36mRemoteConnection._request\u001b[39m\u001b[34m(self, method, url, body)\u001b[39m\n\u001b[32m    426\u001b[39m     body = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client_config.keep_alive:\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m     response = \u001b[38;5;28mself\u001b[39m._conn.request(method, url, body=body, headers=headers, timeout=\u001b[38;5;28mself\u001b[39m._client_config.timeout)\n\u001b[32m    430\u001b[39m     statuscode = response.status\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saiva\\anaconda3\\Lib\\site-packages\\urllib3\\_request_methods.py:143\u001b[39m, in \u001b[36mRequestMethods.request\u001b[39m\u001b[34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[39m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request_encode_url(\n\u001b[32m    136\u001b[39m         method,\n\u001b[32m    137\u001b[39m         url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m         **urlopen_kw,\n\u001b[32m    141\u001b[39m     )\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request_encode_body(\n\u001b[32m    144\u001b[39m         method, url, fields=fields, headers=headers, **urlopen_kw\n\u001b[32m    145\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saiva\\anaconda3\\Lib\\site-packages\\urllib3\\_request_methods.py:278\u001b[39m, in \u001b[36mRequestMethods.request_encode_body\u001b[39m\u001b[34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[39m\n\u001b[32m    274\u001b[39m     extra_kw[\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m].setdefault(\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m, content_type)\n\u001b[32m    276\u001b[39m extra_kw.update(urlopen_kw)\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.urlopen(method, url, **extra_kw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saiva\\anaconda3\\Lib\\site-packages\\urllib3\\poolmanager.py:459\u001b[39m, in \u001b[36mPoolManager.urlopen\u001b[39m\u001b[34m(self, method, url, redirect, **kw)\u001b[39m\n\u001b[32m    457\u001b[39m     response = conn.urlopen(method, url, **kw)\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     response = conn.urlopen(method, u.request_uri, **kw)\n\u001b[32m    461\u001b[39m redirect_location = redirect \u001b[38;5;129;01mand\u001b[39;00m response.get_redirect_location()\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saiva\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28mself\u001b[39m._make_request(\n\u001b[32m    788\u001b[39m     conn,\n\u001b[32m    789\u001b[39m     method,\n\u001b[32m    790\u001b[39m     url,\n\u001b[32m    791\u001b[39m     timeout=timeout_obj,\n\u001b[32m    792\u001b[39m     body=body,\n\u001b[32m    793\u001b[39m     headers=headers,\n\u001b[32m    794\u001b[39m     chunked=chunked,\n\u001b[32m    795\u001b[39m     retries=retries,\n\u001b[32m    796\u001b[39m     response_conn=response_conn,\n\u001b[32m    797\u001b[39m     preload_content=preload_content,\n\u001b[32m    798\u001b[39m     decode_content=decode_content,\n\u001b[32m    799\u001b[39m     **response_kw,\n\u001b[32m    800\u001b[39m )\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saiva\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = conn.getresponse()\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saiva\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28msuper\u001b[39m().getresponse()\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saiva\\anaconda3\\Lib\\http\\client.py:1378\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1377\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1378\u001b[39m         response.begin()\n\u001b[32m   1379\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1380\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saiva\\anaconda3\\Lib\\http\\client.py:318\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     version, status, reason = \u001b[38;5;28mself\u001b[39m._read_status()\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    320\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saiva\\anaconda3\\Lib\\http\\client.py:279\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    281\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saiva\\anaconda3\\Lib\\socket.py:706\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sock.recv_into(b)\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    708\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# # code has issues\n",
    "# import os\n",
    "# import csv\n",
    "# import time\n",
    "# from datetime import datetime, timezone\n",
    "\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "# URL = \"https://wfac.org.au/whats-on/?type=event&pg=1\"\n",
    "# CSV_FILE = \"all_australia.csv\"\n",
    "# EVENT_LOCATION = \"walyalup arts centre\"\n",
    "\n",
    "\n",
    "# def build_driver(headless: bool = True):\n",
    "#     chrome_options = Options()\n",
    "#     if headless:\n",
    "#         chrome_options.add_argument(\"--headless=new\")\n",
    "#     chrome_options.add_argument(\"--disable-gpu\")\n",
    "#     chrome_options.add_argument(\"--no-sandbox\")\n",
    "#     chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "#     chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "#     driver = webdriver.Chrome(options=chrome_options)\n",
    "#     return driver\n",
    "\n",
    "\n",
    "# def wait_for_page_and_lazy_load(driver):\n",
    "#     \"\"\"\n",
    "#     Wait for the events list to appear, then scroll a bit to ensure\n",
    "#     everything lazy-loads. Does NOT touch any popup.\n",
    "#     \"\"\"\n",
    "#     wait = WebDriverWait(driver, 20)\n",
    "\n",
    "#     # Wait until at least one event article is present\n",
    "#     wait.until(\n",
    "#         EC.presence_of_element_located(\n",
    "#             (\n",
    "#                 By.XPATH,\n",
    "#                 '//*[@id=\"whats-on__ajax-wrapper\"]/section/div[2]/div/article[1]'\n",
    "#             )\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     # Simple scroll loop in case of lazy loading\n",
    "#     last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#     for _ in range(3):\n",
    "#         driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#         time.sleep(1.5)\n",
    "#         new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#         if new_height == last_height:\n",
    "#             break\n",
    "#         last_height = new_height\n",
    "\n",
    "\n",
    "# def load_existing_event_names(csv_path):\n",
    "#     \"\"\"\n",
    "#     Returns a set of event_name values that already exist in the CSV.\n",
    "#     Used to avoid adding duplicates across runs.\n",
    "#     \"\"\"\n",
    "#     existing_names = set()\n",
    "#     if not os.path.exists(csv_path):\n",
    "#         return existing_names\n",
    "\n",
    "#     with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "#         reader = csv.DictReader(f)\n",
    "#         for row in reader:\n",
    "#             name = (row.get(\"event_name\") or \"\").strip()\n",
    "#             if name:\n",
    "#                 existing_names.add(name)\n",
    "#     return existing_names\n",
    "\n",
    "\n",
    "# def save_events_to_csv(csv_path, events):\n",
    "#     \"\"\"\n",
    "#     Append new events to the CSV, ensuring:\n",
    "#     - Header is written if file does not exist.\n",
    "#     - Existing rows are kept.\n",
    "#     \"\"\"\n",
    "#     file_exists = os.path.exists(csv_path)\n",
    "#     fieldnames = [\"event_name\", \"event_date\", \"event_location\", \"time_scraped_utc\"]\n",
    "\n",
    "#     with open(csv_path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "#         writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "#         if not file_exists:\n",
    "#             writer.writeheader()\n",
    "#         for ev in events:\n",
    "#             writer.writerow(ev)\n",
    "\n",
    "\n",
    "# def scrape_events(driver):\n",
    "#     driver.get(URL)\n",
    "#     wait_for_page_and_lazy_load(driver)\n",
    "\n",
    "#     # Select all event <article> elements in the list\n",
    "#     articles = driver.find_elements(\n",
    "#         By.XPATH,\n",
    "#         '//*[@id=\"whats-on__ajax-wrapper\"]/section/div[2]/div/article'\n",
    "#     )\n",
    "\n",
    "#     existing_names = load_existing_event_names(CSV_FILE)\n",
    "#     seen_this_run = set()\n",
    "#     new_events = []\n",
    "#     current_utc = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "#     for article in articles:\n",
    "#         try:\n",
    "#             # Event name: //*[@id=\"whats-on__ajax-wrapper\"]/section/div[2]/div/article[1]/h3\n",
    "#             name_elem = article.find_element(By.XPATH, \"./h3\")\n",
    "#             event_name = name_elem.text.strip()\n",
    "#         except Exception:\n",
    "#             event_name = \"\"\n",
    "\n",
    "#         try:\n",
    "#             # Event date: //*[@id=\"whats-on__ajax-wrapper\"]/section/div[2]/div/article[1]/div[1]/span\n",
    "#             date_elem = article.find_element(By.XPATH, \"./div[1]/span\")\n",
    "#             event_date = date_elem.text.strip()\n",
    "#         except Exception:\n",
    "#             event_date = \"\"\n",
    "\n",
    "#         if not event_name:\n",
    "#             continue\n",
    "\n",
    "#         # De-dupe by event name (existing CSV + within this run)\n",
    "#         if event_name in existing_names or event_name in seen_this_run:\n",
    "#             continue\n",
    "\n",
    "#         seen_this_run.add(event_name)\n",
    "\n",
    "#         new_events.append(\n",
    "#             {\n",
    "#                 \"event_name\": event_name,\n",
    "#                 \"event_date\": event_date,\n",
    "#                 \"event_location\": EVENT_LOCATION,\n",
    "#                 \"time_scraped_utc\": current_utc,\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#     return new_events\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     driver = build_driver(headless=True)\n",
    "#     try:\n",
    "#         new_events = scrape_events(driver)\n",
    "#         if new_events:\n",
    "#             save_events_to_csv(CSV_FILE, new_events)\n",
    "#             print(f\"Added {len(new_events)} new events to {CSV_FILE}.\")\n",
    "#         else:\n",
    "#             print(\"No new events found. CSV unchanged.\")\n",
    "#     finally:\n",
    "#         driver.quit()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac728a",
   "metadata": {},
   "source": [
    "# Bali (done)\n",
    "- as of 13/11/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caae7ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95337e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended 13 new rows. Total now: 151\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Savaya Bali ‚Äî Events scraper (append-only, no duplicates)\n",
    "Site: https://www.savaya.com/\n",
    "\n",
    "What it does\n",
    "- Opens the landing page\n",
    "- Clicks the element at /html/body/main/section[1]/div/div[1]/a if present\n",
    "- Slow-scrolls to trigger lazy loading of all event cards\n",
    "- Scrapes Event Name + Event Date using your given XPaths as anchors\n",
    "- Also attempts to capture the event link (anchor href) when available\n",
    "- Appends only new rows to savaya_events.csv (never deletes existing rows)\n",
    "- Adds per-row ScrapedAtUTC (UTC ISO8601)\n",
    "\n",
    "Requirements:\n",
    "    pip install selenium pandas\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException, NoSuchElementException, ElementClickInterceptedException,\n",
    "    StaleElementReferenceException\n",
    ")\n",
    "\n",
    "URL = \"https://www.savaya.com/\"\n",
    "OUT_CSV = \"all_bali.csv\"\n",
    "\n",
    "# ---------------- Your provided XPaths ----------------\n",
    "X_CLICK = \"/html/body/main/section[1]/div/div[1]/a\"  # click this if present (e.g., banner/CTA)\n",
    "# Example event card fields for ONE card (we'll generalize from this):\n",
    "X_NAME_SAMPLE = '//*[@id=\"w-node-_9de959fa-b5bd-ae81-b76c-b5857691ce18-62724cfd\"]/div/a/div/div[2]/h3'\n",
    "X_DATE_SAMPLE = '//*[@id=\"w-node-_9de959fa-b5bd-ae81-b76c-b5857691ce18-62724cfd\"]/div/a/div/div[1]/div'\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def utc_now_iso():\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    return \" \".join(s.strip().lower().split())\n",
    "\n",
    "def build_uid(name: str, date_str: str, link: str) -> str:\n",
    "    return f\"{norm(name)}|{norm(date_str)}|{norm(link)}\"\n",
    "\n",
    "def slow_scroll_to_bottom(driver, step=700, pause=0.35, settle=0.8, max_passes=5):\n",
    "    \"\"\"\n",
    "    Gentle, repeated step-scrolling to trigger lazy loading.\n",
    "    \"\"\"\n",
    "    for _ in range(max_passes):\n",
    "        last_h = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "        y = 0\n",
    "        while y < last_h:\n",
    "            y += step\n",
    "            driver.execute_script(f\"window.scrollTo(0, {y});\")\n",
    "            time.sleep(pause)\n",
    "        time.sleep(settle)\n",
    "        new_h = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "        if new_h == last_h:\n",
    "            break\n",
    "\n",
    "def try_click(driver, xpath):\n",
    "    try:\n",
    "        el = WebDriverWait(driver, 6).until(EC.element_to_be_clickable((By.XPATH, xpath)))\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", el)\n",
    "        time.sleep(0.3)\n",
    "        el.click()\n",
    "        time.sleep(0.8)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def find_event_container(driver):\n",
    "    \"\"\"\n",
    "    Strategy:\n",
    "    1) Find your sample NAME element (X_NAME_SAMPLE).\n",
    "    2) Walk up to a common container that holds ALL event cards.\n",
    "    3) From that container, collect all anchor cards and read their name/date via RELATIVE XPaths.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sample_h3 = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, X_NAME_SAMPLE)))\n",
    "    except TimeoutException:\n",
    "        return None\n",
    "\n",
    "    # Climb up a few levels to get the repeating list container (Webflow grids are usually a few divs up).\n",
    "    # We‚Äôll try progressively broader ancestors until we see multiple <a> children with h3s.\n",
    "    ancestors = [\n",
    "        \"./../../..\",           # div/div[2]/h3 -> div/div[2] -> div/div -> div (card)\n",
    "        \"./../../../..\",        # up one more\n",
    "        \"./../../../../..\",     # up two more\n",
    "        \"./../../../../../..\",  # up three more\n",
    "        \"./../../../../../../..\" # up four more\n",
    "    ]\n",
    "\n",
    "    for anc in ancestors:\n",
    "        try:\n",
    "            container = sample_h3.find_element(By.XPATH, anc)\n",
    "            # Heuristic: a container with multiple anchors that have an h3 child\n",
    "            anchors = container.find_elements(By.XPATH, \".//a[.//h3]\")\n",
    "            if len(anchors) >= 2:\n",
    "                return container\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def collect_cards(container):\n",
    "    \"\"\"\n",
    "    From the container, get all anchor 'cards' that contain an h3.\n",
    "    Return list of dicts with name, date, link.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    anchors = container.find_elements(By.XPATH, \".//a[.//h3]\")\n",
    "    for a in anchors:\n",
    "        try:\n",
    "            # Name: relative to each anchor (mirroring your provided path endings)\n",
    "            name_el = a.find_element(By.XPATH, \".//div/div[2]/h3\")\n",
    "        except NoSuchElementException:\n",
    "            # Fallback: any h3 within the anchor\n",
    "            try:\n",
    "                name_el = a.find_element(By.XPATH, \".//h3\")\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "\n",
    "        # Date block: relative to each anchor; primary and fallback\n",
    "        date_text = \"\"\n",
    "        try:\n",
    "            date_el = a.find_element(By.XPATH, \".//div/div[1]/div\")\n",
    "            date_text = date_el.text.strip()\n",
    "        except NoSuchElementException:\n",
    "            # Fallbacks: sometimes the date might be in a sibling text node or alternative class\n",
    "            try:\n",
    "                # Any div/span near the top portion that looks like a date\n",
    "                date_el = a.find_element(By.XPATH, \".//*[self::div or self::span][1]\")\n",
    "                date_text = date_el.text.strip()\n",
    "            except NoSuchElementException:\n",
    "                date_text = \"\"\n",
    "\n",
    "        name = name_el.text.strip()\n",
    "        link = a.get_attribute(\"href\") or \"\"\n",
    "\n",
    "        if name:  # only keep cards with a name\n",
    "            rows.append({\n",
    "                \"Event Name\": name,\n",
    "                \"Event Date\": date_text,\n",
    "                \"Link\": link\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "def append_only_new(df_new: pd.DataFrame, out_csv: str):\n",
    "    \"\"\"\n",
    "    Append only rows whose uid is NOT already present in out_csv.\n",
    "    Keep all existing rows; do not modify their ScrapedAtUTC.\n",
    "    \"\"\"\n",
    "    if os.path.exists(out_csv):\n",
    "        df_old = pd.read_csv(out_csv)\n",
    "        if \"uid\" not in df_old.columns:\n",
    "            # Backfill for legacy files (best-effort)\n",
    "            df_old[\"uid\"] = (\n",
    "                df_old.get(\"Event Name\", \"\").fillna(\"\").apply(norm) + \"|\" +\n",
    "                df_old.get(\"Event Date\", \"\").fillna(\"\").apply(norm) + \"|\" +\n",
    "                df_old.get(\"Link\", \"\").fillna(\"\").apply(norm)\n",
    "            )\n",
    "    else:\n",
    "        df_old = pd.DataFrame(columns=[\"Event Name\",\"Event Date\",\"Link\",\"ScrapedAtUTC\",\"uid\"])\n",
    "\n",
    "    old_uids = set(df_old.get(\"uid\", pd.Series(dtype=str)).astype(str))\n",
    "    df_new = df_new.copy()\n",
    "    df_new[\"uid\"] = df_new[\"uid\"].astype(str)\n",
    "\n",
    "    df_to_add = df_new[~df_new[\"uid\"].isin(old_uids)]\n",
    "    if df_to_add.empty:\n",
    "        print(\"No new events to append. Existing file left unchanged.\")\n",
    "        return\n",
    "\n",
    "    df_out = pd.concat([df_old, df_to_add], ignore_index=True)\n",
    "    cols = [\"Event Name\",\"Event Date\",\"Link\",\"ScrapedAtUTC\",\"uid\"]\n",
    "    df_out = df_out[[c for c in cols if c in df_out.columns] + [c for c in df_out.columns if c not in cols]]\n",
    "    df_out.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Appended {len(df_to_add)} new rows. Total now: {len(df_out)}\")\n",
    "\n",
    "def main(headless=False):\n",
    "    # --- Browser setup ---\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--start-maximized\")\n",
    "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--window-size=1400,1000\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opts.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "    wait = WebDriverWait(driver, 15)\n",
    "\n",
    "    try:\n",
    "        driver.get(URL)\n",
    "\n",
    "        # Optional click (your provided XPath). If it isn't there, we just carry on.\n",
    "        try_click(driver, X_CLICK)\n",
    "\n",
    "        # Initial slow scroll to let cards render\n",
    "        slow_scroll_to_bottom(driver, step=700, pause=0.35, settle=0.8, max_passes=4)\n",
    "\n",
    "        # Find the repeating container from your sample XPaths\n",
    "        container = find_event_container(driver)\n",
    "        if not container:\n",
    "            # One more scroll attempt (some sites load later)\n",
    "            slow_scroll_to_bottom(driver, step=900, pause=0.4, settle=1.0, max_passes=3)\n",
    "            container = find_event_container(driver)\n",
    "\n",
    "        if not container:\n",
    "            print(\"Could not locate the events container. Site structure may have changed.\")\n",
    "            return\n",
    "\n",
    "        # Final scroll (safety) and collect\n",
    "        slow_scroll_to_bottom(driver, step=900, pause=0.35, settle=0.8, max_passes=2)\n",
    "        rows = collect_cards(container)\n",
    "\n",
    "        if not rows:\n",
    "            print(\"No events found. Check XPaths or if content is gated by a modal.\")\n",
    "            return\n",
    "\n",
    "        # Build DataFrame with per-row timestamp & stable uid\n",
    "        now_iso = utc_now_iso()\n",
    "        for r in rows:\n",
    "            r[\"ScrapedAtUTC\"] = now_iso\n",
    "            r[\"uid\"] = build_uid(r.get(\"Event Name\",\"\"), r.get(\"Event Date\",\"\"), r.get(\"Link\",\"\"))\n",
    "\n",
    "        df_new = pd.DataFrame(rows, columns=[\"Event Name\",\"Event Date\",\"Link\",\"ScrapedAtUTC\",\"uid\"])\n",
    "\n",
    "        # Append-only write\n",
    "        append_only_new(df_new, OUT_CSV)\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(headless=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c215d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ra_bali_list_scraper.py\n",
    "# # Scrape RA Bali listing page (no detail clicks) with the user's XPaths.\n",
    "# # - Prevents lazy-loading misses (incremental scroll until stable)\n",
    "# # - Appends to CSV without removing previous rows\n",
    "# # - De-dupes by event link (fallback: Title+Date)\n",
    "# # - Includes TimeScraped\n",
    "\n",
    "# import os\n",
    "# import time\n",
    "# import pandas as pd\n",
    "# from datetime import datetime\n",
    "# from typing import List, Dict, Set\n",
    "\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# # ---------------- CONFIG ----------------\n",
    "# BASE_URL = \"https://ra.co/events/id/bali\"\n",
    "# OUTPUT_CSV = \"all_bali_list.csv\"\n",
    "# CITY = \"Bali\"\n",
    "\n",
    "# # Section UL (your XPath)\n",
    "# UL_XP = \"//*[@id='__next']/div[3]/div[3]/div[2]/section/div/div/div[2]/div[2]/div/div/ul\"\n",
    "# LI_REL_XP = \"./li\"\n",
    "\n",
    "# # Per-card fields (generalized from your li[1] examples)\n",
    "# # event name (you gave the li[1] path; we adapt to \"this li\")\n",
    "# NAME_REL_XP = \".//div/h3\"\n",
    "# # event venue ‚Äî you provided the same XPath as name, so we‚Äôll take the same text.\n",
    "# VENUE_REL_XP = \".//div/h3\"       # if RA shows venue elsewhere, update this\n",
    "# # event date\n",
    "# DATE_REL_XP = \".//div/div[2]/span\"\n",
    "# # link (often on <a> inside h3)\n",
    "# LINK_REL_CSS = \"h3 a, a[href*='/events/']\"\n",
    "\n",
    "# # ---------------- DRIVER ----------------\n",
    "# def make_driver(headless: bool = False) -> webdriver.Chrome:\n",
    "#     opts = Options()\n",
    "#     if headless:\n",
    "#         opts.add_argument(\"--headless=new\")\n",
    "#     opts.add_argument(\"--window-size=1400,1000\")\n",
    "#     opts.add_argument(\"--disable-gpu\")\n",
    "#     opts.add_argument(\"--no-sandbox\")\n",
    "#     opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "#     # slightly less detectable\n",
    "#     opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "#     opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "#     service = Service(ChromeDriverManager().install())\n",
    "#     drv = webdriver.Chrome(service=service, options=opts)\n",
    "#     return drv\n",
    "\n",
    "# # ---------------- HELPERS ----------------\n",
    "# def now_stamp() -> str:\n",
    "#     return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# def slow_load_list(driver: webdriver.Chrome, list_xpath: str, item_rel_xpath: str = \"./li\",\n",
    "#                    pause: float = 1.0, max_passes: int = 50) -> None:\n",
    "#     \"\"\"\n",
    "#     Incremental scrolling until the number of items stabilizes twice or max_passes reached.\n",
    "#     Prevents missing items due to lazy loading.\n",
    "#     \"\"\"\n",
    "#     wait = WebDriverWait(driver, 25)\n",
    "#     wait.until(EC.presence_of_element_located((By.XPATH, list_xpath)))\n",
    "\n",
    "#     last = 0\n",
    "#     stable = 0\n",
    "#     for _ in range(max_passes):\n",
    "#         driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#         time.sleep(pause)\n",
    "#         ul = wait.until(EC.presence_of_element_located((By.XPATH, list_xpath)))\n",
    "#         count = len(ul.find_elements(By.XPATH, item_rel_xpath))\n",
    "#         if count == last:\n",
    "#             stable += 1\n",
    "#         else:\n",
    "#             stable = 0\n",
    "#         last = count\n",
    "#         if stable >= 2:\n",
    "#             break\n",
    "\n",
    "#     driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "#     time.sleep(0.5)\n",
    "\n",
    "# def get_text_or_blank(el, by: By, sel: str) -> str:\n",
    "#     try:\n",
    "#         t = el.find_element(by, sel).text.strip()\n",
    "#         return t\n",
    "#     except Exception:\n",
    "#         return \"\"\n",
    "\n",
    "# def get_link(el) -> str:\n",
    "#     try:\n",
    "#         a = el.find_element(By.CSS_SELECTOR, LINK_REL_CSS)\n",
    "#         href = a.get_attribute(\"href\")\n",
    "#         return (href or \"\").strip()\n",
    "#     except Exception:\n",
    "#         return \"\"\n",
    "\n",
    "# def append_dedup_csv(new_rows: List[Dict], csv_path: str,\n",
    "#                      key_cols=(\"Link\", \"Title\", \"Date\")) -> int:\n",
    "#     new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "#     if os.path.exists(csv_path):\n",
    "#         old = pd.read_csv(csv_path)\n",
    "#         # Align columns across old and new\n",
    "#         all_cols = list(dict.fromkeys(list(old.columns) + list(new_df.columns)))\n",
    "#         old = old.reindex(columns=all_cols)\n",
    "#         new_df = new_df.reindex(columns=all_cols)\n",
    "#         out = pd.concat([old, new_df], ignore_index=True)\n",
    "#     else:\n",
    "#         out = new_df\n",
    "\n",
    "#     # Ensure TimeScraped is str (prevents NaN)\n",
    "#     if \"TimeScraped\" in out.columns:\n",
    "#         out[\"TimeScraped\"] = out[\"TimeScraped\"].astype(str)\n",
    "\n",
    "#     # Prefer Link as unique key; fallback to Title+Date if link missing\n",
    "#     # Build a helper key column\n",
    "#     def build_key(row):\n",
    "#         link = str(row.get(\"Link\") or \"\").strip()\n",
    "#         if link:\n",
    "#             return f\"URL::{link}\"\n",
    "#         title = str(row.get(\"Title\") or \"\").strip()\n",
    "#         date = str(row.get(\"Date\") or \"\").strip()\n",
    "#         return f\"TD::{title}|{date}\"\n",
    "\n",
    "#     out[\"_dedup_key\"] = out.apply(build_key, axis=1)\n",
    "#     out = out.drop_duplicates(subset=[\"_dedup_key\"], keep=\"first\").drop(columns=[\"_dedup_key\"])\n",
    "#     out.to_csv(csv_path, index=False)\n",
    "#     return len(out)\n",
    "\n",
    "# # ---------------- MAIN ----------------\n",
    "# def main():\n",
    "#     driver = make_driver(headless=False)  # set True on server/CI\n",
    "#     try:\n",
    "#         driver.get(BASE_URL)\n",
    "\n",
    "#         # Ensure the list fully loads\n",
    "#         slow_load_list(driver, UL_XP, LI_REL_XP, pause=1.0, max_passes=60)\n",
    "\n",
    "#         wait = WebDriverWait(driver, 25)\n",
    "#         ul = wait.until(EC.presence_of_element_located((By.XPATH, UL_XP)))\n",
    "#         cards = ul.find_elements(By.XPATH, LI_REL_XP)\n",
    "\n",
    "#         rows: List[Dict] = []\n",
    "\n",
    "#         for i in range(len(cards)):\n",
    "#             # re-fetch after DOM changes (robustness)\n",
    "#             ul = wait.until(EC.presence_of_element_located((By.XPATH, UL_XP)))\n",
    "#             cards = ul.find_elements(By.XPATH, LI_REL_XP)\n",
    "#             if i >= len(cards):\n",
    "#                 break\n",
    "\n",
    "#             li = cards[i]\n",
    "\n",
    "#             title = get_text_or_blank(li, By.XPATH, NAME_REL_XP)\n",
    "#             venue = get_text_or_blank(li, By.XPATH, VENUE_REL_XP)  # as per your spec (same as name)\n",
    "#             date_txt = get_text_or_blank(li, By.XPATH, DATE_REL_XP)\n",
    "#             link = get_link(li)\n",
    "\n",
    "#             if not any([title, link, date_txt]):\n",
    "#                 continue\n",
    "\n",
    "#             rows.append({\n",
    "#                 \"City\": CITY,\n",
    "#                 \"Title\": title,\n",
    "#                 \"Venue\": venue,\n",
    "#                 \"Date\": date_txt,\n",
    "#                 \"Link\": link,\n",
    "#                 \"TimeScraped\": now_stamp()\n",
    "#             })\n",
    "\n",
    "#             # tiny pause ‚Äî friendlier + reduces anti-bot triggers\n",
    "#             time.sleep(0.2)\n",
    "\n",
    "#         total = append_dedup_csv(rows, OUTPUT_CSV, key_cols=(\"Link\", \"Title\", \"Date\"))\n",
    "#         print(f\"Scraped {len(rows)} rows this run. Total unique saved: {total}\")\n",
    "#         print(f\"CSV saved: {OUTPUT_CSV}\")\n",
    "\n",
    "#     finally:\n",
    "#         driver.quit()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc08a7",
   "metadata": {},
   "source": [
    "# Manila (done)\n",
    "- as of 13/11/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d81301b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Wrote 68 total rows to all_manila.csv. Added 2 new rows this run.\n"
     ]
    }
   ],
   "source": [
    "# edit code to show how it scrapes and ensure it scrapes all not just some\n",
    "import time\n",
    "import csv\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException, NoSuchElementException, StaleElementReferenceException, ElementClickInterceptedException\n",
    ")\n",
    "\n",
    "SMTICKETS_URL = \"https://smtickets.com/\"\n",
    "OUT_CSV = \"all_manila.csv\"\n",
    "\n",
    "# Strictly the two sections you asked for:\n",
    "SECTION_1_XPATH = \"/html/body/div[30]/div/div[2]\"\n",
    "SECTION_1_NEXT_BTN_XPATH = \"/html/body/div[30]/div/div[2]/div/div[2]/button[2]\"\n",
    "SECTION_2_XPATH = \"/html/body/div[31]/div/div[2]\"\n",
    "\n",
    "# Within each section, the list often sits under a UL with id 'events_moa_slide'\n",
    "# (Your sample XPaths target li[21] under #events_moa_slide).\n",
    "# We'll try that first; if missing, we fallback to ‚Äúall li descendants that look like event cards‚Äù.\n",
    "PREFERRED_LIST_XPATH = \".//*[@id='events_moa_slide']/li\"\n",
    "\n",
    "# Relative-to-li fallback selectors\n",
    "REL_NAME = \".//a/h1\"\n",
    "REL_VENUE = \".//p\"\n",
    "REL_DATE  = \".//div\"\n",
    "REL_LINK  = \".//a\"\n",
    "\n",
    "\n",
    "def sgt_now_iso():\n",
    "    return datetime.now(tz.gettz(\"Asia/Singapore\")).isoformat(timespec=\"seconds\")\n",
    "\n",
    "\n",
    "def init_driver(headless=True):\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opts.add_argument(\"--window-size=1400,900\")\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "    driver.implicitly_wait(2)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def slow_page_scroll(driver, steps=6, pause=0.5):\n",
    "    \"\"\"Slow vertical scroll to encourage lazy-loaded assets to render.\"\"\"\n",
    "    for i in range(steps):\n",
    "        driver.execute_script(\"window.scrollBy(0, Math.floor(window.innerHeight*0.6));\")\n",
    "        time.sleep(pause)\n",
    "    # small bounce up to trigger observers\n",
    "    driver.execute_script(\"window.scrollBy(0, -200);\")\n",
    "    time.sleep(0.3)\n",
    "\n",
    "\n",
    "def slow_horizontal_scroll(driver, container, max_cycles=12, pause=0.6):\n",
    "    \"\"\"Slow horizontal scroll for carousels (if any use scrollLeft).\"\"\"\n",
    "    for _ in range(max_cycles):\n",
    "        try:\n",
    "            driver.execute_script(\"arguments[0].scrollLeft += Math.floor(arguments[0].clientWidth*0.8);\", container)\n",
    "            time.sleep(pause)\n",
    "        except Exception:\n",
    "            break\n",
    "\n",
    "\n",
    "def click_next_button_safely(driver, xpath, max_clicks=25, pause=0.9):\n",
    "    \"\"\"Click the 'next' button up to max_clicks times, stopping when it disappears or no progress is seen.\"\"\"\n",
    "    last_count = -1\n",
    "    stable_hits = 0\n",
    "    for i in range(max_clicks):\n",
    "        # If the button isn't present or clickable, we stop.\n",
    "        try:\n",
    "            btn = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.XPATH, xpath)))\n",
    "        except TimeoutException:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            btn.click()\n",
    "        except (ElementClickInterceptedException, StaleElementReferenceException):\n",
    "            # try a tiny scroll-nudge and retry once\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", btn)\n",
    "            time.sleep(0.3)\n",
    "            try:\n",
    "                btn.click()\n",
    "            except Exception:\n",
    "                break\n",
    "\n",
    "        # let new slides load\n",
    "        time.sleep(pause)\n",
    "\n",
    "        # crude progress heuristic: count <li> in the nearest list if available\n",
    "        try:\n",
    "            parent = driver.find_element(By.XPATH, xpath + \"/..\")\n",
    "            lis = parent.find_elements(By.XPATH, \".//li\")\n",
    "            curr_count = len(lis)\n",
    "            if curr_count == last_count:\n",
    "                stable_hits += 1\n",
    "            else:\n",
    "                stable_hits = 0\n",
    "                last_count = curr_count\n",
    "            if stable_hits >= 3:  # clicked but no more new items arriving\n",
    "                break\n",
    "        except Exception:\n",
    "            # if we can‚Äôt estimate progress, continue a few clicks then bail\n",
    "            continue\n",
    "\n",
    "\n",
    "def find_event_items_in_section(section_el):\n",
    "    \"\"\"Return list of <li> elements that look like event cards inside a section.\"\"\"\n",
    "    items = section_el.find_elements(By.XPATH, PREFERRED_LIST_XPATH)\n",
    "    if items:\n",
    "        return items\n",
    "    # Fallback: any li under the section that has an <a> and an <h1>\n",
    "    guess = section_el.find_elements(By.XPATH, \".//li[.//a and .//h1]\")\n",
    "    return guess\n",
    "\n",
    "\n",
    "def text_or_blank(el):\n",
    "    try:\n",
    "        return el.text.strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def href_or_blank(el):\n",
    "    try:\n",
    "        return el.get_attribute(\"href\") or \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_event_from_li(li):\n",
    "    \"\"\"Extract name, venue, date, link from one <li> card, using your provided relative structure.\"\"\"\n",
    "    # Name\n",
    "    try:\n",
    "        name_el = li.find_element(By.XPATH, REL_NAME)\n",
    "    except NoSuchElementException:\n",
    "        name_el = None\n",
    "\n",
    "    # Venue (first <p> under the li‚Äîper your XPath sample)\n",
    "    try:\n",
    "        venue_el = li.find_element(By.XPATH, REL_VENUE)\n",
    "    except NoSuchElementException:\n",
    "        venue_el = None\n",
    "\n",
    "    # Date (first <div> under the li‚Äîper your XPath sample)\n",
    "    try:\n",
    "        date_el = li.find_element(By.XPATH, REL_DATE)\n",
    "    except NoSuchElementException:\n",
    "        date_el = None\n",
    "\n",
    "    # Link (anchor wrapping the name)\n",
    "    try:\n",
    "        link_el = li.find_element(By.XPATH, REL_LINK)\n",
    "    except NoSuchElementException:\n",
    "        link_el = None\n",
    "\n",
    "    name = text_or_blank(name_el)\n",
    "    venue = text_or_blank(venue_el)\n",
    "    date_text = text_or_blank(date_el)\n",
    "    link = href_or_blank(link_el)\n",
    "\n",
    "    return name, venue, date_text, link\n",
    "\n",
    "\n",
    "def load_existing_df(csv_path: Path) -> pd.DataFrame:\n",
    "    if csv_path.exists():\n",
    "        try:\n",
    "            return pd.read_csv(csv_path)\n",
    "        except Exception:\n",
    "            # fallback: try latin-1 weirdness\n",
    "            return pd.read_csv(csv_path, encoding=\"latin-1\")\n",
    "    return pd.DataFrame(columns=[\"section\", \"name\", \"venue\", \"date_text\", \"link\", \"scraped_at_sgt\", \"event_id\"])\n",
    "\n",
    "\n",
    "def build_event_id(section, name, venue, date_text, link):\n",
    "    key = \"|\".join([section, name, venue, date_text, link]).strip().lower()\n",
    "    return key\n",
    "\n",
    "\n",
    "def scrape_section(driver, section_xpath, next_btn_xpath=None, label=\"section\"):\n",
    "    \"\"\"Scrape one section: slow scroll, (optional) click next repeatedly, horizontal scroll inside, then extract items.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Wait for the section to render\n",
    "    section = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, section_xpath)))\n",
    "\n",
    "    # Bring into view and slow scroll the page a bit\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView({block: 'start'});\", section)\n",
    "    time.sleep(0.6)\n",
    "    slow_page_scroll(driver, steps=5, pause=0.5)\n",
    "\n",
    "    # Try a gentle horizontal scroll on the section container (some carousels need this)\n",
    "    try:\n",
    "        slow_horizontal_scroll(driver, section, max_cycles=8, pause=0.5)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Click the \"next\" button multiple times to reveal more slides (if provided)\n",
    "    if next_btn_xpath:\n",
    "        click_next_button_safely(driver, next_btn_xpath, max_clicks=30, pause=0.9)\n",
    "\n",
    "    # After navigation, pause and scroll again to let images/text fully render\n",
    "    time.sleep(0.8)\n",
    "    slow_page_scroll(driver, steps=3, pause=0.4)\n",
    "\n",
    "    # Collect list items\n",
    "    lis = find_event_items_in_section(section)\n",
    "    for li in lis:\n",
    "        try:\n",
    "            name, venue, date_text, link = extract_event_from_li(li)\n",
    "        except StaleElementReferenceException:\n",
    "            # retry once\n",
    "            try:\n",
    "                name, venue, date_text, link = extract_event_from_li(li)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if not name:\n",
    "            continue\n",
    "\n",
    "        results.append({\n",
    "            \"section\": label,\n",
    "            \"name\": name,\n",
    "            \"venue\": venue,\n",
    "            \"date_text\": date_text,\n",
    "            \"link\": link,\n",
    "            \"scraped_at_sgt\": sgt_now_iso()\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    headless = True\n",
    "    if len(sys.argv) > 1 and sys.argv[1].lower() in {\"--headed\", \"--show\"}:\n",
    "        headless = False\n",
    "\n",
    "    driver = init_driver(headless=headless)\n",
    "    csv_path = Path(OUT_CSV)\n",
    "\n",
    "    try:\n",
    "        driver.get(SMTICKETS_URL)\n",
    "\n",
    "        # Initial gentle load mitigation\n",
    "        WebDriverWait(driver, 25).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        time.sleep(1.2)\n",
    "        slow_page_scroll(driver, steps=6, pause=0.5)\n",
    "\n",
    "        # Scrape Section 1 (with right/next button)\n",
    "        sec1_results = scrape_section(\n",
    "            driver,\n",
    "            section_xpath=SECTION_1_XPATH,\n",
    "            next_btn_xpath=SECTION_1_NEXT_BTN_XPATH,\n",
    "            label=\"div[30]/div/div[2]\"\n",
    "        )\n",
    "\n",
    "        # Scrape Section 2 (no explicit next button provided‚Äîjust scroll it)\n",
    "        sec2_results = scrape_section(\n",
    "            driver,\n",
    "            section_xpath=SECTION_2_XPATH,\n",
    "            next_btn_xpath=None,\n",
    "            label=\"div[31]/div/div[2]\"\n",
    "        )\n",
    "\n",
    "        all_results = sec1_results + sec2_results\n",
    "\n",
    "        # Build event IDs for dedup\n",
    "        for r in all_results:\n",
    "            r[\"event_id\"] = build_event_id(r[\"section\"], r[\"name\"], r[\"venue\"], r[\"date_text\"], r[\"link\"])\n",
    "\n",
    "        # Load existing and append only new event_ids\n",
    "        existing = load_existing_df(csv_path)\n",
    "        existing_ids = set(existing[\"event_id\"].astype(str)) if not existing.empty else set()\n",
    "\n",
    "        new_rows = [r for r in all_results if r[\"event_id\"] not in existing_ids]\n",
    "\n",
    "        if new_rows:\n",
    "            df_new = pd.DataFrame(new_rows)\n",
    "            df_out = pd.concat([existing, df_new], ignore_index=True)\n",
    "        else:\n",
    "            df_out = existing  # nothing new this run\n",
    "\n",
    "        # Persist without removing older rows\n",
    "        df_out.to_csv(csv_path, index=False, quoting=csv.QUOTE_MINIMAL)\n",
    "        print(f\"Done. Wrote {len(df_out)} total rows to {csv_path}. Added {len(new_rows)} new rows this run.\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e83a5190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 7 new events to all_manila.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "\n",
    "URL = \"https://comedymanila.helixpay.ph/\"\n",
    "CSV_FILE = \"all_manila.csv\"\n",
    "\n",
    "\n",
    "def build_driver(headless: bool = False):\n",
    "    \"\"\"\n",
    "    Build a Chrome WebDriver.\n",
    "    headless=False so the browser window pops up and you can see it.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless=new\")\n",
    "\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--window-size=1400,1000\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def wait_for_page_and_lazy_load(driver):\n",
    "    \"\"\"\n",
    "    Wait for the events section to appear, then scroll down to trigger\n",
    "    lazy loading of all events. Does not close any pop-ups.\n",
    "    \"\"\"\n",
    "    wait = WebDriverWait(driver, 25)\n",
    "\n",
    "    try:\n",
    "        # Wait for the groupSection container that holds the events\n",
    "        wait.until(\n",
    "            EC.presence_of_element_located(\n",
    "                (By.ID, \"groupSection\")\n",
    "            )\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        print(\"Warning: #groupSection did not load in time.\")\n",
    "        return\n",
    "\n",
    "    # Lazy-load scroll loop ‚Äì scroll down multiple times\n",
    "    last_height = 0\n",
    "    for _ in range(6):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1.5)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "\n",
    "def load_existing_event_names(csv_path: str):\n",
    "    \"\"\"\n",
    "    Load existing event_name values from CSV to avoid duplicates on re-runs.\n",
    "    \"\"\"\n",
    "    existing_names = set()\n",
    "    if not os.path.exists(csv_path):\n",
    "        return existing_names\n",
    "\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            name = (row.get(\"event_name\") or \"\").strip()\n",
    "            if name:\n",
    "                existing_names.add(name)\n",
    "    return existing_names\n",
    "\n",
    "\n",
    "def save_events_to_csv(csv_path: str, events: list[dict]):\n",
    "    \"\"\"\n",
    "    Append new events to the CSV. Do not remove or overwrite existing rows.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.exists(csv_path)\n",
    "    fieldnames = [\"event_name\", \"event_price\", \"time_scraped_utc\"]\n",
    "\n",
    "    with open(csv_path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        for ev in events:\n",
    "            writer.writerow(ev)\n",
    "\n",
    "\n",
    "def scrape_events(driver):\n",
    "    driver.get(URL)\n",
    "    wait_for_page_and_lazy_load(driver)\n",
    "\n",
    "    # Find ALL event name elements under groupSection.\n",
    "    # Your sample XPaths:\n",
    "    #   event name:\n",
    "    #     //*[@id=\"groupSection\"]/div[1]/div[1]/a/section[2]/div[1]/div\n",
    "    #   event price:\n",
    "    #     //*[@id=\"groupSection\"]/div[1]/div[1]/a/section[2]/div[2]/div/span\n",
    "    #\n",
    "    # Generalized: under #groupSection/div[1] find all a/section[2]/div[1]/div (names)\n",
    "    name_elements = driver.find_elements(\n",
    "        By.XPATH,\n",
    "        '//*[@id=\"groupSection\"]/div[1]//a/section[2]/div[1]/div'\n",
    "    )\n",
    "\n",
    "    existing_names = load_existing_event_names(CSV_FILE)\n",
    "    seen_this_run = set()\n",
    "    new_events = []\n",
    "    current_utc = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    for name_elem in name_elements:\n",
    "        try:\n",
    "            event_name = name_elem.text.strip()\n",
    "        except Exception:\n",
    "            event_name = \"\"\n",
    "\n",
    "        if not event_name:\n",
    "            continue\n",
    "\n",
    "        # Skip if event_name already exists in CSV or in this run\n",
    "        if event_name in existing_names or event_name in seen_this_run:\n",
    "            continue\n",
    "\n",
    "        # Move from name element to its container to get the price.\n",
    "        # name is at: a/section[2]/div[1]/div\n",
    "        # section[2] is the parent of div[1], so price is at:\n",
    "        #   section[2]/div[2]/div/span  (relative to section[2])\n",
    "        try:\n",
    "            section_two = name_elem.find_element(By.XPATH, './../..')  # section[2]\n",
    "        except Exception:\n",
    "            section_two = None\n",
    "\n",
    "        event_price = \"\"\n",
    "        if section_two is not None:\n",
    "            try:\n",
    "                price_elem = section_two.find_element(By.XPATH, './div[2]/div/span')\n",
    "                event_price = price_elem.text.strip()\n",
    "            except Exception:\n",
    "                event_price = \"\"\n",
    "\n",
    "        seen_this_run.add(event_name)\n",
    "\n",
    "        new_events.append(\n",
    "            {\n",
    "                \"event_name\": event_name,\n",
    "                \"event_price\": event_price,\n",
    "                \"time_scraped_utc\": current_utc,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return new_events\n",
    "\n",
    "\n",
    "def main():\n",
    "    # headless=False ‚Üí website pops up in a visible window\n",
    "    driver = build_driver(headless=False)\n",
    "    try:\n",
    "        new_events = scrape_events(driver)\n",
    "        if new_events:\n",
    "            save_events_to_csv(CSV_FILE, new_events)\n",
    "            print(f\"Added {len(new_events)} new events to {CSV_FILE}.\")\n",
    "        else:\n",
    "            print(\"No new events found. CSV unchanged.\")\n",
    "\n",
    "        # Optional: keep the browser open a bit so you can see it\n",
    "        time.sleep(5)\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2368e0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1 new events to all_manila.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "\n",
    "URL = \"https://smtickets.com/events/listing/arena\"\n",
    "CSV_FILE = \"all_manila.csv\"\n",
    "\n",
    "\n",
    "def build_driver(headless: bool = False):\n",
    "    \"\"\"\n",
    "    Build a Chrome WebDriver.\n",
    "    headless=False so the browser window pops up and you can see it.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless=new\")\n",
    "\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--window-size=1400,1000\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def wait_for_page_and_lazy_load(driver):\n",
    "    \"\"\"\n",
    "    Wait for the first example event block to appear, then scroll down\n",
    "    repeatedly to lazy-load more events.\n",
    "    Uses your sample XPath under id=\"December\".\n",
    "    \"\"\"\n",
    "    wait = WebDriverWait(driver, 30)\n",
    "\n",
    "    try:\n",
    "        # Wait for at least one event block to appear\n",
    "        wait.until(\n",
    "            EC.presence_of_element_located(\n",
    "                (By.XPATH, '//*[@id=\"December\"]/div[1]')\n",
    "            )\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        print(\"Warning: December block did not load in time.\")\n",
    "        return\n",
    "\n",
    "    # Scroll several times to trigger lazy loading of additional events\n",
    "    last_height = 0\n",
    "    for _ in range(10):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1.5)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "\n",
    "def load_existing_name_and_url_sets(csv_path):\n",
    "    \"\"\"\n",
    "    Load existing event_name and event_url sets from CSV so we can\n",
    "    avoid duplicates if EITHER name OR URL matches.\n",
    "    \"\"\"\n",
    "    existing_names = set()\n",
    "    existing_urls = set()\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        return existing_names, existing_urls\n",
    "\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            name = (row.get(\"event_name\") or \"\").strip()\n",
    "            url = (row.get(\"event_url\") or \"\").strip()\n",
    "            if name:\n",
    "                existing_names.add(name)\n",
    "            if url:\n",
    "                existing_urls.add(url)\n",
    "\n",
    "    return existing_names, existing_urls\n",
    "\n",
    "\n",
    "def save_events_to_csv(csv_path, events):\n",
    "    \"\"\"\n",
    "    Append new events to the CSV, preserving existing rows.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.exists(csv_path)\n",
    "    fieldnames = [\"event_name\", \"event_date\", \"event_venue\",\n",
    "                  \"event_url\", \"time_scraped_utc\"]\n",
    "\n",
    "    with open(csv_path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        for ev in events:\n",
    "            writer.writerow(ev)\n",
    "\n",
    "\n",
    "def scrape_events(driver):\n",
    "    driver.get(URL)\n",
    "    wait_for_page_and_lazy_load(driver)\n",
    "\n",
    "    # Your example XPaths:\n",
    "    #   event name:  //*[@id=\"December\"]/div[1]\n",
    "    #   event date:  //*[@id=\"December\"]/div[2]/span\n",
    "    #   event venue: //*[@id=\"December\"]/div[3]\n",
    "    #\n",
    "    # Treat each element with id=\"December\" as an event block.\n",
    "    event_blocks = driver.find_elements(By.ID, \"December\")\n",
    "\n",
    "    existing_names, existing_urls = load_existing_name_and_url_sets(CSV_FILE)\n",
    "\n",
    "    # For this run, also keep track of names & urls to avoid duplicates\n",
    "    seen_names = set()\n",
    "    seen_urls = set()\n",
    "\n",
    "    new_events = []\n",
    "    current_utc = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    for block in event_blocks:\n",
    "        # ---- Event Name ----\n",
    "        try:\n",
    "            name_elem = block.find_element(By.XPATH, \"./div[1]\")\n",
    "            event_name = name_elem.text.strip()\n",
    "        except Exception:\n",
    "            event_name = \"\"\n",
    "\n",
    "        if not event_name:\n",
    "            continue\n",
    "\n",
    "        # ---- Event Date ----\n",
    "        try:\n",
    "            date_elem = block.find_element(By.XPATH, \"./div[2]/span\")\n",
    "            event_date = date_elem.text.strip()\n",
    "        except Exception:\n",
    "            event_date = \"\"\n",
    "\n",
    "        # ---- Event Venue ----\n",
    "        try:\n",
    "            venue_elem = block.find_element(By.XPATH, \"./div[3]\")\n",
    "            event_venue = venue_elem.text.strip()\n",
    "        except Exception:\n",
    "            event_venue = \"\"\n",
    "\n",
    "        # ---- Event URL (from link, if present) ----\n",
    "        event_url = \"\"\n",
    "        try:\n",
    "            link_elem = block.find_element(By.XPATH, \".//a\")\n",
    "            href = link_elem.get_attribute(\"href\")\n",
    "            if href:\n",
    "                event_url = href.strip()\n",
    "        except Exception:\n",
    "            event_url = \"\"\n",
    "\n",
    "        # ---------- DEDUP LOGIC (NAME OR URL) ----------\n",
    "        # Skip this event if:\n",
    "        #   - its name already exists in CSV OR\n",
    "        #   - its URL (non-empty) already exists in CSV OR\n",
    "        #   - its name already seen in this run OR\n",
    "        #   - its URL (non-empty) already seen in this run\n",
    "        name_exists = event_name in existing_names or event_name in seen_names\n",
    "        url_exists = bool(event_url) and (event_url in existing_urls or event_url in seen_urls)\n",
    "\n",
    "        if name_exists or url_exists:\n",
    "            continue\n",
    "\n",
    "        # Mark as seen for this run\n",
    "        seen_names.add(event_name)\n",
    "        if event_url:\n",
    "            seen_urls.add(event_url)\n",
    "\n",
    "        new_events.append(\n",
    "            {\n",
    "                \"event_name\": event_name,\n",
    "                \"event_date\": event_date,\n",
    "                \"event_venue\": event_venue,\n",
    "                \"event_url\": event_url,\n",
    "                \"time_scraped_utc\": current_utc,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return new_events\n",
    "\n",
    "\n",
    "def main():\n",
    "    # headless=False ‚Üí website pops up in a visible window\n",
    "    driver = build_driver(headless=False)\n",
    "    try:\n",
    "        new_events = scrape_events(driver)\n",
    "        if new_events:\n",
    "            save_events_to_csv(CSV_FILE, new_events)\n",
    "            print(f\"Added {len(new_events)} new events to {CSV_FILE}.\")\n",
    "        else:\n",
    "            print(\"No new events found. CSV unchanged.\")\n",
    "\n",
    "        # Optional: keep browser open briefly so you can see the page\n",
    "        time.sleep(5)\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa29ee84",
   "metadata": {},
   "source": [
    "# Taipei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71dea184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 45 rows this run. Saved/merged into all_taipei.csv.\n"
     ]
    }
   ],
   "source": [
    "# tixcraft_scrape.py\n",
    "# pip install selenium webdriver-manager\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "URL = \"https://tixcraft.com/activity\"\n",
    "OUTPUT_CSV = \"all_taipei.csv\"\n",
    "\n",
    "# XPaths you provided (generalized to select ALL items, not just the first one)\n",
    "# name: //*[@id=\"all\"]/div[2]/div[1]/div/a/div[2]/div[2]\n",
    "# date: //*[@id=\"all\"]/div[2]/div[1]/div/a/div[2]/div[1]\n",
    "# We target all cards by finding all occurrences of the \"name\" div, then walk to siblings/ancestors.\n",
    "NAME_XPATH_ALL = '//*[@id=\"all\"]/div[2]//a/div[2]/div[2]'\n",
    "\n",
    "def start_driver(headless=True):\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--window-size=1440,900\")\n",
    "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opts.add_argument(\"--user-agent=Mozilla/5.0\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=opts)\n",
    "    driver.set_page_load_timeout(60)\n",
    "    driver.implicitly_wait(2)\n",
    "    return driver\n",
    "\n",
    "def slow_scroll_to_bottom(driver, min_pause=1.2, max_idle_rounds=3):\n",
    "    \"\"\"\n",
    "    Smoothly scrolls until no new items load for max_idle_rounds rounds.\n",
    "    \"\"\"\n",
    "    last_count = 0\n",
    "    idle_rounds = 0\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollBy(0, document.body.scrollHeight * 0.6);\")\n",
    "        time.sleep(min_pause)\n",
    "        # Try a \"nudge\" to trigger lazy loading\n",
    "        driver.execute_script(\"window.scrollBy(0, 300);\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        # Count currently visible name nodes\n",
    "        try:\n",
    "            names_now = driver.find_elements(By.XPATH, NAME_XPATH_ALL)\n",
    "            count_now = len(names_now)\n",
    "        except Exception:\n",
    "            count_now = 0\n",
    "\n",
    "        if count_now > last_count:\n",
    "            last_count = count_now\n",
    "            idle_rounds = 0\n",
    "        else:\n",
    "            idle_rounds += 1\n",
    "            if idle_rounds >= max_idle_rounds:\n",
    "                break\n",
    "\n",
    "    # Ensure at true bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "\n",
    "def load_existing(filepath):\n",
    "    rows = []\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for r in reader:\n",
    "                rows.append(r)\n",
    "    return rows\n",
    "\n",
    "def save_merged(filepath, rows):\n",
    "    fieldnames = [\"event_name\", \"event_date\", \"event_link\", \"time_scraped\"]\n",
    "    exists = os.path.exists(filepath)\n",
    "    if not exists:\n",
    "        with open(filepath, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "    else:\n",
    "        # Merge with existing, dedupe by (event_name, event_date, event_link)\n",
    "        existing = load_existing(filepath)\n",
    "        seen = set(\n",
    "            (e[\"event_name\"], e[\"event_date\"], e.get(\"event_link\", \"\"))\n",
    "            for e in existing\n",
    "        )\n",
    "        new_unique = []\n",
    "        for r in rows:\n",
    "            key = (r[\"event_name\"], r[\"event_date\"], r.get(\"event_link\", \"\"))\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                new_unique.append(r)\n",
    "\n",
    "        if not new_unique:\n",
    "            return  # nothing to add\n",
    "\n",
    "        with open(filepath, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writerows(new_unique)\n",
    "\n",
    "def scrape():\n",
    "    driver = start_driver(headless=True)\n",
    "    try:\n",
    "        driver.get(URL)\n",
    "\n",
    "        # Wait until the main container is present\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.ID, \"all\"))\n",
    "        )\n",
    "\n",
    "        # Scroll (handles lazy loading)\n",
    "        slow_scroll_to_bottom(driver)\n",
    "\n",
    "        # Collect all name nodes\n",
    "        name_nodes = driver.find_elements(By.XPATH, NAME_XPATH_ALL)\n",
    "        ts = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "        rows = []\n",
    "        for name_el in name_nodes:\n",
    "            try:\n",
    "                event_name = name_el.text.strip()\n",
    "            except Exception:\n",
    "                event_name = \"\"\n",
    "\n",
    "            # date is sibling: ../div[1]\n",
    "            try:\n",
    "                date_el = name_el.find_element(By.XPATH, \"../div[1]\")\n",
    "                event_date = date_el.text.strip()\n",
    "            except Exception:\n",
    "                event_date = \"\"\n",
    "\n",
    "            # link is the ancestor <a>: name is a/div[2]/div[2] -> go up 3 levels to reach <a>\n",
    "            try:\n",
    "                anchor = name_el.find_element(By.XPATH, \"../../..\")\n",
    "                event_link = anchor.get_attribute(\"href\") or \"\"\n",
    "            except Exception:\n",
    "                event_link = \"\"\n",
    "\n",
    "            # Only add if at least a name or date is present\n",
    "            if event_name or event_date:\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"event_name\": event_name,\n",
    "                        \"event_date\": event_date,\n",
    "                        \"event_link\": event_link,\n",
    "                        \"time_scraped\": ts,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Persist without removing older rows; dedupe across runs\n",
    "        save_merged(OUTPUT_CSV, rows)\n",
    "\n",
    "        print(f\"Scraped {len(rows)} rows this run. Saved/merged into {OUTPUT_CSV}.\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3530a9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new events found. CSV unchanged.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "\n",
    "URL = \"https://trendy.taipei/en/Event?type=0\"\n",
    "CSV_FILE = \"all_taipei.csv\"\n",
    "\n",
    "\n",
    "def build_driver(headless: bool = False):\n",
    "    \"\"\"\n",
    "    Build a Chrome WebDriver.\n",
    "    headless=False so the browser window pops up (visible).\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless=new\")\n",
    "\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--window-size=1400,1000\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def wait_for_page_and_lazy_load(driver):\n",
    "    \"\"\"\n",
    "    Wait for the first event card to appear, then scroll down slowly\n",
    "    multiple times to lazy-load more events.\n",
    "    \"\"\"\n",
    "    wait = WebDriverWait(driver, 30)\n",
    "\n",
    "    try:\n",
    "        # Wait until the first event name element is present\n",
    "        # Example provided:\n",
    "        # //*[@id=\"content\"]/section/div/div/div/a[1]/div[2]/div[2]\n",
    "        wait.until(\n",
    "            EC.presence_of_element_located(\n",
    "                (\n",
    "                    By.XPATH,\n",
    "                    '//*[@id=\"content\"]/section/div/div/div/a[1]/div[2]/div[2]'\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        print(\"Warning: First event name element did not load in time.\")\n",
    "        return\n",
    "\n",
    "    # Scroll DOWN slowly to trigger lazy loading for more events\n",
    "    last_height = 0\n",
    "    for _ in range(12):  # more iterations = deeper scrolling\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2.0)  # slow scrolling as requested\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "\n",
    "def load_existing_name_and_url_sets(csv_path: str):\n",
    "    \"\"\"\n",
    "    Load existing event_name and event_url sets from CSV.\n",
    "    Used for dedupe: if name OR url already exists, skip.\n",
    "    \"\"\"\n",
    "    existing_names = set()\n",
    "    existing_urls = set()\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        return existing_names, existing_urls\n",
    "\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            name = (row.get(\"event_name\") or \"\").strip()\n",
    "            url = (row.get(\"event_url\") or \"\").strip()\n",
    "            if name:\n",
    "                existing_names.add(name)\n",
    "            if url:\n",
    "                existing_urls.add(url)\n",
    "\n",
    "    return existing_names, existing_urls\n",
    "\n",
    "\n",
    "def save_events_to_csv(csv_path: str, events: list[dict]):\n",
    "    \"\"\"\n",
    "    Append new events to the CSV.\n",
    "    Never removes or overwrites existing rows.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.exists(csv_path)\n",
    "    fieldnames = [\n",
    "        \"event_name\",\n",
    "        \"event_date\",\n",
    "        \"event_location\",\n",
    "        \"event_url\",\n",
    "        \"time_scraped_utc\",\n",
    "    ]\n",
    "\n",
    "    with open(csv_path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        for ev in events:\n",
    "            writer.writerow(ev)\n",
    "\n",
    "\n",
    "def scrape_events(driver):\n",
    "    driver.get(URL)\n",
    "    wait_for_page_and_lazy_load(driver)\n",
    "\n",
    "    # Example XPaths you gave (for the FIRST card):\n",
    "    # event name:    //*[@id=\"content\"]/section/div/div/div/a[1]/div[2]/div[2]\n",
    "    # event date:    //*[@id=\"content\"]/section/div/div/div/a[1]/div[2]/div[1]\n",
    "    # event location://*[@id=\"content\"]/section/div/div/div/a[1]/div[2]/div[3]/span\n",
    "    #\n",
    "    # Generalized: under #content/section/div/div/div, find all <a> cards:\n",
    "    cards = driver.find_elements(\n",
    "        By.XPATH,\n",
    "        '//*[@id=\"content\"]/section/div/div/div/a'\n",
    "    )\n",
    "\n",
    "    existing_names, existing_urls = load_existing_name_and_url_sets(CSV_FILE)\n",
    "\n",
    "    # Track seen within this run\n",
    "    seen_names = set()\n",
    "    seen_urls = set()\n",
    "\n",
    "    new_events = []\n",
    "    current_utc = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    for card in cards:\n",
    "        # Event URL (href on <a>)\n",
    "        event_url = (card.get_attribute(\"href\") or \"\").strip()\n",
    "\n",
    "        # Inside each <a>, layout is:\n",
    "        # <a>\n",
    "        #   ...\n",
    "        #   <div[2]>  # info panel\n",
    "        #       <div[1]> date\n",
    "        #       <div[2]> name\n",
    "        #       <div[3]><span> location\n",
    "        #   ...\n",
    "        # </a>\n",
    "        try:\n",
    "            info_div = card.find_element(By.XPATH, \"./div[2]\")\n",
    "        except Exception:\n",
    "            info_div = None\n",
    "\n",
    "        if info_div is None:\n",
    "            continue\n",
    "\n",
    "        # Name\n",
    "        try:\n",
    "            name_elem = info_div.find_element(By.XPATH, \"./div[2]\")\n",
    "            event_name = name_elem.text.strip()\n",
    "        except Exception:\n",
    "            event_name = \"\"\n",
    "\n",
    "        if not event_name:\n",
    "            continue\n",
    "\n",
    "        # Date\n",
    "        try:\n",
    "            date_elem = info_div.find_element(By.XPATH, \"./div[1]\")\n",
    "            event_date = date_elem.text.strip()\n",
    "        except Exception:\n",
    "            event_date = \"\"\n",
    "\n",
    "        # Location\n",
    "        try:\n",
    "            loc_elem = info_div.find_element(By.XPATH, \"./div[3]/span\")\n",
    "            event_location = loc_elem.text.strip()\n",
    "        except Exception:\n",
    "            event_location = \"\"\n",
    "\n",
    "        # ---------- DEDUPE: name OR url ----------\n",
    "        name_exists = (\n",
    "            event_name in existing_names or\n",
    "            event_name in seen_names\n",
    "        )\n",
    "        url_exists = (\n",
    "            bool(event_url) and\n",
    "            (event_url in existing_urls or event_url in seen_urls)\n",
    "        )\n",
    "\n",
    "        if name_exists or url_exists:\n",
    "            # either name or url already present ‚Üí skip this event\n",
    "            continue\n",
    "\n",
    "        # Mark as seen for this run\n",
    "        seen_names.add(event_name)\n",
    "        if event_url:\n",
    "            seen_urls.add(event_url)\n",
    "\n",
    "        new_events.append(\n",
    "            {\n",
    "                \"event_name\": event_name,\n",
    "                \"event_date\": event_date,\n",
    "                \"event_location\": event_location,\n",
    "                \"event_url\": event_url,\n",
    "                \"time_scraped_utc\": current_utc,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return new_events\n",
    "\n",
    "\n",
    "def main():\n",
    "    # headless=False ‚Üí show popup browser window while scraping\n",
    "    driver = build_driver(headless=False)\n",
    "    try:\n",
    "        new_events = scrape_events(driver)\n",
    "        if new_events:\n",
    "            save_events_to_csv(CSV_FILE, new_events)\n",
    "            print(f\"Added {len(new_events)} new events to {CSV_FILE}.\")\n",
    "        else:\n",
    "            print(\"No new events found. CSV unchanged.\")\n",
    "\n",
    "        # Optional: let you see the page for a bit before closing\n",
    "        time.sleep(5)\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7cad8d",
   "metadata": {},
   "source": [
    "# Phuket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c902f32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 23 events from Cafe Del Mar Phuket.\n",
      "Existing rows: 119, New rows: 23, Combined after dedupe: 130\n",
      "Saved 130 rows to all_phucket.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "URL = \"https://phuket.cafedelmar.com/events\"\n",
    "CSV_PATH = \"all_phucket.csv\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# UTILITIES\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def get_utc_timestamp():\n",
    "    \"\"\"Return current UTC time as a string.\"\"\"\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S%z\")\n",
    "\n",
    "\n",
    "def setup_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    # options.add_argument(\"--headless=new\")  # uncomment if you want headless\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def wait_for_page_loaded(driver, timeout=30):\n",
    "    \"\"\"\n",
    "    Wait until the main event section is present.\n",
    "    Reference: //*[@id=\"grid-only\"]/section\n",
    "    \"\"\"\n",
    "    WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//*[@id='grid-only']/section\"))\n",
    "    )\n",
    "\n",
    "\n",
    "def lazy_scroll(driver, pause=2.0, max_loops=20):\n",
    "    \"\"\"\n",
    "    Scroll to the bottom repeatedly to trigger lazy-loading until\n",
    "    the page height stops changing or max_loops is reached.\n",
    "    \"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    for _ in range(max_loops):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(pause)\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# SCRAPER\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def scrape_events(driver):\n",
    "    driver.get(URL)\n",
    "    wait_for_page_loaded(driver)\n",
    "    lazy_scroll(driver)\n",
    "\n",
    "    # Main event section\n",
    "    section_el = driver.find_element(By.XPATH, \"//*[@id='grid-only']/section\")\n",
    "\n",
    "    # Get ALL cards under section (no [1] indices)\n",
    "    cards = section_el.find_elements(By.XPATH, \"./div/div/div/div\")\n",
    "\n",
    "    scraped_at = get_utc_timestamp()\n",
    "    rows = []\n",
    "\n",
    "    for card in cards:\n",
    "        # Event name:\n",
    "        try:\n",
    "            name_el = card.find_element(By.XPATH, \".//div[2]/div[2]\")\n",
    "            event_name = name_el.text.strip()\n",
    "        except Exception:\n",
    "            event_name = \"\"\n",
    "\n",
    "        # Event date:\n",
    "        try:\n",
    "            date_el = card.find_element(By.XPATH, \".//div[2]/div[1]/div[2]\")\n",
    "            event_date = date_el.text.strip()\n",
    "        except Exception:\n",
    "            event_date = \"\"\n",
    "\n",
    "        # Event link (if any anchor in the card)\n",
    "        try:\n",
    "            link_el = card.find_element(By.XPATH, \".//a\")\n",
    "            event_link = link_el.get_attribute(\"href\") or \"\"\n",
    "        except Exception:\n",
    "            event_link = \"\"\n",
    "\n",
    "        if not event_name:\n",
    "            continue\n",
    "\n",
    "        row = {\n",
    "            \"event_name\": event_name,\n",
    "            \"event_date\": event_date,\n",
    "            \"event_location\": \"Cafe Del Mar Phuket\",\n",
    "            \"event_section\": \"Cafe Del Mar Phuket Events\",\n",
    "            \"event_link\": event_link,\n",
    "            \"scraped_at_utc\": scraped_at,\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# DEDUPE HELPERS\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def _norm_col(df: pd.DataFrame, col: str) -> pd.Series:\n",
    "    \"\"\"Return a normalised text column (strip, collapse spaces, lowercase).\"\"\"\n",
    "    if col not in df.columns:\n",
    "        return pd.Series([\"\"] * len(df), index=df.index)\n",
    "    s = df[col].fillna(\"\").astype(str)\n",
    "    s = s.str.strip()\n",
    "    s = s.str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    s = s.str.lower()\n",
    "    return s\n",
    "\n",
    "\n",
    "def build_uid(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Build a stable unique ID for each event for deduplication.\n",
    "    Uses normalised event_name + date + location + link.\n",
    "    \"\"\"\n",
    "    name = _norm_col(df, \"event_name\")\n",
    "    date = _norm_col(df, \"event_date\")\n",
    "    loc  = _norm_col(df, \"event_location\")\n",
    "    link = _norm_col(df, \"event_link\")\n",
    "\n",
    "    return name + \" | \" + date + \" | \" + loc + \" | \" + link\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# MERGING / SAVING (NO ROW REMOVAL, NO DUPES)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def save_merged_csv(new_rows, path=CSV_PATH):\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    if new_df.empty:\n",
    "        print(\"No events scraped.\")\n",
    "        return\n",
    "\n",
    "    # Build UID for new data\n",
    "    new_df[\"uid\"] = build_uid(new_df)\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        existing_df = pd.read_csv(path, dtype=str)\n",
    "\n",
    "        # Rebuild uid for existing data (normalised, same logic)\n",
    "        existing_df[\"uid\"] = build_uid(existing_df)\n",
    "\n",
    "        before_existing = len(existing_df)\n",
    "        before_new = len(new_df)\n",
    "\n",
    "        combined = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "\n",
    "        # Drop duplicates by uid\n",
    "        combined = combined.drop_duplicates(subset=[\"uid\"], keep=\"first\")\n",
    "\n",
    "        print(\n",
    "            f\"Existing rows: {before_existing}, \"\n",
    "            f\"New rows: {before_new}, \"\n",
    "            f\"Combined after dedupe: {len(combined)}\"\n",
    "        )\n",
    "    else:\n",
    "        combined = new_df\n",
    "        print(f\"Creating new file with {len(combined)} rows\")\n",
    "\n",
    "    combined.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Saved {len(combined)} rows to {path}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# -MAIN\n",
    "# --------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    driver = setup_driver()\n",
    "    try:\n",
    "        events = scrape_events(driver)\n",
    "        print(f\"Scraped {len(events)} events from Cafe Del Mar Phuket.\")\n",
    "        save_merged_csv(events)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa376806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 15 new events to all_phucket.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "\n",
    "URL = \"https://lotusarenatickets.com/\"\n",
    "CSV_FILE = \"all_phucket.csv\"\n",
    "\n",
    "\n",
    "def build_driver(headless: bool = False):\n",
    "    \"\"\"\n",
    "    Build a Chrome WebDriver.\n",
    "    headless=False so the website pops up in a visible window.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless=new\")\n",
    "\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--window-size=1400,1000\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def wait_for_page_and_lazy_load(driver):\n",
    "    \"\"\"\n",
    "    Wait for the main content section to appear, then give extra time\n",
    "    for JS rendering, and scroll down slowly several times to load\n",
    "    all events.\n",
    "    \"\"\"\n",
    "    wait = WebDriverWait(driver, 40)\n",
    "\n",
    "    try:\n",
    "        # Wait until the main section under root/div[1]/div is present\n",
    "        wait.until(\n",
    "            EC.presence_of_element_located(\n",
    "                (By.XPATH, '//*[@id=\"root\"]/div[1]/div')\n",
    "            )\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        print(\"Warning: main content section did not load in time.\")\n",
    "        return\n",
    "\n",
    "    # Extra pause so React/JS has time to populate the list\n",
    "    time.sleep(4)\n",
    "\n",
    "    # Scroll down slowly several times to lazy-load everything\n",
    "    last_height = 0\n",
    "    for _ in range(15):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2.5)  # slow scrolling\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Final small wait before scraping\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "def load_existing_name_and_url_sets(csv_path: str):\n",
    "    \"\"\"\n",
    "    Load existing event_name and event_url sets from CSV.\n",
    "    Used for dedupe: skip if name OR url already exists.\n",
    "    \"\"\"\n",
    "    existing_names = set()\n",
    "    existing_urls = set()\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        return existing_names, existing_urls\n",
    "\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            name = (row.get(\"event_name\") or \"\").strip()\n",
    "            url = (row.get(\"event_url\") or \"\").strip()\n",
    "            if name:\n",
    "                existing_names.add(name)\n",
    "            if url:\n",
    "                existing_urls.add(url)\n",
    "\n",
    "    return existing_names, existing_urls\n",
    "\n",
    "\n",
    "def save_events_to_csv(csv_path: str, events: list[dict]):\n",
    "    \"\"\"\n",
    "    Append new events to the CSV.\n",
    "    Never removes or overwrites existing rows.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.exists(csv_path)\n",
    "    fieldnames = [\n",
    "        \"event_name\",\n",
    "        \"event_description\",\n",
    "        \"event_date\",\n",
    "        \"event_url\",\n",
    "        \"time_scraped_utc\",\n",
    "    ]\n",
    "\n",
    "    with open(csv_path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        for ev in events:\n",
    "            writer.writerow(ev)\n",
    "\n",
    "\n",
    "def scrape_events(driver):\n",
    "    driver.get(URL)\n",
    "    wait_for_page_and_lazy_load(driver)\n",
    "\n",
    "    # MAIN SECTION containing all events\n",
    "    try:\n",
    "        container = driver.find_element(By.XPATH, '//*[@id=\"root\"]/div[1]/div')\n",
    "    except Exception:\n",
    "        print(\"Could not find main event container.\")\n",
    "        return []\n",
    "\n",
    "    # We now look THROUGH this section and treat any div that has BOTH\n",
    "    # a <span> and a <p> (name + description) as an event card.\n",
    "    # This is more general and should pick up all event cards inside.\n",
    "    event_cards = container.find_elements(\n",
    "        By.XPATH,\n",
    "        './/div[span and p]'\n",
    "    )\n",
    "\n",
    "    existing_names, existing_urls = load_existing_name_and_url_sets(CSV_FILE)\n",
    "\n",
    "    # Track what we see in this run as well\n",
    "    seen_names = set()\n",
    "    seen_urls = set()\n",
    "\n",
    "    new_events = []\n",
    "    current_utc = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    for card in event_cards:\n",
    "        # ---- Event Name ----\n",
    "        try:\n",
    "            name_elem = card.find_element(By.XPATH, \"./span\")\n",
    "            event_name = name_elem.text.strip()\n",
    "        except Exception:\n",
    "            event_name = \"\"\n",
    "\n",
    "        if not event_name:\n",
    "            continue\n",
    "\n",
    "        # ---- Event Description ----\n",
    "        try:\n",
    "            desc_elem = card.find_element(By.XPATH, \"./p\")\n",
    "            event_description = desc_elem.text.strip()\n",
    "        except Exception:\n",
    "            event_description = \"\"\n",
    "\n",
    "        # ---- Event Date ----\n",
    "        # Often date is in a nested div with a span; we use a flexible search:\n",
    "        event_date = \"\"\n",
    "        try:\n",
    "            # First try the direct pattern ./div[1]/span\n",
    "            date_elem = card.find_element(By.XPATH, \"./div[1]/span\")\n",
    "            event_date = date_elem.text.strip()\n",
    "        except Exception:\n",
    "            # Fallback: any descendant div/span that looks like a date\n",
    "            try:\n",
    "                date_elem = card.find_element(By.XPATH, \".//div/span\")\n",
    "                event_date = date_elem.text.strip()\n",
    "            except Exception:\n",
    "                event_date = \"\"\n",
    "\n",
    "        # ---- Event URL ----\n",
    "        event_url = \"\"\n",
    "        try:\n",
    "            link_elem = card.find_element(By.XPATH, \".//a\")\n",
    "            href = link_elem.get_attribute(\"href\")\n",
    "            if href:\n",
    "                event_url = href.strip()\n",
    "        except Exception:\n",
    "            event_url = \"\"\n",
    "\n",
    "        # ---------- DEDUPE: name OR url ----------\n",
    "        name_exists = (\n",
    "            event_name in existing_names or\n",
    "            event_name in seen_names\n",
    "        )\n",
    "        url_exists = (\n",
    "            bool(event_url) and\n",
    "            (event_url in existing_urls or event_url in seen_urls)\n",
    "        )\n",
    "\n",
    "        if name_exists or url_exists:\n",
    "            # Skip if either event name or event url is already present\n",
    "            continue\n",
    "\n",
    "        seen_names.add(event_name)\n",
    "        if event_url:\n",
    "            seen_urls.add(event_url)\n",
    "\n",
    "        new_events.append(\n",
    "            {\n",
    "                \"event_name\": event_name,\n",
    "                \"event_description\": event_description,\n",
    "                \"event_date\": event_date,\n",
    "                \"event_url\": event_url,\n",
    "                \"time_scraped_utc\": current_utc,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return new_events\n",
    "\n",
    "\n",
    "def main():\n",
    "    # headless=False ‚Üí show popup browser window while scraping\n",
    "    driver = build_driver(headless=False)\n",
    "    try:\n",
    "        new_events = scrape_events(driver)\n",
    "        if new_events:\n",
    "            save_events_to_csv(CSV_FILE, new_events)\n",
    "            print(f\"Added {len(new_events)} new events to {CSV_FILE}.\")\n",
    "        else:\n",
    "            print(\"No new events found. CSV unchanged.\")\n",
    "\n",
    "        # Optional: keep browser open briefly so you can see the page\n",
    "        time.sleep(5)\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11786388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://www.impact.co.th/en/visitors/event-calendar?search=&category_id=1&location_id=0&month_select=11&year_select=2025&filter_duration=month_11_year_2025\n",
      "Scraping: https://www.impact.co.th/en/visitors/event-calendar?category_id=1&location_id=0&filter_duration=month_12_year_2025\n",
      "Added 10 new events to all_phucket.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "\n",
    "URLS = [\n",
    "    \"https://www.impact.co.th/en/visitors/event-calendar?search=&category_id=1&location_id=0&month_select=11&year_select=2025&filter_duration=month_11_year_2025\",\n",
    "    \"https://www.impact.co.th/en/visitors/event-calendar?category_id=1&location_id=0&filter_duration=month_12_year_2025\",\n",
    "]\n",
    "\n",
    "CSV_FILE = \"all_phucket.csv\"\n",
    "\n",
    "\n",
    "def build_driver(headless: bool = False):\n",
    "    \"\"\"\n",
    "    Build a Chrome WebDriver.\n",
    "    headless=False so the browser pops up and you can see it.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless=new\")\n",
    "\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--window-size=1400,1000\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def wait_for_page_and_lazy_load(driver):\n",
    "    \"\"\"\n",
    "    Wait for the event grid to appear, then scroll down several times\n",
    "    to allow lazy-loaded events to load.\n",
    "    \"\"\"\n",
    "    wait = WebDriverWait(driver, 30)\n",
    "\n",
    "    try:\n",
    "        # Wait for at least one event card to appear (based on your XPaths)\n",
    "        wait.until(\n",
    "            EC.presence_of_element_located(\n",
    "                (\n",
    "                    By.XPATH,\n",
    "                    '//*[@id=\"eb-category-grid\"]/div/div[1]/div/div[2]/a'\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        print(\"Warning: event grid did not load in time.\")\n",
    "        return\n",
    "\n",
    "    # Extra pause so everything has time to render\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Scroll down slowly to lazy-load more events\n",
    "    last_height = 0\n",
    "    for _ in range(10):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2.0)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Small pause before scraping\n",
    "    time.sleep(1.5)\n",
    "\n",
    "\n",
    "def load_existing_name_and_url_sets(csv_path: str):\n",
    "    \"\"\"\n",
    "    Load existing event_name and event_url sets from CSV.\n",
    "    Used for dedupe: skip if name OR url already exists.\n",
    "    \"\"\"\n",
    "    existing_names = set()\n",
    "    existing_urls = set()\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        return existing_names, existing_urls\n",
    "\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            name = (row.get(\"event_name\") or \"\").strip()\n",
    "            url = (row.get(\"event_url\") or \"\").strip()\n",
    "            if name:\n",
    "                existing_names.add(name)\n",
    "            if url:\n",
    "                existing_urls.add(url)\n",
    "\n",
    "    return existing_names, existing_urls\n",
    "\n",
    "\n",
    "def save_events_to_csv(csv_path: str, events: list[dict]):\n",
    "    \"\"\"\n",
    "    Append new events to the CSV.\n",
    "    Never removes or overwrites existing rows.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.exists(csv_path)\n",
    "    fieldnames = [\n",
    "        \"event_name\",\n",
    "        \"event_date\",\n",
    "        \"event_location\",\n",
    "        \"event_url\",\n",
    "        \"time_scraped_utc\",\n",
    "    ]\n",
    "\n",
    "    with open(csv_path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        for ev in events:\n",
    "            writer.writerow(ev)\n",
    "\n",
    "\n",
    "def scrape_page(driver, url: str, existing_names: set, existing_urls: set):\n",
    "    \"\"\"\n",
    "    Scrape a single URL and return a list of new event dicts.\n",
    "    Uses dedupe sets passed in to avoid duplicates across pages.\n",
    "    \"\"\"\n",
    "    print(f\"Scraping: {url}\")\n",
    "    driver.get(url)\n",
    "    wait_for_page_and_lazy_load(driver)\n",
    "\n",
    "    # Your example XPaths for first event:\n",
    "    # event name:     //*[@id=\"eb-category-grid\"]/div/div[1]/div/div[2]/a\n",
    "    # event date:     //*[@id=\"eb-category-grid\"]/div/div[1]/div/div[3]\n",
    "    # event location: //*[@id=\"eb-category-grid\"]/div/div[1]/div/div[4]/span\n",
    "    #\n",
    "    # Generalized: each event card is under: //*[@id=\"eb-category-grid\"]/div/div\n",
    "    cards = driver.find_elements(\n",
    "        By.XPATH,\n",
    "        '//*[@id=\"eb-category-grid\"]/div/div'\n",
    "    )\n",
    "\n",
    "    # Track names/urls we see in this run (so we don't duplicate within this run)\n",
    "    seen_names = set()\n",
    "    seen_urls = set()\n",
    "\n",
    "    new_events = []\n",
    "    current_utc = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    for card in cards:\n",
    "        # Event name & URL from div[2]/a\n",
    "        try:\n",
    "            name_elem = card.find_element(By.XPATH, \"./div/div[2]/a\")\n",
    "            event_name = name_elem.text.strip()\n",
    "            event_url = (name_elem.get_attribute(\"href\") or \"\").strip()\n",
    "        except Exception:\n",
    "            event_name = \"\"\n",
    "            event_url = \"\"\n",
    "\n",
    "        if not event_name:\n",
    "            continue\n",
    "\n",
    "        # Event date from div[3]\n",
    "        try:\n",
    "            date_elem = card.find_element(By.XPATH, \"./div/div[3]\")\n",
    "            event_date = date_elem.text.strip()\n",
    "        except Exception:\n",
    "            event_date = \"\"\n",
    "\n",
    "        # Event location from div[4]/span\n",
    "        try:\n",
    "            loc_elem = card.find_element(By.XPATH, \"./div/div[4]/span\")\n",
    "            event_location = loc_elem.text.strip()\n",
    "        except Exception:\n",
    "            event_location = \"\"\n",
    "\n",
    "        # ---------- DEDUPE: name OR url ----------\n",
    "        name_exists = (\n",
    "            event_name in existing_names or\n",
    "            event_name in seen_names\n",
    "        )\n",
    "        url_exists = (\n",
    "            bool(event_url) and\n",
    "            (event_url in existing_urls or event_url in seen_urls)\n",
    "        )\n",
    "\n",
    "        if name_exists or url_exists:\n",
    "            continue\n",
    "\n",
    "        # Mark as seen\n",
    "        seen_names.add(event_name)\n",
    "        if event_url:\n",
    "            seen_urls.add(event_url)\n",
    "\n",
    "        new_events.append(\n",
    "            {\n",
    "                \"event_name\": event_name,\n",
    "                \"event_date\": event_date,\n",
    "                \"event_location\": event_location,\n",
    "                \"event_url\": event_url,\n",
    "                \"time_scraped_utc\": current_utc,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Update global sets for cross-page dedupe\n",
    "    existing_names.update(seen_names)\n",
    "    existing_urls.update(seen_urls)\n",
    "\n",
    "    return new_events\n",
    "\n",
    "\n",
    "def main():\n",
    "    # headless=False ‚Üí show popup browser window while scraping\n",
    "    driver = build_driver(headless=False)\n",
    "    try:\n",
    "        existing_names, existing_urls = load_existing_name_and_url_sets(CSV_FILE)\n",
    "        all_new_events = []\n",
    "\n",
    "        for url in URLS:\n",
    "            events = scrape_page(driver, url, existing_names, existing_urls)\n",
    "            all_new_events.extend(events)\n",
    "\n",
    "        if all_new_events:\n",
    "            save_events_to_csv(CSV_FILE, all_new_events)\n",
    "            print(f\"Added {len(all_new_events)} new events to {CSV_FILE}.\")\n",
    "        else:\n",
    "            print(\"No new events found. CSV unchanged.\")\n",
    "\n",
    "        # Optional: keep browser open briefly so you can see the page\n",
    "        time.sleep(5)\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3265f2e2",
   "metadata": {},
   "source": [
    "# Seoul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7984ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.wanderlochhall.com/15\n",
    "# https://m.ticketlink.co.kr/global/en/local/seoul?sorting=LAST_REGISTRATION_ORDER\n",
    "# https://tkglobal.melon.com/main/index.htm?langCd=EN\n",
    "# https://ticket.yes24.com/New/Genre/GenreMain.aspx?genre=15456&Gcode=009_202_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7062164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 event cards\n",
      "Existing rows kept: 204\n",
      "New unique rows added: 0\n",
      "Total rows now in CSV: 204\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "URL = \"https://www.livenation.kr/en/event/allevents\"\n",
    "CSV_FILE = \"all_seoul.csv\"\n",
    "\n",
    "\n",
    "def init_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # Force English if possible\n",
    "    options.add_argument(\"--lang=en-US\")\n",
    "    # options.add_argument(\"--headless=new\")  # uncomment if you want headless\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def scroll_to_load_all(driver, pause=2):\n",
    "    \"\"\"Scroll until no new content is loaded (for lazy loading).\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(pause)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "\n",
    "def load_existing_csv(csv_path):\n",
    "    \"\"\"Load existing rows + keys to avoid duplicates.\"\"\"\n",
    "    existing_rows = []\n",
    "    existing_keys = set()  # (name, venue, date)\n",
    "\n",
    "    if os.path.exists(csv_path):\n",
    "        with open(csv_path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                existing_rows.append(row)\n",
    "                key = (row.get(\"event_name\", \"\").strip(),\n",
    "                       row.get(\"event_venue\", \"\").strip(),\n",
    "                       row.get(\"event_date\", \"\").strip())\n",
    "                existing_keys.add(key)\n",
    "\n",
    "    return existing_rows, existing_keys\n",
    "\n",
    "\n",
    "def save_csv(csv_path, rows):\n",
    "    fieldnames = [\"event_name\", \"event_venue\", \"event_date\", \"scraped_utc\"]\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "\n",
    "def scrape_livenation_seoul():\n",
    "    driver = init_driver()\n",
    "    driver.get(URL)\n",
    "\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "\n",
    "    # Wait for the main event section to appear\n",
    "    event_section = wait.until(\n",
    "        EC.presence_of_element_located(\n",
    "            (By.XPATH, '//*[@id=\"main\"]/div[2]/div[1]/div/div/div/div[2]')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Scroll to load all events\n",
    "    scroll_to_load_all(driver, pause=2)\n",
    "\n",
    "    # After scrolling, re-grab the section (in case DOM updated)\n",
    "    event_section = driver.find_element(\n",
    "        By.XPATH, '//*[@id=\"main\"]/div[2]/div[1]/div/div/div/div[2]'\n",
    "    )\n",
    "\n",
    "    # Heuristic: each event is usually inside a <li> that contains a <time>\n",
    "    event_cards = event_section.find_elements(By.XPATH, './/li[.//time]')\n",
    "\n",
    "    print(f\"Found {len(event_cards)} event cards\")\n",
    "\n",
    "    existing_rows, existing_keys = load_existing_csv(CSV_FILE)\n",
    "    new_rows = []\n",
    "\n",
    "    utc_now = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    for card in event_cards:\n",
    "        try:\n",
    "            # Event name: based on your XPath sample: span/div/div/p[1]\n",
    "            name_el = card.find_element(By.XPATH, './/span/div/div/p[1]')\n",
    "            event_name = name_el.text.strip()\n",
    "\n",
    "            # Event venue: span/div/div/p[3]\n",
    "            venue_el = card.find_element(By.XPATH, './/span/div/div/p[3]')\n",
    "            event_venue = venue_el.text.strip()\n",
    "\n",
    "            # Event date: <time> tag\n",
    "            time_el = card.find_element(By.XPATH, './/time')\n",
    "            # Use the datetime attribute if available, otherwise text\n",
    "            event_date = time_el.get_attribute(\"datetime\") or time_el.text\n",
    "            event_date = event_date.strip()\n",
    "\n",
    "            key = (event_name, event_venue, event_date)\n",
    "\n",
    "            if key in existing_keys:\n",
    "                # Already in CSV, skip\n",
    "                continue\n",
    "\n",
    "            row = {\n",
    "                \"event_name\": event_name,\n",
    "                \"event_venue\": event_venue,\n",
    "                \"event_date\": event_date,\n",
    "                \"scraped_utc\": utc_now,\n",
    "            }\n",
    "            new_rows.append(row)\n",
    "            existing_keys.add(key)\n",
    "\n",
    "        except Exception as e:\n",
    "            # If any element is missing for a card, just skip that card\n",
    "            print(\"Error parsing an event card:\", e)\n",
    "            continue\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    combined_rows = existing_rows + new_rows\n",
    "    save_csv(CSV_FILE, combined_rows)\n",
    "\n",
    "    print(f\"Existing rows kept: {len(existing_rows)}\")\n",
    "    print(f\"New unique rows added: {len(new_rows)}\")\n",
    "    print(f\"Total rows now in CSV: {len(combined_rows)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_livenation_seoul()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "487d4a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 126 event items\n",
      "Existing rows kept: 204\n",
      "New unique rows added: 25\n",
      "Total rows now in CSV: 229\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "URL = \"https://tkglobal.melon.com/main/index.htm?langCd=EN\"\n",
    "CSV_FILE = \"all_seoul.csv\"\n",
    "\n",
    "\n",
    "def init_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--lang=en-US\")\n",
    "    # options.add_argument(\"--headless=new\")  # uncomment for headless\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def scroll_to_load_all(driver, pause=2):\n",
    "    \"\"\"Scroll down until page height stops changing (for lazy loading).\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(pause)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "\n",
    "def load_existing_csv(csv_path):\n",
    "    \"\"\"Load existing rows & build a key set to avoid duplicates.\"\"\"\n",
    "    existing_rows = []\n",
    "    existing_keys = set()  # (name, venue, date)\n",
    "\n",
    "    if os.path.exists(csv_path):\n",
    "        with open(csv_path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                existing_rows.append(row)\n",
    "                key = (\n",
    "                    row.get(\"event_name\", \"\").strip(),\n",
    "                    row.get(\"event_venue\", \"\").strip(),\n",
    "                    row.get(\"event_date\", \"\").strip(),\n",
    "                )\n",
    "                existing_keys.add(key)\n",
    "\n",
    "    return existing_rows, existing_keys\n",
    "\n",
    "\n",
    "def save_csv(csv_path, rows):\n",
    "    fieldnames = [\"event_name\", \"event_venue\", \"event_date\", \"scraped_utc\"]\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "\n",
    "def scrape_melon_all_seoul():\n",
    "    driver = init_driver()\n",
    "    driver.get(URL)\n",
    "\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "\n",
    "    # Wait for the main contents section\n",
    "    conts_div = wait.until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//*[@id=\"conts\"]/div'))\n",
    "    )\n",
    "\n",
    "    # Scroll to load all events (lazy load)\n",
    "    scroll_to_load_all(driver, pause=2)\n",
    "\n",
    "    # Re-locate the section after scrolling (in case DOM updated)\n",
    "    conts_div = driver.find_element(By.XPATH, '//*[@id=\"conts\"]/div')\n",
    "\n",
    "    # Assuming events are in ul/li under div[1]: //*[@id=\"conts\"]/div/div[1]/ul/li[*]\n",
    "    event_list_items = conts_div.find_elements(\n",
    "        By.XPATH, './div[1]/ul/li'\n",
    "    )\n",
    "\n",
    "    print(f\"Found {len(event_list_items)} event items\")\n",
    "\n",
    "    existing_rows, existing_keys = load_existing_csv(CSV_FILE)\n",
    "    new_rows = []\n",
    "\n",
    "    # Single scrape timestamp for this run\n",
    "    utc_now = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    for li in event_list_items:\n",
    "        try:\n",
    "            # Event name: //*[@id=\"conts\"]/div/div[1]/ul/li[1]/div[2]/h2\n",
    "            name_el = li.find_element(By.XPATH, './div[2]/h2')\n",
    "            event_name = name_el.text.strip()\n",
    "\n",
    "            # Event venue: //*[@id=\"conts\"]/div/div[1]/ul/li[1]/div[2]/dl/dd[2]\n",
    "            venue_el = li.find_element(By.XPATH, './div[2]/dl/dd[2]')\n",
    "            event_venue = venue_el.text.strip()\n",
    "\n",
    "            # Event date: //*[@id=\"conts\"]/div/div[1]/ul/li[1]/div[2]/dl/dd[1]\n",
    "            date_el = li.find_element(By.XPATH, './div[2]/dl/dd[1]')\n",
    "            event_date = date_el.text.strip()\n",
    "\n",
    "            key = (event_name, event_venue, event_date)\n",
    "\n",
    "            # Skip if already in CSV\n",
    "            if key in existing_keys:\n",
    "                continue\n",
    "\n",
    "            row = {\n",
    "                \"event_name\": event_name,\n",
    "                \"event_venue\": event_venue,\n",
    "                \"event_date\": event_date,\n",
    "                \"scraped_utc\": utc_now,\n",
    "            }\n",
    "            new_rows.append(row)\n",
    "            existing_keys.add(key)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing an event item:\", e)\n",
    "            continue\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    combined_rows = existing_rows + new_rows\n",
    "    save_csv(CSV_FILE, combined_rows)\n",
    "\n",
    "    print(f\"Existing rows kept: {len(existing_rows)}\")\n",
    "    print(f\"New unique rows added: {len(new_rows)}\")\n",
    "    print(f\"Total rows now in CSV: {len(combined_rows)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_melon_all_seoul()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
