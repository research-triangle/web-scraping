{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e922ee9",
   "metadata": {},
   "source": [
    "# Singapore (Completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeff49a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_rows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Add scrape timestamp\u001b[39;00m\n\u001b[32m     13\u001b[39m scraped_at = time.strftime(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m\"\u001b[39m, time.gmtime())\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m all_rows:\n\u001b[32m     15\u001b[39m     row[\u001b[33m\"\u001b[39m\u001b[33mscraped_at_utc\u001b[39m\u001b[33m\"\u001b[39m] = scraped_at\n\u001b[32m     17\u001b[39m df_all = pd.DataFrame(\n\u001b[32m     18\u001b[39m     all_rows,\n\u001b[32m     19\u001b[39m     columns=[\u001b[33m\"\u001b[39m\u001b[33mTitle\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mLocation\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mLink\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mscraped_at_utc\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     20\u001b[39m ).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'all_rows' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# COMBINED: Eventbrite + Ticketmaster + Megatix (Featured)\n",
    "# NO DEDUP â€” KEEP ALL EVENTS\n",
    "# pip install tabulate\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.set_window_size(1400, 1000)\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "# -------------------- 1) EVENTBRITE (with auto-scroll + load more) --------------------\n",
    "try:\n",
    "    url = 'https://www.eventbrite.sg/d/singapore--singapore/singapore/?subcategories=3006%2C3025&page=1'\n",
    "    driver.get(url)\n",
    "\n",
    "    LIST_X = \"//*[@id='root']/div/div[2]/div/div/div/div[1]/div/main/div/div/div/section[1]/div/section/div/div/section/ul\"\n",
    "    CARD_X = LIST_X + \"/li\"\n",
    "    BASE   = \".//div/div/div[2]/section/div/section[2]/div\"\n",
    "\n",
    "    WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, LIST_X)))\n",
    "    time.sleep(1)\n",
    "\n",
    "    prev = -1\n",
    "    stable = 0\n",
    "    for _ in range(80):\n",
    "        cards = driver.find_elements(By.XPATH, CARD_X)\n",
    "        count = len(cards)\n",
    "        if count == prev:\n",
    "            stable += 1\n",
    "            if stable >= 2:\n",
    "                try:\n",
    "                    load_more = driver.find_element(\n",
    "                        By.XPATH,\n",
    "                        \"//*[self::button or self::a][contains(.,'Load more') or contains(.,'Show more')]\"\n",
    "                    )\n",
    "                    driver.execute_script(\"arguments[0].click();\", load_more)\n",
    "                    time.sleep(1.2)\n",
    "                    stable = 0\n",
    "                    prev = -1\n",
    "                    continue\n",
    "                except:\n",
    "                    break\n",
    "        else:\n",
    "            stable = 0\n",
    "        prev = count\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1.0)\n",
    "\n",
    "    for ev in driver.find_elements(By.XPATH, CARD_X):\n",
    "        try: title = ev.find_element(By.XPATH, f\"{BASE}/a/h3\").text.strip()\n",
    "        except: title = \"No Title\"\n",
    "        try: date = ev.find_element(By.XPATH, f\"{BASE}/p[1]\").text.strip()\n",
    "        except: date = \"No Date\"\n",
    "        try: location = ev.find_element(By.XPATH, f\"{BASE}/p[2]\").text.strip()\n",
    "        except: location = \"No Location\"\n",
    "        try: link = ev.find_element(By.XPATH, f\"{BASE}/a\").get_attribute(\"href\")\n",
    "        except: link = \"No Link\"\n",
    "        all_rows.append({\"Title\": title, \"Date\": date, \"Location\": location, \"Link\": link})\n",
    "\n",
    "    print(f\"ğŸŸ  Eventbrite: grabbed {len(driver.find_elements(By.XPATH, CARD_X))} events\")\n",
    "except Exception as e:\n",
    "    print(f\"âš  Eventbrite error: {e}\")\n",
    "\n",
    "# -------------------- 2) TICKETMASTER (scroll) --------------------\n",
    "try:\n",
    "    url = 'https://ticketmaster.sg/categories/concerts'\n",
    "    driver.get(url)\n",
    "\n",
    "    EVENTS_X = \"//*[@id='events']\"\n",
    "    ANCHOR_X = \"//*[@id='events']//a[contains(@href,'/activity/detail/')]\"\n",
    "    TITLE_REL = \".//div[2]/div[2]\"\n",
    "    DATE_REL  = \".//div[2]/div[1]\"\n",
    "\n",
    "    WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, EVENTS_X)))\n",
    "    time.sleep(1)\n",
    "\n",
    "    prev = -1\n",
    "    for _ in range(60):\n",
    "        anchors = driver.find_elements(By.XPATH, ANCHOR_X)\n",
    "        if len(anchors) == prev:\n",
    "            break\n",
    "        prev = len(anchors)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1.0)\n",
    "\n",
    "    for a in driver.find_elements(By.XPATH, ANCHOR_X):\n",
    "        try: title = a.find_element(By.XPATH, TITLE_REL).text.strip()\n",
    "        except: title = \"No Title\"\n",
    "        try: date = a.find_element(By.XPATH, DATE_REL).text.strip()\n",
    "        except: date = \"No Date\"\n",
    "        link = a.get_attribute(\"href\") or \"No Link\"\n",
    "        location = \"Singapore\"\n",
    "        all_rows.append({\"Title\": title, \"Date\": date, \"Location\": location, \"Link\": link})\n",
    "\n",
    "    print(f\"ğŸ”µ Ticketmaster: grabbed {len(driver.find_elements(By.XPATH, ANCHOR_X))} events\")\n",
    "except Exception as e:\n",
    "    print(f\"âš  Ticketmaster error: {e}\")\n",
    "\n",
    "# -------------------- 3) MEGATIX (Featured scroll) --------------------\n",
    "try:\n",
    "    url = \"https://megatix.com.sg/?page=2\"\n",
    "    driver.get(url)\n",
    "\n",
    "    WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='__nuxt']\")))\n",
    "    FEATURED_WRAP_X = \"//*[@id='__nuxt']/div/div[3]/div/main/section/section[2]/div[2]/div\"\n",
    "    FEATURED_CARD_X = \"//*[@id='__nuxt']/div/div[3]/div/main/section/section[2]/div[2]//article[.//h3]\"\n",
    "\n",
    "    wrap = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, FEATURED_WRAP_X)))\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", wrap)\n",
    "    time.sleep(0.8)\n",
    "\n",
    "    prev = -1\n",
    "    stable = 0\n",
    "    for _ in range(60):\n",
    "        cards = driver.find_elements(By.XPATH, FEATURED_CARD_X)\n",
    "        count = len(cards)\n",
    "        if count == prev:\n",
    "            stable += 1\n",
    "            if stable >= 2:\n",
    "                break\n",
    "        else:\n",
    "            stable = 0\n",
    "        prev = count\n",
    "        driver.execute_script(\"window.scrollBy(0, 900);\")\n",
    "        time.sleep(1.2)\n",
    "\n",
    "    for ev in driver.find_elements(By.XPATH, FEATURED_CARD_X):\n",
    "        try: title = ev.find_element(By.XPATH, \".//h3/span\").text.strip()\n",
    "        except: title = \"No Title\"\n",
    "        try: date = ev.find_element(By.XPATH, \".//div[1]/div[1]/span\").text.strip()\n",
    "        except: date = \"No Date\"\n",
    "        try: location = ev.find_element(By.XPATH, \".//div[1]/div[3]/span\").text.strip()\n",
    "        except: location = \"Singapore\"\n",
    "        try: link = ev.find_element(By.XPATH, \".//ancestor::a\").get_attribute(\"href\") or \"No Link\"\n",
    "        except: link = \"No Link\"\n",
    "        all_rows.append({\"Title\": title, \"Date\": date, \"Location\": location, \"Link\": link})\n",
    "\n",
    "    print(f\"ğŸŸ¢ Megatix (Featured): grabbed {len(driver.find_elements(By.XPATH, FEATURED_CARD_X))} events\")\n",
    "except Exception as e:\n",
    "    print(f\"âš  Megatix error: {e}\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# -------------------- FINAL OUTPUT (append-only, write to Google Sheets) --------------------\n",
    "# pip install gspread google-auth gspread-dataframe pandas\n",
    "import json\n",
    "# time, pandas already imported above\n",
    "\n",
    "SPREADSHEET_URL = \"https://docs.google.com/spreadsheets/d/1gjYJ1TNwJB8VvEHVNVm7aLVfNIIP0-OrFGymtyvNaAc/edit\"\n",
    "TAB_NAME = \"incompleted\"\n",
    "SERVICE_ACCOUNT_JSON = \"service_account.json\"  # or use env var GOOGLE_SERVICE_ACCOUNT_JSON\n",
    "\n",
    "# Add scrape timestamp\n",
    "scraped_at = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
    "for row in all_rows:\n",
    "    row[\"scraped_at_utc\"] = scraped_at\n",
    "\n",
    "df_all = pd.DataFrame(\n",
    "    all_rows,\n",
    "    columns=[\"Title\", \"Date\", \"Location\", \"Link\", \"scraped_at_utc\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Clean up text fields\n",
    "df_all[\"Link\"] = df_all[\"Link\"].fillna(\"\").str.strip()\n",
    "df_all[\"Title\"] = df_all[\"Title\"].fillna(\"\").str.strip()\n",
    "df_all[\"Date\"] = df_all[\"Date\"].fillna(\"\").str.strip()\n",
    "df_all[\"Location\"] = df_all[\"Location\"].fillna(\"\").str.strip()\n",
    "\n",
    "# ---- Google Sheets append-only (dedup by Link across runs) ----\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "def _authorize():\n",
    "    scopes = [\n",
    "        \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "        \"https://www.googleapis.com/auth/drive\",\n",
    "    ]\n",
    "    if \"GOOGLE_SERVICE_ACCOUNT_JSON\" in os.environ:\n",
    "        keyfile_dict = json.loads(os.environ[\"GOOGLE_SERVICE_ACCOUNT_JSON\"])\n",
    "        creds = Credentials.from_service_account_info(keyfile_dict, scopes=scopes)\n",
    "    else:\n",
    "        creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_JSON, scopes=scopes)\n",
    "    return gspread.authorize(creds)\n",
    "\n",
    "def _open_or_create_ws(client, url, tab_name):\n",
    "    sh = client.open_by_url(url)\n",
    "    try:\n",
    "        ws = sh.worksheet(tab_name)\n",
    "    except gspread.exceptions.WorksheetNotFound:\n",
    "        ws = sh.add_worksheet(title=tab_name, rows=100, cols=10)\n",
    "    # Ensure header\n",
    "    header = [\"Title\", \"Date\", \"Location\", \"Link\", \"scraped_at_utc\"]\n",
    "    current = ws.get_values(\"A1:E1\")\n",
    "    if not current or current[0] != header:\n",
    "        ws.update(\"A1\", [header])\n",
    "    return ws\n",
    "\n",
    "gc = _authorize()\n",
    "ws = _open_or_create_ws(gc, SPREADSHEET_URL, TAB_NAME)\n",
    "\n",
    "# Existing Links (col D) to dedup across runs\n",
    "existing_links = set(v.strip() for v in ws.col_values(4)[1:] if v and v.strip())\n",
    "\n",
    "new_rows_df = df_all[~df_all[\"Link\"].astype(str).str.strip().isin(existing_links)].copy()\n",
    "\n",
    "# Counts/prints aligned with your original console output\n",
    "print(f\"\\nâœ… Newly scraped this run: {len(df_all)}\")\n",
    "print(f\"â• New (not seen before): {len(new_rows_df)}\")\n",
    "\n",
    "# Append new rows (if any)\n",
    "if not new_rows_df.empty:\n",
    "    values = new_rows_df[[\"Title\", \"Date\", \"Location\", \"Link\", \"scraped_at_utc\"]].values.tolist()\n",
    "    ws.append_rows(values, value_input_option=\"RAW\")\n",
    "\n",
    "# Total rows after save (header excluded)\n",
    "total_after = len(ws.col_values(1)) - 1  # header excluded\n",
    "print(f\"ğŸ“¦ Total in master after save: {total_after}\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Google Sheet updated: {SPREADSHEET_URL} (tab: {TAB_NAME})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8569b2",
   "metadata": {},
   "source": [
    "# Kuala Lumpur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eca76fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ¢ Found 8 event cards\n",
      "âœ… Live Nation MY (All Events): grabbed 2 new events this run\n",
      "\n",
      "âœ… TOTAL Unique Events: 85\n",
      "| Title                                                          | Date                  | Location                                 | Link                                                                                                                    |   Event Name |   Scraped At (UTC) | TimeScraped      |\n",
      "|----------------------------------------------------------------|-----------------------|------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|--------------|--------------------|------------------|\n",
      "| MOZARTâ€™S 270TH BIRTHDAY!                                       | DATE 27 Jan 2026      | LOCATION Dewan Filharmonik Petronas KLCC | https://myticket.asia/events/mozarts-270th-birthday/?occurrence=2026-01-27                                              |          nan |                nan | 28/10/2025 11:20 |\n",
      "| MEMOIRS OF A FLORAL FANTASY                                    | DATE 07 Feb 2026      | LOCATION Dewan Filharmonik Petronas KLCC | https://myticket.asia/events/memoirs-of-a-floral-fantasy/?occurrence=2026-02-07                                         |          nan |                nan | 28/10/2025 11:20 |\n",
      "| HARRY POTTER AND THE PRISONER OF AZKABAN IN CONCERT            | DATE 04 - 05 Apr 2026 | LOCATION Dewan Filharmonik Petronas KLCC | https://myticket.asia/events/harry-potter-and-the-prisoner-of-azkaban-in-concert/?occurrence=2026-04-04&time=1775332800 |          nan |                nan | 28/10/2025 11:20 |\n",
      "| THE MPO AND BELLE SISOSKI: ETHNOSPHERE                         | DATE 11 Apr 2026      | LOCATION Dewan Filharmonik Petronas KLCC | https://myticket.asia/events/the-mpo-and-belle-sisoski-ethnosphere/?occurrence=2026-04-11                               |          nan |                nan | 28/10/2025 11:20 |\n",
      "| THE PRODIGIES                                                  | DATE 18 Apr 2026      | LOCATION Aula Simfonia Jakarta           | https://myticket.asia/events/the-prodigies/?occurrence=2026-04-18                                                       |          nan |                nan | 28/10/2025 11:20 |\n",
      "| GLAZUNOV: THE SEASONS IN SYMPHONY                              | DATE 18 Apr 2026      | LOCATION Dewan Filharmonik Petronas KLCC | https://myticket.asia/events/glazunov-the-seasons-in-symphony/?occurrence=2026-04-18                                    |          nan |                nan | 28/10/2025 11:20 |\n",
      "| AN EVENING OF VERDI                                            | DATE 25 Apr 2026      | LOCATION Dewan Filharmonik Petronas KLCC | https://myticket.asia/events/an-evening-of-verdi/?occurrence=2026-04-25                                                 |          nan |                nan | 28/10/2025 11:20 |\n",
      "| STAR WARS: A NEW HOPE IN CONCERT                               | DATE 03 - 04 May 2026 | LOCATION Dewan Filharmonik Petronas KLCC | https://myticket.asia/events/star-wars-a-new-hope-in-concert/?occurrence=2026-05-03&time=1777820400                     |          nan |                nan | 28/10/2025 11:21 |\n",
      "| RESONANCE SHILA AMZAH IN HARMONY: 25 YEARS OF MUSIC & MEMORIES | DATE 09 May 2026      | LOCATION Dewan Filharmonik Petronas KLCC | https://myticket.asia/events/resonance-shila-amzah-in-harmony-25-years-of-music-memories/?occurrence=2026-05-09         |          nan |                nan | 28/10/2025 11:21 |\n",
      "| CHARLES YANG RELOADED                                          | DATE 16 May 2026      | LOCATION Dewan Filharmonik Petronas KLCC | https://myticket.asia/events/charles-yang-reloaded/?occurrence=2026-05-16                                               |          nan |                nan | 28/10/2025 11:21 |\n",
      "| DFP SHOWCASE: CHANSON DE KUALA LUMPUR                          | DATE 01 Nov 2025      | LOCATION Dewan Filharmonik Petronas KLCC | https://myticket.asia/events/dfp-showcase-chanson-de-kuala-lumpur/?occurrence=2025-11-01                                |          nan |                nan | 28/10/2025 11:21 |\n",
      "| MAHLER SYMPHONY NO. 4                                          | DATE 08 Nov 2025      | LOCATION Dewan Filharmonik Petronas KLCC | https://myticket.asia/events/mahler-symphony-no-4/?occurrence=2025-11-08                                                |          nan |                nan | 28/10/2025 11:21 |\n",
      "| APOTHEOSIS: A STAIRWAY TO HEAVEN                               | DATE 10 Nov 2025      | LOCATION Dewan Filharmonik Petronas KLCC | https://myticket.asia/events/apotheosis-a-stairway-to-heaven/?occurrence=2025-11-10                                     |          nan |                nan | 28/10/2025 11:21 |\n",
      "| MALAYSIAN CHAMBER SHOWCASE â€“ 2025                              | DATE 11 Nov 2025      | LOCATION Dewan Filharmonik Petronas KLCC | https://myticket.asia/events/malaysian-chamber-showcase-2025/?occurrence=2025-11-11                                     |          nan |                nan | 28/10/2025 11:21 |\n",
      "| GUITAR SERENADE BY JOSE MARIA GALLARDO DEL REY                 | DATE 22 Nov 2025      | LOCATION Dewan Filharmonik Petronas KLCC | https://myticket.asia/events/guitar-serenade-by-jose-maria-gallardo-del-rey/?occurrence=2025-11-22                      |          nan |                nan | 28/10/2025 11:21 |\n",
      "| BALLET FESTIVAL: SWAN LAKE                                     | DATE 05 - 06 Dec 2025 | LOCATION Dewan Filharmonik Petronas KLCC | https://myticket.asia/events/ballet-festival-swan-lake/?occurrence=2025-12-05&time=1764964800                           |          nan |                nan | 28/10/2025 11:21 |\n",
      "| BALLET FESTIVAL: THE NUTCRACKER                                | DATE 12 - 13 Dec 2025 | LOCATION Dewan Filharmonik Petronas KLCC | https://myticket.asia/events/ballet-festival-the-nutcracker/?occurrence=2025-12-12&time=1765569600                      |          nan |                nan | 28/10/2025 11:21 |\n",
      "| EXISTS X SLAM SINGAPORE                                        | DATE 29 Nov 2025      | LOCATION The Star Theatre                | https://myticket.asia/events/exists-x-slam-singapore/?occurrence=2025-11-29                                             |          nan |                nan | 28/10/2025 11:21 |\n",
      "| Blue 25th Anniversary Tour                                     | MON 09 FEB            | Kuala Lumpur | Zepp KL                   | https://www.livenation.my/event/blue-25th-anniversary-tour-kuala-lumpur-tickets-edp1630573                              |          nan |                nan | nan              |\n",
      "| 2025-26 TREASURE TOUR [PULSE ON]                               | SAT 30 MAY            | Kuala Lumpur | Axiata Arena              | https://www.livenation.my/event/2025-26-treasure-tour-pulse-on--kuala-lumpur-tickets-edp1630577                         |          nan |                nan | nan              |\n",
      "\n",
      "ğŸ’¾ CSV saved: all_events_KL.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Live Nation Malaysia: All Events Scraper (deduplicated) ---\n",
    "# URL: https://www.livenation.my/event/allevents\n",
    "# Output CSV: livenation_my_all_events.csv\n",
    "# Columns: Title, Date, Location, Link\n",
    "# pip install selenium tabulate pandas\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "OUTPUT_CSV = \"all_events_KL.csv\"\n",
    "URL = \"https://www.livenation.my/event/allevents\"\n",
    "\n",
    "# Load existing CSV (if any) to prevent duplicates\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    try:\n",
    "        existing_df = pd.read_csv(OUTPUT_CSV)\n",
    "        seen_links = set(existing_df[\"Link\"].dropna().astype(str))\n",
    "    except Exception:\n",
    "        existing_df = pd.DataFrame(columns=[\"Title\", \"Date\", \"Location\", \"Link\"])\n",
    "        seen_links = set()\n",
    "else:\n",
    "    existing_df = pd.DataFrame(columns=[\"Title\", \"Date\", \"Location\", \"Link\"])\n",
    "    seen_links = set()\n",
    "\n",
    "# ---------------- DRIVER SETUP ----------------\n",
    "opts = webdriver.ChromeOptions()\n",
    "opts.add_argument(\"--start-maximized\")\n",
    "opts.add_argument(\"--disable-notifications\")\n",
    "opts.add_argument(\"--disable-popup-blocking\")\n",
    "driver = webdriver.Chrome(options=opts)\n",
    "driver.set_window_size(1400, 1000)\n",
    "all_rows = []\n",
    "\n",
    "def accept_cookies(drv):\n",
    "    for xp in [\n",
    "        \"//button[contains(., 'Accept All')]\",\n",
    "        \"//button[contains(., 'Accept all')]\",\n",
    "        \"//button[contains(., 'I Accept')]\",\n",
    "        \"//button[contains(., 'Agree')]\",\n",
    "        \"//button[contains(., 'OK')]\",\n",
    "        \"//*[@id='onetrust-accept-btn-handler']\",\n",
    "    ]:\n",
    "        try:\n",
    "            btn = WebDriverWait(drv, 3).until(EC.element_to_be_clickable((By.XPATH, xp)))\n",
    "            drv.execute_script(\"arguments[0].click();\", btn)\n",
    "            time.sleep(0.2)\n",
    "            break\n",
    "        except TimeoutException:\n",
    "            continue\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    # Collapse whitespace/newlines and trim\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
    "\n",
    "# ---------------- SCRAPER ----------------\n",
    "try:\n",
    "    driver.get(URL)\n",
    "    WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main']\")))\n",
    "    time.sleep(1.0)\n",
    "    accept_cookies(driver)\n",
    "\n",
    "    # Each LI that has an event link and the date container\n",
    "    ALL_CARDS_X = \"//*[@id='main']//ul//li[.//a[contains(@href,'/event/')]]\"\n",
    "\n",
    "    # Scroll to load everything (and click Load/Show more if present)\n",
    "    prev = -1\n",
    "    stable = 0\n",
    "    for _ in range(80):\n",
    "        cards = driver.find_elements(By.XPATH, ALL_CARDS_X)\n",
    "        count = len(cards)\n",
    "        if count == prev:\n",
    "            stable += 1\n",
    "            # Try \"Load/Show more\" if present\n",
    "            try:\n",
    "                more = driver.find_element(By.XPATH, \"//button[contains(.,'Load more') or contains(.,'Show more')]\")\n",
    "                if more.is_enabled() and more.is_displayed():\n",
    "                    driver.execute_script(\"arguments[0].click();\", more)\n",
    "                    time.sleep(1.2)\n",
    "                    cards = driver.find_elements(By.XPATH, ALL_CARDS_X)\n",
    "                    count = len(cards)\n",
    "                    if count > prev:\n",
    "                        stable = 0\n",
    "            except Exception:\n",
    "                pass\n",
    "            if stable >= 2:\n",
    "                break\n",
    "        else:\n",
    "            stable = 0\n",
    "        prev = count\n",
    "        driver.execute_script(\"window.scrollBy(0, 1200);\")\n",
    "        time.sleep(1.1)\n",
    "\n",
    "    cards = driver.find_elements(By.XPATH, ALL_CARDS_X)\n",
    "    print(f\"ğŸŸ¢ Found {len(cards)} event cards\")\n",
    "\n",
    "    for li in cards:\n",
    "        try:\n",
    "            # Link (primary dedup key)\n",
    "            try:\n",
    "                link = li.find_element(By.XPATH, \".//a[contains(@href,'/event/')]\").get_attribute(\"href\") or \"\"\n",
    "            except Exception:\n",
    "                link = \"\"\n",
    "            if not link or link in seen_links:\n",
    "                continue\n",
    "\n",
    "            # Title (your path first)\n",
    "            title = \"\"\n",
    "            for xp in [\n",
    "                \".//span/div/div/p[1]\",   # close to //*[@id='1616419']/span/div/div/p[1]\n",
    "                \".//p[1]\",\n",
    "                \".//div//p[1]\"\n",
    "            ]:\n",
    "                try:\n",
    "                    title = clean_text(li.find_element(By.XPATH, xp).text)\n",
    "                    if title:\n",
    "                        break\n",
    "                except Exception:\n",
    "                    continue\n",
    "            if not title:\n",
    "                title = \"No Title\"\n",
    "\n",
    "            # Location (your path first)\n",
    "            location = \"\"\n",
    "            for xp in [\n",
    "                \".//span/div/div/p[3]/span\",  # close to //*[@id='1616419']/span/div/div/p[3]/span\n",
    "                \".//p[3]/span\",\n",
    "                \".//p[3]\"\n",
    "            ]:\n",
    "                try:\n",
    "                    location = clean_text(li.find_element(By.XPATH, xp).text)\n",
    "                    if location:\n",
    "                        break\n",
    "                except Exception:\n",
    "                    continue\n",
    "            if not location:\n",
    "                location = \"Malaysia\"\n",
    "\n",
    "            # Date â€” USE YOUR DIV CONTAINER XPATH RELATIVELY, then fall back to <time>\n",
    "            # Provided absolute: //*[@id=\"main\"]/div[2]/div[1]/div/div/div/div[2]/ul[1]/li[1]/div/div[1]\n",
    "            # Relative to each li, this corresponds to: .//div/div[1]\n",
    "            date_text = \"\"\n",
    "            for xp in [\n",
    "                \".//div/div[1]\",          # your container (first choice)\n",
    "                \".//time\",                # fallback: visible text\n",
    "            ]:\n",
    "                try:\n",
    "                    node = li.find_element(By.XPATH, xp)\n",
    "                    # prefer node.text, otherwise datetime attribute if it's <time>\n",
    "                    date_text = clean_text(node.text or node.get_attribute(\"datetime\") or \"\")\n",
    "                    if date_text:\n",
    "                        break\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            all_rows.append({\n",
    "                \"Title\": title,\n",
    "                \"Date\": date_text,\n",
    "                \"Location\": location,\n",
    "                \"Link\": link\n",
    "            })\n",
    "            seen_links.add(link)\n",
    "\n",
    "        except StaleElementReferenceException:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"âš  Skipped one card due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"âœ… Live Nation MY (All Events): grabbed {len(all_rows)} new events this run\")\n",
    "\n",
    "except TimeoutException:\n",
    "    print(\"âš  Timeout waiting for Live Nation MY page\")\n",
    "except Exception as e:\n",
    "    print(f\"âš  Error: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "# ---------------- OUTPUT ----------------\n",
    "if all_rows:\n",
    "    new_df = pd.DataFrame(all_rows, columns=[\"Title\", \"Date\", \"Location\", \"Link\"])\n",
    "    final_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "else:\n",
    "    final_df = existing_df.copy()\n",
    "\n",
    "# Final safety dedup\n",
    "if not final_df.empty:\n",
    "    final_df[\"Link\"] = final_df[\"Link\"].astype(str).str.strip()\n",
    "    final_df = final_df.drop_duplicates(subset=[\"Link\"], keep=\"first\")\n",
    "    final_df = final_df.drop_duplicates(subset=[\"Title\", \"Date\", \"Location\"], keep=\"first\")\n",
    "\n",
    "print(f\"\\nâœ… TOTAL Unique Events: {len(final_df)}\")\n",
    "print(tabulate(final_df.tail(20), headers=\"keys\", tablefmt=\"github\", showindex=False))\n",
    "\n",
    "final_df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\nğŸ’¾ CSV saved: {OUTPUT_CSV}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6435c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening listing: https://my.bookmyshow.com/en/collections/e?cities=Kuala+Lumpur___Kuala+Lumpur\n",
      "Found 20 cards in listing.\n",
      "\n",
      "[1/20] 3Pã€Šé‘«ä¸–çºªã€‹å·¡å›æ¼”å”±ä¼š - é©¬æ¥è¥¿äºšç«™ | 31 Oct 2025\n",
      "  â†’ Opening tab #1: https://my.bookmyshow.com/en/events/3P-2025/BMS3PGMY\n",
      "    âœ“ Location scraped: Axiata Arena Bukit Jalil\n",
      "\n",
      "[2/20] Janice Vidal OUT OF FRAME World Tour in Malaysia | 1 Nov 2025\n",
      "  â†’ Opening tab #2: https://my.bookmyshow.com/en/events/janice-vidal-out-of-frame-world-tour-in-malaysia/BMSJAN25\n",
      "    âœ“ Location scraped: Arena of Stars, Resorts World Genting\n",
      "\n",
      "[3/20] [CANCELLED] 2025 LOCO CONCERT: SCRAPS TOUR in Asia - Malaysia | 2 Nov 2025\n",
      "  â†’ Opening tab #3: https://my.bookmyshow.com/en/events/cancelled-2025-loco-concert-scraps-tour-in-asia-malaysia/LOCOMY25\n",
      "    âœ“ Location scraped: Zepp Kuala Lumpur\n",
      "\n",
      "[4/20] ã€Šä½³ä¹ä¹‹å¤œé‡‘æ›²æ¼”å”±ä¼šã€‹é©¬æ¥è¥¿äºšç«™ | 2 Nov 2025\n",
      "  â†’ Opening tab #4: https://my.bookmyshow.com/en/events/jia-le-hokkien-hits-2025/BMSJIALE\n",
      "    âœ“ Location scraped: Axiata Arena Bukit Jalil\n",
      "\n",
      "[5/20] Engelbert Humperdinck's LAST WALTZ FAREWELL TOUR - Malaysia | 2 Nov 2025\n",
      "  â†’ Opening tab #5: https://my.bookmyshow.com/en/events/engelbert-humperdincks-last-waltz-farewell-tour-malaysia/BMSENGE1\n",
      "    âœ“ Location scraped: Mega Star Arena, Sungei Wang Plaza\n",
      "\n",
      "[6/20] 2025 ZEROBASEONE WORLD TOUR [HERE & NOW] IN KUALA LUMPUR | 8 Nov 2025\n",
      "  â†’ Opening tab #6: https://my.bookmyshow.com/en/events/2025-zerobaseone/BMSZBOKL\n",
      "    âœ“ Location scraped: IDEA LIVE ARENA\n",
      "\n",
      "[7/20] ã€ŠGO WITH THE FLOWã€‹æ—å³¯ä¸–ç•Œå·¡å›æ¼”å”±ä¼š ï½œ å‰éš†å¡ç«™ â€œGO WITH THE FLOWâ€ LF Live Around the World | Kuala Lumpur | 15 Nov 2025\n",
      "  â†’ Opening tab #7: https://my.bookmyshow.com/en/events/go-with-the-flow-lf-live-around-the-world-kuala-lumpur/GWTFKL25\n",
      "    âš  Location not found (leaving blank).\n",
      "\n",
      "[8/20] 2025 FTISLAND LIVE â€˜MAD HAPPYâ€™ IN KUALA LUMPUR | 15 Nov 2025\n",
      "  â†’ Opening tab #8: https://my.bookmyshow.com/en/events/2025-ftisland-live-mad-happy-2025/FTMHKL25\n",
      "    âœ“ Location scraped: Mega Star Arena, Sungei Wang Plaza\n",
      "\n",
      "[9/20] â€œBOUNDLESSâ€ MIKA KOBAYASHI SPECIAL SHOW 2025 | 16 Nov 2025\n",
      "  â†’ Opening tab #9: https://my.bookmyshow.com/en/events/mika-kobayashi-2025/MIKAKL25\n",
      "    âœ“ Location scraped: Zepp Kuala Lumpur\n",
      "\n",
      "[10/20] 2025 JONATHAN LEE æå®—ç›› ã€Šæœ‰æ­Œä¹‹å¹´ã€‹å·¡å›æ¼”å”±ä¼š - é©¬æ¥è¥¿äºšç«™ | 22 Nov 2025\n",
      "  â†’ Opening tab #10: https://my.bookmyshow.com/en/events/2025-jonathan-lee-li-zong-sheng/BMSLZS25\n",
      "    âœ“ Location scraped: Axiata Arena Bukit Jalil\n",
      "\n",
      "[11/20] HATSUNE MIKU EXPO 2025 in KUALA LUMPUR | 22 Nov 2025\n",
      "  â†’ Opening tab #11: https://my.bookmyshow.com/en/events/hatsune-miku-expo-2025/BMSHATSU\n",
      "    âœ“ Location scraped: IDEA LIVE ARENA\n",
      "\n",
      "[12/20] Alan Tam è°­å’éºŸã€Šç»å…¸ä¼ å¥‡ã€‹å·¡å›æ¼”å”±ä¼š - é©¬æ¥è¥¿äºšç«™ 2025 | 29 Nov 2025\n",
      "  â†’ Opening tab #12: https://my.bookmyshow.com/en/events/alan-tam-2025/BMSALANT\n",
      "    âœ“ Location scraped: Axiata Arena Bukit Jalil\n",
      "\n",
      "[13/20] Shila Amzah åŠ è”¡æ©é›¨ã€Šç¾¤æ˜Ÿé—ªè€€ï¼Œçˆ±åœ¨ç¾é‡Œã€‹Cultural Concert | 29 Nov 2025\n",
      "  â†’ Opening tab #13: https://my.bookmyshow.com/en/events/shila-amzah-priscilla-abby-2025/MALAMGMT\n",
      "    âš  Location not found (leaving blank).\n",
      "\n",
      "[14/20] Yue YunPeng & Sun Yue 2025 Crosstalk World Tour in Malaysia å²³äº‘é¹ã€å­™è¶Šã€Šå››æµ·ç¦ä¸´ 2025 ç›¸å£°ä¸“åœºã€‹ä¸–ç•Œå·¡æ¼” â€“ å‰éš†å¡ç«™ | 4 Dec 2025\n",
      "  â†’ Opening tab #14: https://my.bookmyshow.com/en/events/yue-yunpeng-sun-yue-2025/DEYUNSHE\n",
      "    âœ“ Location scraped: Mega Star Arena, Sungei Wang Plaza\n",
      "\n",
      "[15/20] Leon Live 2025 in Malaysia 2025é»æ˜å·¡å›æ¼”å”±ä¼š â€” é©¬æ¥è¥¿äºšç«™ | 6 Dec 2025\n",
      "  â†’ Opening tab #15: https://my.bookmyshow.com/en/events/leon-lai-2025/BMSLEONL\n",
      "    âœ“ Location scraped: Axiata Arena Bukit Jalil\n",
      "\n",
      "[16/20] MAN WITH A MISSION \"HOWLING ACROSS THE WORLD 2025 - ASIA TOUR\" in KUALA LUMPUR | 12 Dec 2025\n",
      "  â†’ Opening tab #16: https://my.bookmyshow.com/en/events/man-with-a-mission-2025/MWAM25KL\n",
      "    âœ“ Location scraped: Zepp Kuala Lumpur\n",
      "\n",
      "[17/20] 2025 DIORå¤§ç©ã€ŠKAMPUNG GIRL æ™®é€šäººã€‹ é¦–å ´å€‹äººæ¼”å”±æœƒ | 13 Dec 2025\n",
      "  â†’ Opening tab #17: https://my.bookmyshow.com/en/events/2025-dior-kampung-girl/KPGLDIOR\n",
      "    âœ“ Location scraped: Arena of Stars, Resorts World Genting\n",
      "\n",
      "[18/20] 2025 WENDY 1st WORLD TOUR <W:EALIVE> IN KUALA LUMPUR | 20 Dec 2025\n",
      "  â†’ Opening tab #18: https://my.bookmyshow.com/en/events/2025-wendy/WNDYKL25\n",
      "    âœ“ Location scraped: Zepp Kuala Lumpur\n",
      "\n",
      "[19/20] ç—›ä»° Miserable Faith  â€œThe World Will Be BetterÂ²â€ 2026 ä¸œå—äºšå·¡æ¼” â€“ å‰éš†å¡ | 25 Jan 2026\n",
      "  â†’ Opening tab #19: https://my.bookmyshow.com/en/events/miserable-faith-2026/MF2026MY\n",
      "    âš  Location not found (leaving blank).\n",
      "\n",
      "[20/20] TOMORROW X TOGETHER WORLD TOUR <ACT : TOMORROW> IN KUALA LUMPUR | 14 Feb 2026\n",
      "  â†’ Opening tab #20: https://my.bookmyshow.com/en/events/tomorrow-x-together-world-tour/BMSTXT26\n",
      "    âœ“ Location scraped: Axiata Arena Bukit Jalil\n",
      "\n",
      "Saved 85 total rows -> all_events_KL.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BookMyShow MY â€” Kuala Lumpur scraper (visible tabs; NaN-safe merge; 4-column CSV)\n",
    "- Stable scrape with slower listing reading\n",
    "- Safe merge that handles NaN, floats, blanks gracefully\n",
    "- CSV schema: Title | Date | Location | Link\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import traceback\n",
    "import hashlib\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    StaleElementReferenceException,\n",
    "    WebDriverException,\n",
    ")\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "LIST_URL   = \"https://my.bookmyshow.com/en/collections/e?cities=Kuala+Lumpur___Kuala+Lumpur\"\n",
    "OUTPUT_CSV = \"all_events_KL.csv\"\n",
    "\n",
    "HEADLESS          = False\n",
    "OPEN_IN_NEW_TAB   = True\n",
    "PAUSE_ON_EACH_TAB = 0.8\n",
    "TAKE_ERROR_SHOTS  = True\n",
    "\n",
    "SLOW_MODE               = True\n",
    "LISTING_INITIAL_PAUSE   = 1.2\n",
    "PER_CARD_SCROLL_SETTLE  = 0.35\n",
    "PER_CARD_READ_RETRIES   = 3\n",
    "PER_CARD_READ_SLEEP     = 0.25\n",
    "BETWEEN_CARD_PAUSE      = 0.1\n",
    "\n",
    "WAIT_SEC          = 20\n",
    "PAGE_LOAD_TIMEOUT = 25\n",
    "\n",
    "X_LIST_CONTAINER = \"/html/body/div[1]/main/div/div/div[2]\"\n",
    "X_CARD_ANCHOR    = X_LIST_CONTAINER + \"/a[{i}]\"\n",
    "X_NAME           = \"/html/body/div[1]/main/div/div/div[2]/a[{i}]/div/div[2]/div/div/div[1]\"\n",
    "X_DATE           = \"/html/body/div[1]/main/div/div/div[2]/a[{i}]/div/div[2]/div/div/div[2]/span\"\n",
    "X_VENUE_DETAIL   = \"/html/body/div[1]/main/div/div/div[2]/div[1]/div[3]/div[4]/div[1]\"\n",
    "\n",
    "# ===================== DRIVER =====================\n",
    "def build_driver(headless: bool = HEADLESS) -> webdriver.Chrome:\n",
    "    ua = (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "          \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\")\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    opts.add_argument(f\"user-agent={ua}\")\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--window-size=1400,1000\")\n",
    "    opts.add_argument(\"--disable-notifications\")\n",
    "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    opts.page_load_strategy = \"eager\"\n",
    "\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "    driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)\n",
    "    driver.implicitly_wait(2)\n",
    "    return driver\n",
    "\n",
    "# ===================== SAFE STRING HELPERS =====================\n",
    "def _s(x) -> str:\n",
    "    \"\"\"Convert any NaN, None, or float to safe string.\"\"\"\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        if isinstance(x, float) and pd.isna(x):\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return str(x)\n",
    "\n",
    "def normalize_link(u) -> str:\n",
    "    u = _s(u).strip().lower()\n",
    "    return u.rstrip(\"/\")\n",
    "\n",
    "def stable_event_id(title, date_str, location, link_norm) -> str:\n",
    "    link_norm = _s(link_norm).strip().lower()\n",
    "    if link_norm:\n",
    "        return link_norm\n",
    "    key = \"|\".join([\n",
    "        _s(title).strip().lower(),\n",
    "        _s(date_str).strip().lower(),\n",
    "        _s(location).strip().lower(),\n",
    "    ])\n",
    "    return hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# ===================== HELPERS =====================\n",
    "def wait_present(driver, xpath, timeout=WAIT_SEC):\n",
    "    return WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_element_located((By.XPATH, xpath))\n",
    "    )\n",
    "\n",
    "def scroll_lazy(driver, rounds=10, pause=0.5):\n",
    "    last_h = 0\n",
    "    for _ in range(rounds):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(pause)\n",
    "        h = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "        if h == last_h:\n",
    "            break\n",
    "        last_h = h\n",
    "\n",
    "def get_text_abs(driver, xpath) -> str:\n",
    "    try:\n",
    "        return driver.find_element(By.XPATH, xpath).text.strip()\n",
    "    except NoSuchElementException:\n",
    "        return \"\"\n",
    "    except StaleElementReferenceException:\n",
    "        try:\n",
    "            return driver.find_element(By.XPATH, xpath).text.strip()\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "def read_text_nonempty(driver, xpath: str, retries: int, sleep_s: float) -> str:\n",
    "    txt = get_text_abs(driver, xpath)\n",
    "    if txt:\n",
    "        return txt\n",
    "    for _ in range(retries):\n",
    "        time.sleep(sleep_s)\n",
    "        txt = get_text_abs(driver, xpath)\n",
    "        if txt:\n",
    "            break\n",
    "    return txt\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def safe_screenshot(driver, path: str):\n",
    "    try:\n",
    "        driver.save_screenshot(path)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def robust_get(driver, url: str, tries: int = 2, settle: float = 0.4):\n",
    "    last_err: Optional[Exception] = None\n",
    "    for _ in range(tries):\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(settle)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(0.7)\n",
    "    if last_err:\n",
    "        raise last_err\n",
    "\n",
    "def open_in_new_tab(driver, url: str):\n",
    "    current = driver.current_window_handle\n",
    "    driver.execute_script(\"window.open(arguments[0], '_blank');\", url)\n",
    "    WebDriverWait(driver, 10).until(lambda d: len(d.window_handles) > 1)\n",
    "    new_handle = [h for h in driver.window_handles if h != current][-1]\n",
    "    driver.switch_to.window(new_handle)\n",
    "    return current, new_handle\n",
    "\n",
    "def close_tab_and_return(driver, original_handle: str):\n",
    "    try:\n",
    "        driver.close()\n",
    "    finally:\n",
    "        driver.switch_to.window(original_handle)\n",
    "\n",
    "# ===================== MERGE (NaN-SAFE) =====================\n",
    "def merge_with_existing(df_new: pd.DataFrame, path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge with old CSV (if exists):\n",
    "    - Keeps old rows (no removals)\n",
    "    - No duplicates\n",
    "    - Overwrites same events with latest scrape\n",
    "    - NaN-safe\n",
    "    \"\"\"\n",
    "    df_new = df_new.copy()\n",
    "    for col in [\"Title\",\"Date\",\"Location\",\"Link\"]:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = \"\"\n",
    "    df_new[[\"Title\",\"Date\",\"Location\",\"Link\"]] = df_new[[\"Title\",\"Date\",\"Location\",\"Link\"]].fillna(\"\")\n",
    "    df_new[\"LinkNorm\"] = df_new[\"Link\"].apply(normalize_link)\n",
    "    df_new[\"EventID\"]  = df_new.apply(\n",
    "        lambda r: stable_event_id(r.get(\"Title\",\"\"), r.get(\"Date\",\"\"),\n",
    "                                  r.get(\"Location\",\"\"), r.get(\"LinkNorm\",\"\")), axis=1\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        df_old = pd.read_csv(path)\n",
    "    except FileNotFoundError:\n",
    "        return df_new[[\"Title\",\"Date\",\"Location\",\"Link\"]]\n",
    "\n",
    "    df_old = df_old.copy()\n",
    "    for col in [\"Title\",\"Date\",\"Location\",\"Link\"]:\n",
    "        if col not in df_old.columns:\n",
    "            df_old[col] = \"\"\n",
    "    df_old[[\"Title\",\"Date\",\"Location\",\"Link\"]] = df_old[[\"Title\",\"Date\",\"Location\",\"Link\"]].fillna(\"\")\n",
    "    df_old[\"LinkNorm\"] = df_old[\"Link\"].apply(normalize_link)\n",
    "    df_old[\"EventID\"]  = df_old.apply(\n",
    "        lambda r: stable_event_id(r.get(\"Title\",\"\"), r.get(\"Date\",\"\"),\n",
    "                                  r.get(\"Location\",\"\"), r.get(\"LinkNorm\",\"\")), axis=1\n",
    "    )\n",
    "\n",
    "    merged = pd.concat([df_old, df_new], ignore_index=True)\n",
    "    merged = merged.drop_duplicates(subset=[\"EventID\"], keep=\"last\")\n",
    "    merged = merged.drop_duplicates(subset=[\"LinkNorm\"], keep=\"last\")\n",
    "    return merged[[\"Title\",\"Date\",\"Location\",\"Link\"]]\n",
    "\n",
    "# ===================== CORE SCRAPE =====================\n",
    "def collect_listing_cards(driver) -> List[Dict[str, str]]:\n",
    "    print(f\"Opening listing: {LIST_URL}\")\n",
    "    robust_get(driver, LIST_URL, tries=3, settle=0.6)\n",
    "    wait_present(driver, X_LIST_CONTAINER, timeout=WAIT_SEC)\n",
    "    scroll_lazy(driver, rounds=10, pause=0.5)\n",
    "    if SLOW_MODE:\n",
    "        time.sleep(LISTING_INITIAL_PAUSE)\n",
    "\n",
    "    anchors = driver.find_elements(By.XPATH, X_LIST_CONTAINER + \"/a\")\n",
    "    n = len(anchors)\n",
    "    print(f\"Found {n} cards in listing.\")\n",
    "    cards = []\n",
    "    for i in range(1, n + 1):\n",
    "        name_x = X_NAME.format(i=i)\n",
    "        date_x = X_DATE.format(i=i)\n",
    "        a_x    = X_CARD_ANCHOR.format(i=i)\n",
    "\n",
    "        try:\n",
    "            a_el = driver.find_element(By.XPATH, a_x)\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", a_el)\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(PER_CARD_SCROLL_SETTLE)\n",
    "\n",
    "        title = read_text_nonempty(driver, name_x, PER_CARD_READ_RETRIES, PER_CARD_READ_SLEEP)\n",
    "        date  = read_text_nonempty(driver, date_x, PER_CARD_READ_RETRIES, PER_CARD_READ_SLEEP)\n",
    "\n",
    "        href = \"\"\n",
    "        try:\n",
    "            a_el = driver.find_element(By.XPATH, a_x)\n",
    "            href = (a_el.get_attribute(\"href\") or \"\").strip()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        cards.append({\"i\": i, \"title\": title, \"date\": date, \"href\": href})\n",
    "        time.sleep(BETWEEN_CARD_PAUSE)\n",
    "    return cards\n",
    "\n",
    "def scrape_detail_for_location(driver, url: str, idx: int) -> str:\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    try:\n",
    "        if OPEN_IN_NEW_TAB:\n",
    "            print(f\"  â†’ Opening tab #{idx}: {url}\")\n",
    "            original = driver.current_window_handle\n",
    "            driver.execute_script(\"window.open(arguments[0], '_blank');\", url)\n",
    "            WebDriverWait(driver, 10).until(lambda d: len(d.window_handles) > 1)\n",
    "            new_handle = [h for h in driver.window_handles if h != original][-1]\n",
    "            driver.switch_to.window(new_handle)\n",
    "        else:\n",
    "            original = driver.current_window_handle\n",
    "            driver.get(url)\n",
    "\n",
    "        time.sleep(PAUSE_ON_EACH_TAB)\n",
    "\n",
    "        try:\n",
    "            WebDriverWait(driver, WAIT_SEC).until(\n",
    "                EC.presence_of_element_located((By.XPATH, X_VENUE_DETAIL))\n",
    "            )\n",
    "            location = driver.find_element(By.XPATH, X_VENUE_DETAIL).text.strip()\n",
    "            print(f\"    âœ“ Location scraped: {location[:60]}\")\n",
    "        except TimeoutException:\n",
    "            location = \"\"\n",
    "            print(\"    âš  Location not found (leaving blank).\")\n",
    "\n",
    "        if OPEN_IN_NEW_TAB:\n",
    "            driver.close()\n",
    "            driver.switch_to.window(original)\n",
    "        return location\n",
    "    except Exception as e:\n",
    "        print(f\"    âœ— Error on detail page: {type(e).__name__}: {e}\")\n",
    "        if TAKE_ERROR_SHOTS:\n",
    "            ensure_dir(\"debug_screens\")\n",
    "            safe_screenshot(driver, f\"debug_screens/error_event_{idx}.png\")\n",
    "        try:\n",
    "            if OPEN_IN_NEW_TAB:\n",
    "                driver.close()\n",
    "                driver.switch_to.window(original)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return \"\"\n",
    "\n",
    "def scrape_bms_kl(driver: webdriver.Chrome) -> pd.DataFrame:\n",
    "    cards = collect_listing_cards(driver)\n",
    "    rows = []\n",
    "    for idx, c in enumerate(cards, start=1):\n",
    "        title = c[\"title\"]\n",
    "        date  = c[\"date\"]\n",
    "        href  = c[\"href\"]\n",
    "        print(f\"\\n[{idx}/{len(cards)}] {title} | {date}\")\n",
    "        location = scrape_detail_for_location(driver, href, idx) if href else \"\"\n",
    "        rows.append({\"Title\": title, \"Date\": date, \"Location\": location, \"Link\": href})\n",
    "    return pd.DataFrame(rows, columns=[\"Title\",\"Date\",\"Location\",\"Link\"])\n",
    "\n",
    "# ===================== MAIN =====================\n",
    "def main(headless=HEADLESS):\n",
    "    driver = build_driver(headless=headless)\n",
    "    try:\n",
    "        df_new = scrape_bms_kl(driver)\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except Exception:\n",
    "            pass\n",
    "    if df_new.empty:\n",
    "        print(\"No events found.\")\n",
    "        return\n",
    "    out = merge_with_existing(df_new, OUTPUT_CSV)\n",
    "    out.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nSaved {len(out)} total rows -> {OUTPUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main(headless=HEADLESS)\n",
    "    except Exception as e:\n",
    "        print(f\"[fatal] {type(e).__name__}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ad3f1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ†• 0 new event(s) added (85 total now in file).\n",
      "| Title                                                                 | Date                  | Location                                    | Link                                                                                                                    | TimeScraped         |\n",
      "|-----------------------------------------------------------------------|-----------------------|---------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|---------------------|\n",
      "| THE INCREDIBLE VOYAGE OF ALASDAIR MALLOY                              | DATE 23 May 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/the-incredible-voyage-of-alasdair-malloy/?occurrence=2026-05-23                            | 2025-10-28 17:19:58 |\n",
      "| A TRIBUTE TO SHARIFAH AINI                                            | DATE 06 Jun 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/a-tribute-to-sharifah-aini/?occurrence=2026-06-06                                          | 2025-10-28 17:20:01 |\n",
      "| JAZZ, CLASSICAL AND BEYOND WITH THE MPO                               | DATE 13 Jun 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/jazz-classical-and-beyond-with-the-mpo/?occurrence=2026-06-13                              | 2025-10-28 17:20:03 |\n",
      "| ARCTIC WINDS AND SUMMER SUN                                           | DATE 20 Jun 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/arctic-winds-and-summer-sun/?occurrence=2026-06-20                                         | 2025-10-28 17:20:06 |\n",
      "| SYMPHONIC GHIBLI II                                                   | DATE 03 - 04 Jul 2026 | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/symphonic-ghibli-ii/?occurrence=2026-07-03&time=1783108800                                 | 2025-10-28 17:20:08 |\n",
      "| METROPOLITAN RHYTHMS: THE EMIGRÃ‰ AND THE AMERICAN                     | DATE 18 Jul 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/metropolitan-rhythms-the-emigre-and-the-american/?occurrence=2026-07-18                    | 2025-10-28 17:20:10 |\n",
      "| WORKSHOP â€“ MAKYONG SHAKESPEAREâ€™S THE COMEDY OF ERRORS (AN ADAPTATION) | DATE 25 Jul 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/workshop-makyong-shakespeares-the-comedy-of-errors-an-adaptation/?occurrence=2026-07-25    | 2025-10-28 17:20:12 |\n",
      "| MAK YONG SHAKESPEARE: THE COMEDY OF ERRORS â€“ AN ADAPTATION            | DATE 25 Jul 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/mak-yong-shakespeare-the-comedy-of-errors-an-adaptation/?occurrence=2026-07-25             | 2025-10-28 17:20:15 |\n",
      "| A HEART UNVEILED: THE MUSIC OF TCHAIKOVSKY                            | DATE 22 Aug 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/a-heart-unveiled-the-music-of-tchaikovsky/?occurrence=2026-08-22                           | 2025-10-28 17:20:17 |\n",
      "| A KNIGHTâ€™S TALE: VALOR AND ROMANCE IN MUSIC                           | DATE 29 Aug 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/a-knights-tale-valor-and-romance-in-music/?occurrence=2026-08-29                           | 2025-10-28 17:20:19 |\n",
      "| A TRIBUTE TO ALFONSO SOLIANO                                          | DATE 02 Sep 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/a-tribute-to-alfonso-soliano/?occurrence=2026-09-02                                        | 2025-10-28 17:20:21 |\n",
      "| YIN AND YANG: A DANCE KALEIDOSCOPE                                    | DATE 05 Sep 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/yin-and-yang-a-dance-kaleidoscope/?occurrence=2026-09-05                                   | 2025-10-28 17:20:24 |\n",
      "| WORKSHOP â€“ YIN AND YANG: A DANCE KALEIDOSCOPE                         | DATE 06 Sep 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/workshop-yin-and-yang-a-dance-kaleidoscope/?occurrence=2026-09-06                          | 2025-10-28 17:20:26 |\n",
      "| MAURICE STEGERâ€™S NATURE CONCERTI                                      | DATE 12 Sep 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/maurice-stegers-nature-concerti/?occurrence=2026-09-12                                     | 2025-10-28 17:20:29 |\n",
      "| FROM BROADWAY TO DISNEY: LEA SALONGA WITH THE MPO                     | DATE 26 Sep 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/from-broadway-to-disney-lea-salonga-with-the-mpo/?occurrence=2026-09-26                    | 2025-10-28 17:20:32 |\n",
      "| THREE BY THREE                                                        | DATE 03 Oct 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/three-by-three/?occurrence=2026-10-03                                                      | 2025-10-28 17:20:36 |\n",
      "| THE MUSIC OF QUEENâ€¦LIVES ON!                                          | DATE 10 Oct 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/the-music-of-queen-lives-on/?occurrence=2026-10-10                                         | 2025-10-28 17:20:44 |\n",
      "| JACLYN VICTOR GEMILANG BERSAMA MPO                                    | DATE 17 Oct 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/jaclyn-victor-gemilang-bersama-mpo/?occurrence=2026-10-17                                  | 2025-10-28 17:20:47 |\n",
      "| A REGAL EVENING WITH STEPHEN HOUGH                                    | DATE 24 Oct 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/a-regal-evening-with-stephen-hough/?occurrence=2026-10-24                                  | 2025-10-28 17:20:55 |\n",
      "| BEATS OF BORNEO: ALENA MURANG WITH THE MPO                            | DATE 31 Oct 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/beats-of-borneo-alena-murang-with-the-mpo/?occurrence=2026-10-31                           | 2025-10-28 17:21:01 |\n",
      "| AS IF SHE WERE HERE: CHEN JIA SINGS TERESA TENG                       | DATE 07 Nov 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/as-if-she-were-here-chen-jia-sings-teresa-teng/?occurrence=2026-11-07                      | 2025-10-28 17:21:08 |\n",
      "| SIMFONI MANTRA: KUNTO AJI BERSAMA MPO                                 | DATE 14 Nov 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/simfoni-mantra-kunto-aji-bersama-mpo/?occurrence=2026-11-14                                | 2025-10-28 17:21:12 |\n",
      "| A CHORALE SPECTACULAR                                                 | DATE 05 Dec 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/a-chorale-spectacular/?occurrence=2026-12-05                                               | 2025-10-28 17:21:15 |\n",
      "| CRYSTAL HALLOWEEN FEAT YURI NAKAGAWA                                  | DATE 31 Oct 2025      | LOCATION Wet Deck W KL W Hotel Kuala Lumpur | https://myticket.asia/events/crystal-halloween-feat-yuri-nakagawa/?occurrence=2025-10-31                                | 2025-10-28 17:21:18 |\n",
      "| CRYSTAL HALLOWEEN â€“ SECOND WAVE                                       | DATE 01 Nov 2025      | LOCATION Wet Deck W KL W Hotel Kuala Lumpur | https://myticket.asia/events/crystal-halloween-second-wave/?occurrence=2025-11-01                                       | 2025-10-28 17:21:21 |\n",
      "| SHOWCASE JIWA MALAYSIA                                                | DATE 01 Nov 2025      | LOCATION Dewan Sri Putra Bukit Jalil        | https://myticket.asia/events/showcase-jiwa-malaysia/?occurrence=2025-11-01                                              | 2025-10-28 17:21:27 |\n",
      "| VOICE MASTERCLASS WITH HERA HYESANG PARK                              | DATE 06 Nov 2025      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/voice-masterclass-with-hera-hyesang-park/?occurrence=2025-11-06                            | 2025-10-28 17:21:30 |\n",
      "| WET SESSIONS FEAT AMINE K                                             | DATE 15 Nov 2025      | LOCATION Wet Deck W KL W Hotel Kuala Lumpur | https://myticket.asia/events/wet-sessions-feat-amine-k/?occurrence=2025-11-15                                           | 2025-10-28 17:21:34 |\n",
      "| SAVED BY GRACE A SING ALONG HYMNS CONCERT                             | DATE 22 Nov 2025      | LOCATION Aula Simfonia Jakarta              | https://myticket.asia/events/saved-by-grace-a-sing-along-hymns-concert/?occurrence=2025-11-22                           | 2025-10-28 17:21:38 |\n",
      "| KS CHITHRA LIVE WITH THE MPO                                          | DATE 29 Nov 2025      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/ks-chithra-live-with-the-mpo/?occurrence=2025-11-29                                        | 2025-10-28 17:21:44 |\n",
      "| RISE THROUGH RHYTHM                                                   | DATE 30 Nov 2025      | LOCATION Aula Simfonia Jakarta              | https://myticket.asia/events/rise-through-rhythm/?occurrence=2025-11-30                                                 | 2025-10-28 17:21:50 |\n",
      "| INUL DARATISTA MILLENIAL DANGDUT NUSANTARA                            | DATE 06 Dec 2025      | LOCATION SPICE Arena Penang Penang          | https://myticket.asia/events/inul-daratista-millenial-dangdut-nusantara/?occurrence=2025-12-06                          | 2025-10-28 17:21:59 |\n",
      "| AMADEUS LIVE                                                          | DATE 10 Jan 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/amadeus-live/?occurrence=2026-01-10                                                        | 2025-10-28 17:22:06 |\n",
      "| FAMILY FUN DAY: CIRQUE DE LA SYMPHONIE                                | DATE 17 Jan 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/family-fun-day-cirque-de-la-symphonie/?occurrence=2026-01-17                               | 2025-10-28 17:22:11 |\n",
      "| CIRQUE DE LA SYMPHONIE                                                | DATE 17 Jan 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/cirque-de-la-symphonie-2/?occurrence=2026-01-17                                            | 2025-10-28 17:22:14 |\n",
      "| P. RAMLEEâ€™S MADU TIGA: â€˜LIVEâ€™ IN CONCERT                              | DATE 24 Jan 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/p-ramlees-madu-tiga-live-in-concert/?occurrence=2026-01-24                                 | 2025-10-28 17:22:17 |\n",
      "| MOZARTâ€™S 270TH BIRTHDAY!                                              | DATE 27 Jan 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/mozarts-270th-birthday/?occurrence=2026-01-27                                              | 2025-10-28 17:22:20 |\n",
      "| MEMOIRS OF A FLORAL FANTASY                                           | DATE 07 Feb 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/memoirs-of-a-floral-fantasy/?occurrence=2026-02-07                                         | 2025-10-28 17:22:22 |\n",
      "| HARRY POTTER AND THE PRISONER OF AZKABAN IN CONCERT                   | DATE 04 - 05 Apr 2026 | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/harry-potter-and-the-prisoner-of-azkaban-in-concert/?occurrence=2026-04-04&time=1775332800 | 2025-10-28 17:22:25 |\n",
      "| THE MPO AND BELLE SISOSKI: ETHNOSPHERE                                | DATE 11 Apr 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/the-mpo-and-belle-sisoski-ethnosphere/?occurrence=2026-04-11                               | 2025-10-28 17:22:27 |\n",
      "| THE PRODIGIES                                                         | DATE 18 Apr 2026      | LOCATION Aula Simfonia Jakarta              | https://myticket.asia/events/the-prodigies/?occurrence=2026-04-18                                                       | 2025-10-28 17:22:31 |\n",
      "| GLAZUNOV: THE SEASONS IN SYMPHONY                                     | DATE 18 Apr 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/glazunov-the-seasons-in-symphony/?occurrence=2026-04-18                                    | 2025-10-28 17:22:34 |\n",
      "| AN EVENING OF VERDI                                                   | DATE 25 Apr 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/an-evening-of-verdi/?occurrence=2026-04-25                                                 | 2025-10-28 17:22:37 |\n",
      "| STAR WARS: A NEW HOPE IN CONCERT                                      | DATE 03 - 04 May 2026 | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/star-wars-a-new-hope-in-concert/?occurrence=2026-05-03&time=1777820400                     | 2025-10-28 17:22:39 |\n",
      "| RESONANCE SHILA AMZAH IN HARMONY: 25 YEARS OF MUSIC & MEMORIES        | DATE 09 May 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/resonance-shila-amzah-in-harmony-25-years-of-music-memories/?occurrence=2026-05-09         | 2025-10-28 17:22:43 |\n",
      "| CHARLES YANG RELOADED                                                 | DATE 16 May 2026      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/charles-yang-reloaded/?occurrence=2026-05-16                                               | 2025-10-28 17:22:45 |\n",
      "| MUSICAL NAN KONG CONCERT VERSION                                      | DATE 28 Oct 2025      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/musical-nan-kong-concert-version/?occurrence=2025-10-28                                    | 2025-10-28 17:22:48 |\n",
      "| DFP SHOWCASE: CHANSON DE KUALA LUMPUR                                 | DATE 01 Nov 2025      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/dfp-showcase-chanson-de-kuala-lumpur/?occurrence=2025-11-01                                | 2025-10-28 17:22:50 |\n",
      "| MAHLER SYMPHONY NO. 4                                                 | DATE 08 Nov 2025      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/mahler-symphony-no-4/?occurrence=2025-11-08                                                | 2025-10-28 17:22:53 |\n",
      "| APOTHEOSIS: A STAIRWAY TO HEAVEN                                      | DATE 10 Nov 2025      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/apotheosis-a-stairway-to-heaven/?occurrence=2025-11-10                                     | 2025-10-28 17:22:56 |\n",
      "| MALAYSIAN CHAMBER SHOWCASE â€“ 2025                                     | DATE 11 Nov 2025      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/malaysian-chamber-showcase-2025/?occurrence=2025-11-11                                     | 2025-10-28 17:22:59 |\n",
      "| GUITAR SERENADE BY JOSE MARIA GALLARDO DEL REY                        | DATE 22 Nov 2025      | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/guitar-serenade-by-jose-maria-gallardo-del-rey/?occurrence=2025-11-22                      | 2025-10-28 17:23:03 |\n",
      "| BALLET FESTIVAL: SWAN LAKE                                            | DATE 05 - 06 Dec 2025 | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/ballet-festival-swan-lake/?occurrence=2025-12-05&time=1764964800                           | 2025-10-28 17:23:05 |\n",
      "| BALLET FESTIVAL: THE NUTCRACKER                                       | DATE 12 - 13 Dec 2025 | LOCATION Dewan Filharmonik Petronas KLCC    | https://myticket.asia/events/ballet-festival-the-nutcracker/?occurrence=2025-12-12&time=1765569600                      | 2025-10-28 17:23:08 |\n",
      "| EXISTS X SLAM SINGAPORE                                               | DATE 29 Nov 2025      | LOCATION The Star Theatre                   | https://myticket.asia/events/exists-x-slam-singapore/?occurrence=2025-11-29                                             | 2025-10-28 17:23:12 |\n",
      "\n",
      "Saved 55 unique rows â†’ all_events_KL.csv\n"
     ]
    }
   ],
   "source": [
    "# scrape_myticket_asia.py\n",
    "# pip install selenium pandas tabulate\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, time, re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException, StaleElementReferenceException, ElementClickInterceptedException\n",
    ")\n",
    "\n",
    "HOME = \"https://myticket.asia/\"\n",
    "OUTPUT_CSV = \"all_events_KL.csv\"\n",
    "WAIT = 25\n",
    "\n",
    "# ---------- driver ----------\n",
    "def make_driver() -> webdriver.Chrome:\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    opts.add_argument(\"--start-maximized\")\n",
    "    opts.add_argument(\"--disable-notifications\")\n",
    "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "    # Uncomment for headless:\n",
    "    # opts.add_argument(\"--headless=new\")\n",
    "    return webdriver.Chrome(options=opts)\n",
    "\n",
    "# ---------- utils ----------\n",
    "def slow_scroll_full_page(driver: webdriver.Chrome, step_px=500, nap=0.25, max_loops=200):\n",
    "    last = -1\n",
    "    for _ in range(max_loops):\n",
    "        driver.execute_script(\"window.scrollBy(0, arguments[0]);\", step_px)\n",
    "        time.sleep(nap)\n",
    "        y = driver.execute_script(\"return window.scrollY;\")\n",
    "        if y == last:\n",
    "            break\n",
    "        last = y\n",
    "\n",
    "def click_all_load_more(driver: webdriver.Chrome, max_clicks=50):\n",
    "    \"\"\"\n",
    "    Click all visible 'Load More' variants used by Modern Events Calendar.\n",
    "    Non-exact: CSS + text-normalization fallback.\n",
    "    \"\"\"\n",
    "    def visible(e): \n",
    "        try: return e.is_displayed() and e.is_enabled()\n",
    "        except: return False\n",
    "\n",
    "    clicks = 0\n",
    "    while clicks < max_clicks:\n",
    "        # Try common MEC 'load more' controls\n",
    "        buttons = []\n",
    "        buttons += driver.find_elements(By.CSS_SELECTOR, \"a.mec-load-more, button.mec-load-more\")\n",
    "        buttons += driver.find_elements(By.CSS_SELECTOR, \".mec-load-more a, .mec-load-more button\")\n",
    "        # Text fallback (case-insensitive contains 'load more')\n",
    "        buttons += [e for e in driver.find_elements(By.TAG_NAME, \"a\") if re.search(r'load\\s*more', (e.text or '').lower())]\n",
    "\n",
    "        # Dedup DOM elements\n",
    "        seen_ids = set()\n",
    "        uniq = []\n",
    "        for b in buttons:\n",
    "            try:\n",
    "                key = b.id\n",
    "                if key not in seen_ids:\n",
    "                    seen_ids.add(key)\n",
    "                    uniq.append(b)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        clicked_any = False\n",
    "        for b in uniq:\n",
    "            if not visible(b): \n",
    "                continue\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", b)\n",
    "                time.sleep(0.25)\n",
    "                b.click()\n",
    "                clicked_any = True\n",
    "                clicks += 1\n",
    "                time.sleep(1.2)  # allow new cards to render\n",
    "            except (ElementClickInterceptedException, StaleElementReferenceException):\n",
    "                continue\n",
    "            except Exception:\n",
    "                continue\n",
    "        if not clicked_any:\n",
    "            break\n",
    "\n",
    "def load_existing(path: str) -> pd.DataFrame:\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            return pd.read_csv(path)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return pd.DataFrame(columns=[\"Title\", \"Date\", \"Location\", \"Link\", \"TimeScraped\"])\n",
    "\n",
    "def save_append_dedup(df_new: pd.DataFrame, path: str):\n",
    "    old = load_existing(path)\n",
    "    all_df = pd.concat([old, df_new], ignore_index=True)\n",
    "    # primary dedup by Link; secondary by (Title, Date, Location)\n",
    "    all_df = all_df.drop_duplicates(subset=[\"Link\"], keep=\"first\")\n",
    "    all_df = all_df.drop_duplicates(subset=[\"Title\", \"Date\", \"Location\"], keep=\"first\")\n",
    "    all_df.to_csv(path, index=False)\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    return \" \".join((s or \"\").split())\n",
    "\n",
    "# ---------- flexible field extraction on detail pages (no exact XPaths) ----------\n",
    "def extract_title(driver: webdriver.Chrome) -> str:\n",
    "    # Prefer page H1 within main area, fallback to any h1\n",
    "    for css in [\"main h1\", \"#main-content h1\", \"h1\"]:\n",
    "        els = driver.find_elements(By.CSS_SELECTOR, css)\n",
    "        for el in els:\n",
    "            t = clean_text(el.text)\n",
    "            if t: return t\n",
    "    return \"\"\n",
    "\n",
    "def extract_date(driver: webdriver.Chrome) -> str:\n",
    "    # Try <time>, then common date classes, then definition list dd near date-like labels\n",
    "    # 1) time tags\n",
    "    for el in driver.find_elements(By.TAG_NAME, \"time\"):\n",
    "        t = clean_text(el.text)\n",
    "        if t: return t\n",
    "    # 2) common date-ish classes/labels\n",
    "    for css in [\"[class*='date']\", \"[class*='time']\"]:\n",
    "        for el in driver.find_elements(By.CSS_SELECTOR, css):\n",
    "            t = clean_text(el.text)\n",
    "            if t and len(t) > 2: return t\n",
    "    # 3) simple dl/dd extraction: pick the dd whose preceding label mentions date/time\n",
    "    dds = driver.find_elements(By.CSS_SELECTOR, \"dl dd\")\n",
    "    dts = driver.find_elements(By.CSS_SELECTOR, \"dl dt\")\n",
    "    for i, dt in enumerate(dts):\n",
    "        label = (dt.text or \"\").lower()\n",
    "        if any(k in label for k in [\"date\", \"time\", \"when\"]):\n",
    "            try:\n",
    "                t = clean_text(dds[i].text)\n",
    "                if t: return t\n",
    "            except Exception:\n",
    "                pass\n",
    "    # fallback: first dd with non-empty text\n",
    "    for el in dds:\n",
    "        t = clean_text(el.text)\n",
    "        if t: return t\n",
    "    return \"\"\n",
    "\n",
    "def extract_location(driver: webdriver.Chrome) -> str:\n",
    "    # Look for 'venue' or 'location' labels first\n",
    "    dds = driver.find_elements(By.CSS_SELECTOR, \"dl dd\")\n",
    "    dts = driver.find_elements(By.CSS_SELECTOR, \"dl dt\")\n",
    "    for i, dt in enumerate(dts):\n",
    "        label = (dt.text or \"\").lower()\n",
    "        if any(k in label for k in [\"venue\", \"location\", \"where\"]):\n",
    "            try:\n",
    "                t = clean_text(dds[i].text)\n",
    "                if t: return t\n",
    "            except Exception:\n",
    "                pass\n",
    "    # Generic class-based fallbacks\n",
    "    for css in [\"[class*='venue']\", \"[class*='location']\"]:\n",
    "        for el in driver.find_elements(By.CSS_SELECTOR, css):\n",
    "            t = clean_text(el.text)\n",
    "            if t and len(t) > 2: return t\n",
    "    # last resort: look for address-like blocks\n",
    "    for tag in [\"address\", \"p\", \"span\", \"div\"]:\n",
    "        for el in driver.find_elements(By.TAG_NAME, tag):\n",
    "            txt = clean_text(el.text)\n",
    "            if any(k in txt.lower() for k in [\"hall\", \"theatre\", \"theater\", \"ballroom\", \"arena\", \"convention\", \"stadium\", \"klcc\"]):\n",
    "                if len(txt) <= 120:\n",
    "                    return txt\n",
    "    return \"\"\n",
    "\n",
    "# ---------- main ----------\n",
    "def main():\n",
    "    driver = make_driver()\n",
    "    wait = WebDriverWait(driver, WAIT)\n",
    "    rows = []\n",
    "\n",
    "    try:\n",
    "        driver.get(HOME)\n",
    "        wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        time.sleep(1.0)\n",
    "\n",
    "        # Load everything we can see: scroll + load more + scroll again\n",
    "        slow_scroll_full_page(driver)\n",
    "        click_all_load_more(driver)\n",
    "        slow_scroll_full_page(driver)\n",
    "\n",
    "        # Collect ALL event links (no exact XPath):\n",
    "        # MEC usually wraps titles in h4 > a inside containers with id starting mec_skin_events_.\n",
    "        card_links = driver.find_elements(By.CSS_SELECTOR, \"div[id^='mec_skin_events_'] h4 a, section[id^='mec_skin_events_'] h4 a\")\n",
    "        # If homepage changes, also try generic h4 > a with myticket links\n",
    "        if not card_links:\n",
    "            card_links = [a for a in driver.find_elements(By.CSS_SELECTOR, \"h4 a\") if \"myticket.asia\" in (a.get_attribute(\"href\") or \"\")]\n",
    "\n",
    "        # Dedup by href\n",
    "        links = []\n",
    "        seen = set()\n",
    "        for el in card_links:\n",
    "            try:\n",
    "                href = (el.get_attribute(\"href\") or \"\").strip()\n",
    "                title_from_list = clean_text(el.text)\n",
    "                if href and href not in seen:\n",
    "                    seen.add(href)\n",
    "                    links.append((title_from_list, href))\n",
    "            except StaleElementReferenceException:\n",
    "                continue\n",
    "\n",
    "        if not links:\n",
    "            print(\"No event links found. Site structure may have changed.\")\n",
    "            return\n",
    "\n",
    "        # Visit each link in a new tab and extract fields\n",
    "        for title_from_list, href in links:\n",
    "            driver.execute_script(\"window.open(arguments[0], '_blank');\", href)\n",
    "            driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "            try:\n",
    "                wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "            except TimeoutException:\n",
    "                pass\n",
    "\n",
    "            name = extract_title(driver) or title_from_list\n",
    "            date = extract_date(driver)\n",
    "            location = extract_location(driver)\n",
    "\n",
    "            row = {\n",
    "                \"Title\": clean_text(name),\n",
    "                \"Date\": clean_text(date),\n",
    "                \"Location\": clean_text(location),\n",
    "                \"Link\": href,\n",
    "                \"TimeScraped\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "            # close tab and return\n",
    "            driver.close()\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "            time.sleep(0.4)\n",
    "\n",
    "        df = pd.DataFrame(rows)\n",
    "\n",
    "        # -------- NEW: show how many new events were added --------\n",
    "        old_df = load_existing(OUTPUT_CSV)\n",
    "        old_count = len(old_df)\n",
    "\n",
    "        save_append_dedup(df, OUTPUT_CSV)\n",
    "\n",
    "        updated_df = load_existing(OUTPUT_CSV)\n",
    "        new_total = len(updated_df)\n",
    "        added_count = new_total - old_count\n",
    "        print(f\"\\nğŸ†• {added_count} new event(s) added ({new_total} total now in file).\")\n",
    "        # -------- END NEW --------\n",
    "\n",
    "        from tabulate import tabulate\n",
    "        print(tabulate(df, headers=\"keys\", tablefmt=\"github\", showindex=False))\n",
    "        print(f\"\\nSaved {len(df)} unique rows â†’ {OUTPUT_CSV}\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577adaeb",
   "metadata": {},
   "source": [
    "# Jarkata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b023b0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ¢ Found 7 featured cards\n",
      "âœ… Megatix Indonesia (Featured): grabbed 0 new events\n",
      "\n",
      "âœ… TOTAL Unique Events: 9\n",
      "| Title                                       | Date             | Location             | Link                                                       |\n",
      "|---------------------------------------------|------------------|----------------------|------------------------------------------------------------|\n",
      "| X-Clusive Presents: Bali NYE 2025 with Tyga | Wed, 31 Dec 2025 | The Stage            | https://megatix.co.id/events/nye-2025-with-tyga            |\n",
      "| X-Clusive Presents: Bali NYE 2025 with Tyga | Wed, 31 Dec 2025 | The Stage            | https://megatix.co.id/events/nye-2025-with-tyga            |\n",
      "| MEDUZA                                      | Sat, 25 Oct 2025 | Savaya Bali          | https://megatix.co.id/events/meduza-2510                   |\n",
      "| ILOVEUNITED Jakarta                         | Sat, 25 Oct 2025 | Community Park PIK 2 | https://megatix.co.id/events/iloveunited-jakarta           |\n",
      "| NYE 2025 FEATURING DASH BERLIN              | Wed, 31 Dec 2025 | CafÃ© del Mar Bali    | https://megatix.co.id/events/NYE-2025-FEATURING-DASHBERLIN |\n",
      "| JUNCTION: AFRICA VOL.6                      | Fri, 31 Oct 2025 | The Mesa Bali        | https://megatix.co.id/events/junction-africa-vol6          |\n",
      "| NEW YEAR'S EVE PARTY AT W BALI - SEMINYAK   | Wed, 31 Dec 2025 | W Bali - Seminyak    | https://megatix.co.id/events/NYE-W-BALI-SEMINYAK           |\n",
      "| HAYDEN JAMES                                | Sat, 08 Nov 2025 | Savaya Bali          | https://megatix.co.id/events/hayden-james-0811             |\n",
      "| DEFECTED                                    | Sun, 09 Nov 2025 | Savaya Bali          | https://megatix.co.id/events/defected-0911                 |\n",
      "\n",
      "ğŸ’¾ CSV saved: all_jakarta.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Megatix Indonesia: Featured Events Scraper (deduplicated) ---\n",
    "# Output CSV: all_jakarta.csv\n",
    "# Works for https://megatix.co.id/\n",
    "# pip install selenium tabulate pandas\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime   # <-- added for timestamp\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "OUTPUT_CSV = \"all_jakarta.csv\"\n",
    "URL = \"https://megatix.co.id/\"\n",
    "\n",
    "# Load existing CSV (if any) to prevent duplicates\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    existing_df = pd.read_csv(OUTPUT_CSV)\n",
    "    seen_links = set(existing_df[\"Link\"].dropna())\n",
    "else:\n",
    "    existing_df = pd.DataFrame()\n",
    "    seen_links = set()\n",
    "\n",
    "# ---------------- DRIVER SETUP ----------------\n",
    "opts = webdriver.ChromeOptions()\n",
    "opts.add_argument(\"--start-maximized\")\n",
    "opts.add_argument(\"--disable-notifications\")\n",
    "driver = webdriver.Chrome(options=opts)\n",
    "driver.set_window_size(1400, 1000)\n",
    "all_rows = []\n",
    "\n",
    "# ---------------- SCRAPER ----------------\n",
    "try:\n",
    "    driver.get(URL)\n",
    "    WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='__nuxt']\")))\n",
    "    time.sleep(1.0)\n",
    "\n",
    "    FEATURED_CARD_X = \"//*[@id='__nuxt']/div/div[3]/div/main/section/section[2]/div[2]//article[.//h3]\"\n",
    "\n",
    "    # Scroll to load all cards\n",
    "    prev = -1\n",
    "    stable = 0\n",
    "    for _ in range(60):\n",
    "        cards = driver.find_elements(By.XPATH, FEATURED_CARD_X)\n",
    "        count = len(cards)\n",
    "        if count == prev:\n",
    "            stable += 1\n",
    "            if stable >= 2:\n",
    "                break\n",
    "        else:\n",
    "            stable = 0\n",
    "        prev = count\n",
    "        driver.execute_script(\"window.scrollBy(0, 900);\")\n",
    "        time.sleep(1.2)\n",
    "\n",
    "    cards = driver.find_elements(By.XPATH, FEATURED_CARD_X)\n",
    "    print(f\"ğŸŸ¢ Found {len(cards)} featured cards\")\n",
    "\n",
    "    for ev in cards:\n",
    "        try:\n",
    "            title = ev.find_element(By.XPATH, \".//h3/span\").text.strip()\n",
    "        except:\n",
    "            title = \"No Title\"\n",
    "        try:\n",
    "            date = ev.find_element(By.XPATH, \".//div[1]/div[1]/span\").text.strip()\n",
    "        except:\n",
    "            date = \"No Date\"\n",
    "        try:\n",
    "            location = ev.find_element(By.XPATH, \".//div[1]/div[3]/span\").text.strip()\n",
    "        except:\n",
    "            location = \"Indonesia\"\n",
    "        try:\n",
    "            link = ev.find_element(By.XPATH, \".//ancestor::a\").get_attribute(\"href\") or \"No Link\"\n",
    "        except:\n",
    "            link = \"No Link\"\n",
    "\n",
    "        # Skip duplicates already in CSV\n",
    "        if link in seen_links:\n",
    "            continue\n",
    "\n",
    "        all_rows.append({\n",
    "            \"Title\": title,\n",
    "            \"Date\": date,\n",
    "            \"Location\": location,\n",
    "            \"Link\": link,\n",
    "            \"TimeScraped\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # <-- added line\n",
    "        })\n",
    "        seen_links.add(link)\n",
    "\n",
    "    print(f\"âœ… Megatix Indonesia (Featured): grabbed {len(all_rows)} new events\")\n",
    "\n",
    "except TimeoutException:\n",
    "    print(\"âš  Timeout waiting for Megatix Indonesia page\")\n",
    "except Exception as e:\n",
    "    print(f\"âš  Error: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "# ---------------- OUTPUT ----------------\n",
    "if all_rows:\n",
    "    new_df = pd.DataFrame(all_rows)\n",
    "    final_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "else:\n",
    "    final_df = existing_df\n",
    "\n",
    "print(f\"\\nâœ… TOTAL Unique Events: {len(final_df)}\")\n",
    "print(tabulate(final_df.tail(20), headers=\"keys\", tablefmt=\"github\", showindex=False))\n",
    "\n",
    "final_df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\nğŸ’¾ CSV saved: {OUTPUT_CSV}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e007c521",
   "metadata": {},
   "source": [
    "# Hong Kong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16fb98de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†º Loaded 20 previously-scraped rows\n",
      "\n",
      "âœ… New rows appended this run: 2\n",
      "ğŸ“¦ Total rows in file: 25\n",
      "| Platform     | Title                                                | Date       | Location   | Link                                                                                                               |\n",
      "|--------------|------------------------------------------------------|------------|------------|--------------------------------------------------------------------------------------------------------------------|\n",
      "| LiveNationHK | DAY6 10th Anniversary Tour <The DECADE> in HONG KONG | SUN 18 JAN | DAY6       | https://www.livenation.hk/en/event/day6-10th-anniversary-tour-the-decade-in-hong-kong-hong-kong-tickets-edp1630128 |\n",
      "| LiveNationHK | Blue 25th Anniversary Tour                           | SAT 07 FEB | Blue       | https://www.livenation.hk/en/event/blue-25th-anniversary-tour-hong-kong-tickets-edp1630645                         |\n",
      "\n",
      "ğŸ’¾ CSV updated: all_hong_kong.csv\n",
      "ğŸ§¯ Backup saved: all_hong_kong.csv.bak\n"
     ]
    }
   ],
   "source": [
    "# --- HONG KONG: Live Nation only ---\n",
    "# Output CSV: all_hong_kong.csv\n",
    "# pip install selenium tabulate pandas\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import time, os, unicodedata, shutil\n",
    "from urllib.parse import urlparse\n",
    "from datetime import datetime  # <-- added for timestamp\n",
    "\n",
    "# =================== CONFIG ===================\n",
    "OUTPUT_CSV = \"all_hong_kong.csv\"\n",
    "BACKUP_CSV = OUTPUT_CSV + \".bak\"\n",
    "LIVE_HOME = \"https://www.livenation.hk/en\"\n",
    "WAIT = 25\n",
    "\n",
    "# =================== DRIVER ===================\n",
    "ua = (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "      \"(KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\")\n",
    "opts = webdriver.ChromeOptions()\n",
    "opts.add_argument(f\"user-agent={ua}\")\n",
    "opts.add_argument(\"--start-maximized\")\n",
    "opts.add_argument(\"--disable-notifications\")\n",
    "opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "opts.page_load_strategy = \"eager\"\n",
    "\n",
    "driver = webdriver.Chrome(options=opts)\n",
    "driver.set_window_size(1400, 1000)\n",
    "driver.set_page_load_timeout(60)\n",
    "driver.set_script_timeout(60)\n",
    "\n",
    "# =================== UTILS ===================\n",
    "def accept_cookies(driver):\n",
    "    for by, sel in [\n",
    "        (By.ID, \"onetrust-accept-btn-handler\"),\n",
    "        (By.XPATH, \"//*[@id='onetrust-accept-btn-handler']\"),\n",
    "        (By.XPATH, \"//*[self::button or self::a][contains(.,'Accept All') or contains(.,'Accept')]\"),\n",
    "    ]:\n",
    "        try:\n",
    "            el = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((by, sel)))\n",
    "            driver.execute_script(\"arguments[0].click();\", el)\n",
    "            time.sleep(0.2)\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "def safe_text(el):\n",
    "    try:\n",
    "        return unicodedata.normalize(\"NFKC\", \" \".join(el.text.split()))\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def norm(s):\n",
    "    return \" \".join(unicodedata.normalize(\"NFKC\", (s or \"\").strip().lower()).split())\n",
    "\n",
    "def canonicalize_link(link):\n",
    "    if not link:\n",
    "        return \"\"\n",
    "    p = urlparse(link)\n",
    "    return f\"{(p.netloc or '').lower()}{p.path or ''}\"\n",
    "\n",
    "def make_event_id(platform, link, title, date):\n",
    "    canon = canonicalize_link(link)\n",
    "    if canon:\n",
    "        return f\"{platform}|{canon}\"\n",
    "    return f\"{norm(platform)}|{norm(title)}|{norm(date)}\"\n",
    "\n",
    "# =================== LOAD OLD CSV ===================\n",
    "seen_ids_persisted = set()\n",
    "old_df = None\n",
    "column_order = [\"Platform\", \"Title\", \"Date\", \"Location\", \"Link\", \"TimeScraped\"]  # <-- added column here\n",
    "\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    try:\n",
    "        old_df = pd.read_csv(OUTPUT_CSV, dtype=str, keep_default_na=False)\n",
    "        if list(old_df.columns):\n",
    "            column_order = list(old_df.columns)\n",
    "        for _, r in old_df.iterrows():\n",
    "            seen_ids_persisted.add(\n",
    "                make_event_id(\n",
    "                    str(r.get(\"Platform\",\"\")),\n",
    "                    str(r.get(\"Link\",\"\")),\n",
    "                    str(r.get(\"Title\",\"\")),\n",
    "                    str(r.get(\"Date\",\"\")),\n",
    "                )\n",
    "            )\n",
    "        print(f\"â†º Loaded {len(seen_ids_persisted)} previously-scraped rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Could not read existing CSV: {e}\")\n",
    "\n",
    "all_rows = []\n",
    "seen_links_this_run = set()\n",
    "\n",
    "def append_row(platform, title, date_text, location, link):\n",
    "    row = {\n",
    "        \"Platform\": platform,\n",
    "        \"Title\": title or \"No Title\",\n",
    "        \"Date\": date_text or \"No Date\",\n",
    "        \"Location\": location or \"Hong Kong\",\n",
    "        \"Link\": link or \"No Link\",\n",
    "        \"TimeScraped\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # <-- added timestamp\n",
    "    }\n",
    "    eid = make_event_id(row[\"Platform\"], row[\"Link\"], row[\"Title\"], row[\"Date\"])\n",
    "    if eid in seen_ids_persisted:\n",
    "        return\n",
    "    canon = canonicalize_link(row[\"Link\"])\n",
    "    if canon and canon in seen_links_this_run:\n",
    "        return\n",
    "    if canon:\n",
    "        seen_links_this_run.add(canon)\n",
    "    all_rows.append(row)\n",
    "\n",
    "# =================== LIVE NATION HK ===================\n",
    "def find_all_events_section():\n",
    "    try:\n",
    "        return WebDriverWait(driver, 12).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//*[self::h2 or self::h3][contains(.,'All Events')]/ancestor::section[1]\"))\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        return WebDriverWait(driver, 12).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//*[@id='main']/div/div[6]/section/div/div/div[2]\"))\n",
    "        )\n",
    "\n",
    "def scroll_deep(times=8, pause=0.45):\n",
    "    for _ in range(times):\n",
    "        driver.execute_script(\"window.scrollBy(0, 1000);\")\n",
    "        time.sleep(pause)\n",
    "\n",
    "def scrape_livenation_page(label=\"LiveNationHK\"):\n",
    "    section = find_all_events_section()\n",
    "    cards = section.find_elements(By.XPATH, \".//li[.//a[@href]]\")\n",
    "    if not cards:\n",
    "        cards = section.find_elements(By.XPATH, \".//ul[1]/li | .//div[contains(@class,'Card')]\")\n",
    "    for li in cards:\n",
    "        link, title, location, date_text = \"\", \"\", \"Hong Kong\", \"\"\n",
    "        try:\n",
    "            a = li.find_element(By.XPATH, \".//a[@href]\")\n",
    "            link = a.get_attribute(\"href\") or \"\"\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        for xp in [\".//p[contains(@class,'title')][1]\", \".//p[1]\", \".//*[self::h3 or self::h2 or self::p][1]\"]:\n",
    "            try:\n",
    "                title = safe_text(li.find_element(By.XPATH, xp))\n",
    "                if title: break\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "        for xp in [\".//p[contains(@class,'location')]\", \".//p[2]\"]:\n",
    "            try:\n",
    "                l = safe_text(li.find_element(By.XPATH, xp))\n",
    "                if l: location = l; break\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "        for xp in [\".//time\", \".//p[contains(@class,'date')][1]\", \".//p[2]\"]:\n",
    "            try:\n",
    "                d = safe_text(li.find_element(By.XPATH, xp))\n",
    "                if d and d.lower() != location.lower():\n",
    "                    date_text = d; break\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "        append_row(label, title, date_text, location, link)\n",
    "\n",
    "def run_livenation():\n",
    "    driver.get(LIVE_HOME)\n",
    "    accept_cookies(driver)\n",
    "    scroll_deep(8, 0.45)\n",
    "    scrape_livenation_page(\"LiveNationHK\")\n",
    "    # Try \"Page 2\"\n",
    "    try:\n",
    "        nxt = WebDriverWait(driver, 6).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//*[@id='main']/div/div[6]//nav//li[3]/button\"))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].click();\", nxt)\n",
    "        time.sleep(1.0)\n",
    "        scroll_deep(8, 0.45)\n",
    "        scrape_livenation_page(\"LiveNationHK\")\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "\n",
    "# =================== RUN ===================\n",
    "try:\n",
    "    run_livenation()\n",
    "finally:\n",
    "    try:\n",
    "        driver.quit()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# =================== SAVE (append-only) ===================\n",
    "new_df = pd.DataFrame(all_rows, dtype=str).reset_index(drop=True)\n",
    "\n",
    "if os.path.exists(OUTPUT_CSV) and old_df is not None:\n",
    "    if not new_df.empty:\n",
    "        new_df[\"__id\"] = new_df.apply(\n",
    "            lambda r: make_event_id(r.get(\"Platform\",\"\"), r.get(\"Link\",\"\"), r.get(\"Title\",\"\"), r.get(\"Date\",\"\")),\n",
    "            axis=1\n",
    "        )\n",
    "        new_df = new_df[~new_df[\"__id\"].isin(seen_ids_persisted)].drop(columns=\"__id\")\n",
    "    for col in column_order:\n",
    "        if col not in new_df.columns:\n",
    "            new_df[col] = \"\"\n",
    "    new_df = new_df[column_order]\n",
    "    final_df = pd.concat([old_df, new_df], ignore_index=True)\n",
    "else:\n",
    "    for col in column_order:\n",
    "        if col not in new_df.columns:\n",
    "            new_df[col] = \"\"\n",
    "    new_df = new_df[column_order]\n",
    "    final_df = new_df\n",
    "\n",
    "# Fill missing TimeScraped for older rows\n",
    "if \"TimeScraped\" in final_df.columns:\n",
    "    final_df[\"TimeScraped\"] = final_df[\"TimeScraped\"].replace(\"\", pd.NA).fillna(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "added = len(new_df)\n",
    "print(f\"\\nâœ… New rows appended this run: {added}\")\n",
    "print(f\"ğŸ“¦ Total rows in file: {len(final_df)}\")\n",
    "print(tabulate(final_df.tail(max(1, min(added, 20))), headers=\"keys\", tablefmt=\"github\", showindex=False))\n",
    "\n",
    "tmp = OUTPUT_CSV + \".tmp\"\n",
    "final_df.to_csv(tmp, index=False, encoding=\"utf-8-sig\")\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    try:\n",
    "        shutil.copy2(OUTPUT_CSV, BACKUP_CSV)\n",
    "    except Exception:\n",
    "        pass\n",
    "os.replace(tmp, OUTPUT_CSV)\n",
    "print(f\"\\nğŸ’¾ CSV updated: {OUTPUT_CSV}\")\n",
    "if os.path.exists(BACKUP_CSV):\n",
    "    print(f\"ğŸ§¯ Backup saved: {BACKUP_CSV}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "247965ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  Timeout while loading or scraping: Message: \n",
      "\n",
      "âš  Saved debug artifacts: fatal_timeout_screenshot.png, fatal_timeout_source.html\n",
      "\n",
      "â„¹ No new events scraped this run.\n",
      "\n",
      "âœ… TOTAL Unique Events: 25\n",
      "| Platform     | Title                                                                            | Date                                                       | Location                                                                            | Link                                                                                                                                                            |\n",
      "|--------------|----------------------------------------------------------------------------------|------------------------------------------------------------|-------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| LiveNationHK | bbno$ : it's pronounced baby no money                                            | THU 13 NOV                                                 | Hong Kong | TIDES                                                                   | https://www.livenation.hk/en/event/bbno-it-s-pronounced-baby-no-money-hong-kong-tickets-edp1621730                                                              |\n",
      "| LiveNationHK | TENBLANK from \"Glass Heart\" FAN MEETING - ASIA TOUR feat. Takeru Satoh(ä½è—¤ å¥ï¼‰ | WED 19 NOV                                                 | Hong Kong | AsiaWorld-Expo, Hall 10                                                 | https://www.livenation.hk/en/event/tenblank-from-glass-heart-fan-meeting-asia-tour-feat-takeru-satoh-%E4%BD%90%E8%97%A4-%E5%81%A5--hong-kong-tickets-edp1617522 |\n",
      "| LiveNationHK | TWICE <THIS IS FOR> WORLD TOUR IN HONG KONG                                      | SAT 06 DEC                                                 | Hong Kong | Kai Tak Stadium                                                         | https://www.livenation.hk/en/event/twice-this-is-for-world-tour-in-hong-kong-hong-kong-tickets-edp1619693                                                       |\n",
      "| LiveNationHK | TWICE <THIS IS FOR> WORLD TOUR IN HONG KONG                                      | SUN 07 DEC                                                 | Hong Kong | Kai Tak Stadium                                                         | https://www.livenation.hk/en/event/twice-this-is-for-world-tour-in-hong-kong-hong-kong-tickets-edp1621711                                                       |\n",
      "| LiveNationHK | Ali Abdaal - How to Make 2026 The Best Year of Your Life                         | MON 08 DEC                                                 | Hong Kong | Hong Kong Jockey Club Amphitheatre, HKAPA                               | https://www.livenation.hk/en/event/ali-abdaal-how-to-make-2026-the-best-year-of-your-life-hong-kong-tickets-edp1625852                                          |\n",
      "| LiveNationHK | Fly By Midnight - The Fastest Time Of Our Lives Tour                             | TUE 13 JAN                                                 | Hong Kong | Kitty Woo Stadium, Tung Po                                              | https://www.livenation.hk/en/event/fly-by-midnight-the-fastest-time-of-our-lives-tour-hong-kong-tickets-edp1621734                                              |\n",
      "| LiveNationHK | DAY6 10th Anniversary Tour <The DECADE> in HONG KONG                             | SAT 17 JAN                                                 | Hong Kong | AsiaWorld-Arena                                                         | https://www.livenation.hk/en/event/day6-10th-anniversary-tour-the-decade-in-hong-kong-hong-kong-tickets-edp1626774                                              |\n",
      "| LiveNationHK | BLACKPINK WORLD TOUR <DEADLINE> IN HONG KONG                                     | SAT 24 JAN                                                 | Hong Kong | Kai Tak Stadium                                                         | https://www.livenation.hk/en/event/blackpink-world-tour-deadline-in-hong-kong-hong-kong-tickets-edp1601705                                                      |\n",
      "| LiveNationHK | BLACKPINK WORLD TOUR <DEADLINE> IN HONG KONG                                     | SUN 25 JAN                                                 | Hong Kong | Kai Tak Stadium                                                         | https://www.livenation.hk/en/event/blackpink-world-tour-deadline-in-hong-kong-hong-kong-tickets-edp1601721                                                      |\n",
      "| LiveNationHK | BLACKPINK WORLD TOUR <DEADLINE> IN HONG KONG                                     | MON 26 JAN                                                 | Hong Kong | Kai Tak Stadium                                                         | https://www.livenation.hk/en/event/blackpink-world-tour-deadline-in-hong-kong-hong-kong-tickets-edp1620999                                                      |\n",
      "| LiveNationHK | 2025-26 aespa LIVE TOUR - SYNK: aeXIS LINE - in HONG KONG                        | SAT 07 FEB                                                 | Hong Kong | AsiaWorld-Arena                                                         | https://www.livenation.hk/en/event/2025-26-aespa-live-tour-synk-aexis-line-in-hong-kong-hong-kong-tickets-edp1620467                                            |\n",
      "| LiveNationHK | 2025-26 aespa LIVE TOUR - SYNK: aeXIS LINE - in HONG KONG                        | SUN 08 FEB                                                 | Hong Kong | AsiaWorld-Arena                                                         | https://www.livenation.hk/en/event/2025-26-aespa-live-tour-synk-aexis-line-in-hong-kong-hong-kong-tickets-edp1620483                                            |\n",
      "| LiveNationHK | ONEREPUBLIC â€œFrom Asia, With Loveâ€ 2026                                          | SAT 21 FEB                                                 | Hong Kong | AsiaWorld-Arena                                                         | https://www.livenation.hk/en/event/onerepublic-from-asia-with-love-2026-hong-kong-tickets-edp1626588                                                            |\n",
      "| HKTicketing  | ATTACK ON TITAN - Beyond the Walls World Tour - The Official Concert             | Displaying results: 1-3 of 3                               | HATSUNE MIKU EXPO 2025 in Hong Kong                                                 | https://premier.hkticketing.com/shows/show.aspx?sh=ATTAC1125                                                                                                    |\n",
      "| HKTicketing  | HK TICKETING - Calum Scott The Avenoir Tour 2026 in Hong Kong                    | Date:\n",
      "Thu 05 Feb 2026 8:00PM                                                            | Venue:\n",
      "Runway 11, AsiaWorld-Expo, Lantau                                                                                     | https://premier.hkticketing.com/shows/show.aspx?sh=CALUM0226                                                                                                    |\n",
      "| HKTicketing  | HK TICKETING - HATSUNE MIKU EXPO 2025 in Hong Kong                               | Date:\n",
      "    ---- Select Date ----\n",
      "       \n",
      "      Sat 8 Nov 2025 8:00pm - ä¼ä½(ä¸è¨­åŠƒä½)Free Standing\n",
      "    \n",
      "       \n",
      "      Sat 8 Nov 2025 8:00pm - æŒ‡å®šåº§ä½/Marked Seating                                                            | Venue:\n",
      "AsiaWorld-Arena, AsiaWorld-Expo, Lantau                                                                                     | https://premier.hkticketing.com/shows/show.aspx?sh=HATSU1125                                                                                                    |\n",
      "| HKTicketing  | HK TICKETING - Disney On Ice presents Magic In the Stars                         | Date:\n",
      "    ---- Select Date ----\n",
      "       \n",
      "      Fri 23 Jan 2026 3:30PM\n",
      "    \n",
      "       \n",
      "      Fri 23 Jan 2026 7:30PM\n",
      "    \n",
      "       \n",
      "      Sat 24 Jan 2026 11:30AM\n",
      "    \n",
      "       \n",
      "      Sat 24 Jan 2026 3:30PM\n",
      "    \n",
      "       \n",
      "      Sat 24 Jan 2026 7:30PM\n",
      "    \n",
      "       \n",
      "      Sun 25 Jan 2026 11:30AM\n",
      "    \n",
      "       \n",
      "      Sun 25 Jan 2026 3:30PM\n",
      "    \n",
      "       \n",
      "      Sun 25 Jan 2026 7:30PM                                                            | Venue:\n",
      "Kai Tak Arena, Kai Tak Sports Park, Kowloon                                                                                     | https://premier.hkticketing.com/shows/show.aspx?sh=DISNE0126                                                                                                    |\n",
      "| HKTicketing  | TRAVIS SCOTT CIRCUS MAXIMUS 2025 æ¾³é—¨ç«™                                          | Â· A maximum of 4 tickets can be purchased per Access Code. | 8 Oct 2025 (Wed) from 12:00 PM to 11:59 PM, or until pre-sale tickets are sold out. | https://hkt.hkticketing.com/en/#/allEvents/detail?projectId=50000000795004&noredirect=true                                                                      |\n",
      "| LiveNationHK | DAY6 10th Anniversary Tour <The DECADE> in HONG KONG                             | SUN 18 JAN                                                 | DAY6                                                                                | https://www.livenation.hk/en/event/day6-10th-anniversary-tour-the-decade-in-hong-kong-hong-kong-tickets-edp1630128                                              |\n",
      "| LiveNationHK | Blue 25th Anniversary Tour                                                       | SAT 07 FEB                                                 | Blue                                                                                | https://www.livenation.hk/en/event/blue-25th-anniversary-tour-hong-kong-tickets-edp1630645                                                                      |\n",
      "\n",
      "ğŸ’¾ CSV saved: all_hong_kong.csv\n"
     ]
    }
   ],
   "source": [
    "# --- HKTicketing Scraper (XPath-only; no fallbacks) ---\n",
    "# Site: https://premier.hkticketing.com/\n",
    "# Output CSV: all_hong_kong.csv (Platform, Title, Date, Location, Link)\n",
    "# pip install selenium tabulate pandas\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import time, os, math, sys\n",
    "\n",
    "HOME = \"https://premier.hkticketing.com/\"\n",
    "OUTPUT_CSV = \"all_hong_kong.csv\"\n",
    "\n",
    "# Tunables\n",
    "WAIT = 25                 # explicit wait seconds\n",
    "PAGELOAD_TIMEOUT = 60     # page load timeout seconds\n",
    "SCRIPT_TIMEOUT = 60\n",
    "NAV_RETRIES = 4           # number of times to retry loading homepage\n",
    "\n",
    "# Load previous results (for de-dup by Link)\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    try:\n",
    "        existing_df = pd.read_csv(OUTPUT_CSV)\n",
    "        if \"Link\" in existing_df.columns:\n",
    "            seen_links = set(existing_df[\"Link\"].dropna().astype(str))\n",
    "        else:\n",
    "            seen_links = set()\n",
    "    except Exception:\n",
    "        existing_df = pd.DataFrame(columns=[\"Platform\", \"Title\", \"Date\", \"Location\", \"Link\"])\n",
    "        seen_links = set()\n",
    "else:\n",
    "    existing_df = pd.DataFrame(columns=[\"Platform\", \"Title\", \"Date\", \"Location\", \"Link\"])\n",
    "    seen_links = set()\n",
    "\n",
    "# ------------- Driver -------------\n",
    "opts = webdriver.ChromeOptions()\n",
    "opts.add_argument(\"--start-maximized\")\n",
    "opts.add_argument(\"--disable-notifications\")\n",
    "opts.add_argument(\"--disable-popup-blocking\")\n",
    "opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "# If you want headless, uncomment:\n",
    "# opts.add_argument(\"--headless=new\")\n",
    "# opts.add_argument(\"--window-size=1440,1000\")\n",
    "\n",
    "driver = webdriver.Chrome(options=opts)\n",
    "driver.set_window_size(1400, 1000)\n",
    "driver.set_page_load_timeout(PAGELOAD_TIMEOUT)\n",
    "driver.set_script_timeout(SCRIPT_TIMEOUT)\n",
    "actions = ActionChains(driver)\n",
    "\n",
    "def save_debug(prefix: str):\n",
    "    try:\n",
    "        png = f\"{prefix}_screenshot.png\"\n",
    "        html = f\"{prefix}_source.html\"\n",
    "        driver.save_screenshot(png)\n",
    "        with open(html, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(driver.page_source)\n",
    "        print(f\"âš  Saved debug artifacts: {png}, {html}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def js_ready() -> bool:\n",
    "    try:\n",
    "        return driver.execute_script(\"return document.readyState\") == \"complete\"\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def wait_for_home_ready(timeout=WAIT):\n",
    "    \"\"\"Wait until the homepage is fully ready:\n",
    "       - document.readyState == 'complete'\n",
    "       - and one of the key sections exists\n",
    "    \"\"\"\n",
    "    end = time.time() + timeout\n",
    "    while time.time() < end:\n",
    "        if js_ready():\n",
    "            try:\n",
    "                # Either 'More Events' or 'Hero' nav visible is sufficient\n",
    "                if driver.find_elements(By.XPATH, '//*[@id=\"moreEventsSection\"]') or \\\n",
    "                   driver.find_elements(By.XPATH, '//*[@id=\"heroNavPaging\"]'):\n",
    "                    return True\n",
    "            except Exception:\n",
    "                pass\n",
    "        time.sleep(0.3)\n",
    "    return False\n",
    "\n",
    "def get_with_retries(url: str, attempts=NAV_RETRIES) -> None:\n",
    "    \"\"\"Robust navigation with retries + backoff.\"\"\"\n",
    "    last_err = None\n",
    "    for i in range(1, attempts + 1):\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            # Tiny scroll to nudge lazy content\n",
    "            driver.execute_script(\"window.scrollTo(0, 50);\")\n",
    "            time.sleep(0.3)\n",
    "            if wait_for_home_ready(timeout=WAIT + 5):\n",
    "                return\n",
    "            else:\n",
    "                raise TimeoutException(\"Home not ready within wait window.\")\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            save_debug(f\"load_attempt_{i}\")\n",
    "            # Backoff\n",
    "            sleep_s = min(2 * i, 8)\n",
    "            print(f\"âš  Load attempt {i}/{attempts} failed: {e}. Retrying in {sleep_s}s...\")\n",
    "            time.sleep(sleep_s)\n",
    "    # If we get here, all attempts failed\n",
    "    raise TimeoutException(f\"Failed to load {url} after {attempts} attempts: {last_err}\")\n",
    "\n",
    "def safe_get_text(by, xp, timeout=WAIT):\n",
    "    el = WebDriverWait(driver, timeout).until(EC.presence_of_element_located((by, xp)))\n",
    "    return el.text.strip()\n",
    "\n",
    "def safe_click(by, xp, timeout=WAIT):\n",
    "    el = WebDriverWait(driver, timeout).until(EC.element_to_be_clickable((by, xp)))\n",
    "    # bring into view + small pause to ensure itâ€™s interactable\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", el)\n",
    "    time.sleep(0.15)\n",
    "    driver.execute_script(\"arguments[0].click();\", el)\n",
    "\n",
    "rows = []\n",
    "\n",
    "try:\n",
    "    # Open homepage robustly\n",
    "    get_with_retries(HOME, attempts=NAV_RETRIES)\n",
    "\n",
    "    # -----------------------------\n",
    "    # (1) MORE EVENTS section loop\n",
    "    # -----------------------------\n",
    "    idx = 1\n",
    "    while True:\n",
    "        tile_xp = f'//*[@id=\"moreEventsSection\"]/div[{idx}]/div[2]/a'\n",
    "        title_click_xp = f'//*[@id=\"moreEventsSection\"]/div[{idx}]/div[2]/a/div[2]/strong'\n",
    "\n",
    "        try:\n",
    "            tile = WebDriverWait(driver, 4).until(EC.presence_of_element_located((By.XPATH, tile_xp)))\n",
    "        except TimeoutException:\n",
    "            # No more tiles at this index â†’ end loop\n",
    "            break\n",
    "\n",
    "        link = tile.get_attribute(\"href\") or \"\"\n",
    "\n",
    "        # Click the <strong> element (as requested)\n",
    "        safe_click(By.XPATH, title_click_xp)\n",
    "        # allow any dynamic panels to render\n",
    "        time.sleep(0.6)\n",
    "\n",
    "        # Venue & Date (strict XPaths)\n",
    "        venue_xp = '//*[@id=\"ctl00_ctl00_uiBodyMain_uiBodyRight_uiPerfSelector_uiPerfSelectorUpdatePanel\"]/div[2]/div[3]/p[1]'\n",
    "        date_xp  = '//*[@id=\"ctl00_ctl00_uiBodyMain_uiBodyRight_uiPerfSelector_uiPerfSelectorUpdatePanel\"]/div[2]/div[3]/p[2]'\n",
    "\n",
    "        try:\n",
    "            venue = safe_get_text(By.XPATH, venue_xp, timeout=WAIT)\n",
    "        except Exception:\n",
    "            venue = \"\"\n",
    "\n",
    "        try:\n",
    "            date_time = safe_get_text(By.XPATH, date_xp, timeout=WAIT)\n",
    "        except Exception:\n",
    "            date_time = \"\"\n",
    "\n",
    "        # Title: use page <title> (no alternative selectors)\n",
    "        try:\n",
    "            page_title = driver.title.strip()\n",
    "        except Exception:\n",
    "            page_title = \"\"\n",
    "        title_val = page_title if page_title else f\"Event {idx}\"\n",
    "\n",
    "        if not link:\n",
    "            try:\n",
    "                link = driver.current_url\n",
    "            except Exception:\n",
    "                link = \"\"\n",
    "\n",
    "        if link and link not in seen_links:\n",
    "            rows.append({\n",
    "                \"Platform\": \"HKTicketing\",\n",
    "                \"Title\": title_val,\n",
    "                \"Date\": date_time,      # map DateTime -> Date (no parsing, keep as-is)\n",
    "                \"Location\": venue,      # map Venue -> Location\n",
    "                \"Link\": link\n",
    "            })\n",
    "            seen_links.add(link)\n",
    "\n",
    "        # Back to homepage and re-wait for readiness (prevents half-rendered state)\n",
    "        driver.back()\n",
    "        if not wait_for_home_ready(timeout=WAIT + 5):\n",
    "            # If coming back lands on a stale state, reload home robustly\n",
    "            print(\"â„¹ Re-loading home after back navigation...\")\n",
    "            get_with_retries(HOME, attempts=2)\n",
    "        time.sleep(0.4)\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "    # -----------------------------\n",
    "    # (2) HERO flow (exact steps)\n",
    "    # -----------------------------\n",
    "    safe_click(By.XPATH, '//*[@id=\"heroNavPaging\"]/a[1]')\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    hero_name = safe_get_text(By.XPATH, '//*[@id=\"heroModuleInner\"]/div[2]/div[2]/div[1]/h2/a')\n",
    "\n",
    "    safe_click(By.XPATH, '//*[@id=\"heroModuleInner\"]/div[3]/div[2]/div[2]/a')\n",
    "    # wait for the app container to render its details\n",
    "    WebDriverWait(driver, WAIT).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//*[@id=\"app-scroll-container\"]'))\n",
    "    )\n",
    "    time.sleep(0.6)\n",
    "\n",
    "    hero_venue = safe_get_text(By.XPATH, '//*[@id=\"app-scroll-container\"]/div[1]/div[2]/div/div[2]/div/div/div[2]/div[2]/p[5]')\n",
    "    hero_datetime = safe_get_text(By.XPATH, '//*[@id=\"app-scroll-container\"]/div[1]/div[2]/div/div[2]/div/div/div[2]/div[2]/p[6]')\n",
    "\n",
    "    try:\n",
    "        hero_link = driver.current_url\n",
    "    except Exception:\n",
    "        hero_link = \"\"\n",
    "\n",
    "    if hero_link and hero_link not in seen_links:\n",
    "        rows.append({\n",
    "            \"Platform\": \"HKTicketing\",\n",
    "            \"Title\": hero_name,\n",
    "            \"Date\": hero_datetime,\n",
    "            \"Location\": hero_venue,\n",
    "            \"Link\": hero_link\n",
    "        })\n",
    "        seen_links.add(hero_link)\n",
    "\n",
    "except TimeoutException as e:\n",
    "    print(f\"âš  Timeout while loading or scraping: {e}\")\n",
    "    save_debug(\"fatal_timeout\")\n",
    "except NoSuchElementException as e:\n",
    "    print(f\"âš  Missing element via provided XPath: {e}\")\n",
    "    save_debug(\"fatal_no_such_element\")\n",
    "except Exception as e:\n",
    "    print(f\"âš  Unexpected error: {e}\")\n",
    "    save_debug(\"fatal_error\")\n",
    "finally:\n",
    "    try:\n",
    "        driver.quit()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ---------------- OUTPUT ----------------\n",
    "if rows:\n",
    "    new_df = pd.DataFrame(rows, columns=[\"Platform\", \"Title\", \"Date\", \"Location\", \"Link\"])\n",
    "    # Align previous dataframe to new schema if needed\n",
    "    missing_cols = [c for c in [\"Platform\", \"Title\", \"Date\", \"Location\", \"Link\"] if c not in existing_df.columns]\n",
    "    if missing_cols:\n",
    "        for c in missing_cols:\n",
    "            existing_df[c] = \"\"  # fill missing\n",
    "        existing_df = existing_df[[\"Platform\", \"Title\", \"Date\", \"Location\", \"Link\"]]\n",
    "    final_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "else:\n",
    "    # Align schema even if no new rows\n",
    "    if set(existing_df.columns) != set([\"Platform\", \"Title\", \"Date\", \"Location\", \"Link\"]):\n",
    "        aligned = pd.DataFrame(columns=[\"Platform\", \"Title\", \"Date\", \"Location\", \"Link\"])\n",
    "        for c in [\"Platform\", \"Title\", \"Date\", \"Location\", \"Link\"]:\n",
    "            if c in existing_df.columns:\n",
    "                aligned[c] = existing_df[c]\n",
    "            else:\n",
    "                aligned[c] = \"\"\n",
    "        final_df = aligned\n",
    "    else:\n",
    "        final_df = existing_df.copy()\n",
    "\n",
    "if not final_df.empty:\n",
    "    final_df[\"Link\"] = final_df[\"Link\"].astype(str).str.strip()\n",
    "    final_df = final_df.drop_duplicates(subset=[\"Link\"], keep=\"first\")\n",
    "\n",
    "# ------- NEW: print if new events were added and list them -------\n",
    "try:\n",
    "    existing_links_before = set(existing_df[\"Link\"].astype(str).str.strip())\n",
    "except Exception:\n",
    "    existing_links_before = set()\n",
    "\n",
    "if rows:\n",
    "    # new_df contains only rows scraped this run; detect which are actually new vs. pre-existing\n",
    "    new_mask = ~new_df[\"Link\"].astype(str).str.strip().isin(existing_links_before)\n",
    "    actually_new = new_df[new_mask].copy()\n",
    "    added_count = len(actually_new)\n",
    "    if added_count > 0:\n",
    "        print(f\"\\nğŸ†• Added {added_count} new event(s):\")\n",
    "        for _, r in actually_new.iterrows():\n",
    "            print(f\"  â€¢ {r['Title']} | {r['Date']} | {r['Location']} | {r['Link']}\")\n",
    "    else:\n",
    "        print(\"\\nâ„¹ No new events added (everything scraped already existed).\")\n",
    "else:\n",
    "    print(\"\\nâ„¹ No new events scraped this run.\")\n",
    "# ------- END NEW -------\n",
    "\n",
    "print(f\"\\nâœ… TOTAL Unique Events: {len(final_df)}\")\n",
    "print(tabulate(final_df.tail(20), headers=\"keys\", tablefmt=\"github\", showindex=False))\n",
    "\n",
    "final_df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\nğŸ’¾ CSV saved: {OUTPUT_CSV}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438694ca",
   "metadata": {},
   "source": [
    "# Jarkata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b7d7054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ¢ Found 7 featured cards\n",
      "âœ… Megatix Indonesia (Featured): grabbed 7 events\n",
      "\n",
      "â„¹ No new events added since last run.\n",
      "\n",
      "âœ… TOTAL Raw Events: 7\n",
      "| Title                                       | Date             | Location          | Link                                                       |\n",
      "|---------------------------------------------|------------------|-------------------|------------------------------------------------------------|\n",
      "| X-Clusive Presents: Bali NYE 2025 with Tyga | Wed, 31 Dec 2025 | The Stage         | https://megatix.co.id/events/nye-2025-with-tyga            |\n",
      "| X-Clusive Presents: Bali NYE 2025 with Tyga | Wed, 31 Dec 2025 | The Stage         | https://megatix.co.id/events/nye-2025-with-tyga            |\n",
      "| HAYDEN JAMES                                | Sat, 08 Nov 2025 | Savaya Bali       | https://megatix.co.id/events/hayden-james-0811             |\n",
      "| DEFECTED                                    | Sun, 09 Nov 2025 | Savaya Bali       | https://megatix.co.id/events/defected-0911                 |\n",
      "| JUNCTION: AFRICA VOL.6                      | Fri, 31 Oct 2025 | The Mesa Bali     | https://megatix.co.id/events/junction-africa-vol6          |\n",
      "| NYE 2025 FEATURING DASH BERLIN              | Wed, 31 Dec 2025 | CafÃ© del Mar Bali | https://megatix.co.id/events/NYE-2025-FEATURING-DASHBERLIN |\n",
      "| NEW YEAR'S EVE PARTY AT W BALI - SEMINYAK   | Wed, 31 Dec 2025 | W Bali - Seminyak | https://megatix.co.id/events/NYE-W-BALI-SEMINYAK           |\n",
      "\n",
      "ğŸ’¾ CSV saved: all_jakarta.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Megatix Indonesia: Featured Events Scraper ---\n",
    "# Output CSV: all_jakarta.csv\n",
    "# Works for https://megatix.co.id/\n",
    "# pip install selenium tabulate pandas\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import time\n",
    "import os  # <-- added to read existing CSV for \"new events\" comparison\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "OUTPUT_CSV = \"all_jakarta.csv\"\n",
    "URL = \"https://megatix.co.id/\"\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.set_window_size(1400, 1000)\n",
    "all_rows = []\n",
    "\n",
    "# ---------------- SCRAPER ----------------\n",
    "try:\n",
    "    driver.get(URL)\n",
    "\n",
    "    # Wait for base element\n",
    "    WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='__nuxt']\")))\n",
    "    time.sleep(1.0)\n",
    "\n",
    "    FEATURED_WRAP_X = \"//*[@id='__nuxt']/div/div[3]/div/main/section/section[2]/div[2]/div\"\n",
    "    FEATURED_CARD_X = \"//*[@id='__nuxt']/div/div[3]/div/main/section/section[2]/div[2]//article[.//h3]\"\n",
    "\n",
    "    # Scroll and load all featured cards\n",
    "    prev = -1\n",
    "    stable = 0\n",
    "    for _ in range(60):\n",
    "        cards = driver.find_elements(By.XPATH, FEATURED_CARD_X)\n",
    "        count = len(cards)\n",
    "        if count == prev:\n",
    "            stable += 1\n",
    "            if stable >= 2:\n",
    "                break\n",
    "        else:\n",
    "            stable = 0\n",
    "        prev = count\n",
    "        driver.execute_script(\"window.scrollBy(0, 900);\")\n",
    "        time.sleep(1.2)\n",
    "\n",
    "    print(f\"ğŸŸ¢ Found {len(driver.find_elements(By.XPATH, FEATURED_CARD_X))} featured cards\")\n",
    "\n",
    "    for ev in driver.find_elements(By.XPATH, FEATURED_CARD_X):\n",
    "        try:\n",
    "            title = ev.find_element(By.XPATH, \".//h3/span\").text.strip()\n",
    "        except:\n",
    "            title = \"No Title\"\n",
    "        try:\n",
    "            date = ev.find_element(By.XPATH, \".//div[1]/div[1]/span\").text.strip()\n",
    "        except:\n",
    "            date = \"No Date\"\n",
    "        try:\n",
    "            location = ev.find_element(By.XPATH, \".//div[1]/div[3]/span\").text.strip()\n",
    "        except:\n",
    "            location = \"Indonesia\"\n",
    "        try:\n",
    "            link = ev.find_element(By.XPATH, \".//ancestor::a\").get_attribute(\"href\") or \"No Link\"\n",
    "        except:\n",
    "            link = \"No Link\"\n",
    "\n",
    "        all_rows.append({\n",
    "            \"Title\": title,\n",
    "            \"Date\": date,\n",
    "            \"Location\": location,\n",
    "            \"Link\": link\n",
    "        })\n",
    "\n",
    "    print(f\"âœ… Megatix Indonesia (Featured): grabbed {len(all_rows)} events\")\n",
    "\n",
    "except TimeoutException:\n",
    "    print(\"âš  Timeout waiting for Megatix Indonesia page\")\n",
    "except Exception as e:\n",
    "    print(f\"âš  Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "# ---------------- OUTPUT ----------------\n",
    "df = pd.DataFrame(all_rows).reset_index(drop=True)\n",
    "\n",
    "# --- NEW: Compare with existing CSV and print any newly added events ---\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    try:\n",
    "        old_df = pd.read_csv(OUTPUT_CSV)\n",
    "        old_links = set(old_df.get(\"Link\", pd.Series(dtype=str)).astype(str).str.strip())\n",
    "    except Exception:\n",
    "        old_links = set()\n",
    "else:\n",
    "    old_links = set()\n",
    "\n",
    "# Normalize links for comparison and detect new ones\n",
    "if not df.empty:\n",
    "    df[\"Link\"] = df[\"Link\"].astype(str).str.strip()\n",
    "    new_mask = ~df[\"Link\"].isin(old_links)\n",
    "    new_events = df[new_mask].copy()\n",
    "    if len(new_events) > 0:\n",
    "        print(f\"\\nğŸ†• {len(new_events)} new event(s) since last run:\")\n",
    "        for _, r in new_events.iterrows():\n",
    "            print(f\"  â€¢ {r['Title']} | {r['Date']} | {r['Location']} | {r['Link']}\")\n",
    "    else:\n",
    "        print(\"\\nâ„¹ No new events added since last run.\")\n",
    "else:\n",
    "    print(\"\\nâ„¹ No events scraped this run (empty listing).\")\n",
    "# --- END NEW ---\n",
    "\n",
    "print(f\"\\nâœ… TOTAL Raw Events: {len(df)}\")\n",
    "print(tabulate(df, headers=\"keys\", tablefmt=\"github\", showindex=False))\n",
    "\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\nğŸ’¾ CSV saved: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3321c2b6",
   "metadata": {},
   "source": [
    "# Bangkok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cc41d44",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutException\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 163\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m# Wait for name node to appear\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m wait(driver, NAME_X, sec=\u001b[32m20\u001b[39m)\n\u001b[32m    165\u001b[39m name  = t(driver, NAME_X)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 128\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(drv, xp, sec, clickable)\u001b[39m\n\u001b[32m    127\u001b[39m cond = EC.element_to_be_clickable \u001b[38;5;28;01mif\u001b[39;00m clickable \u001b[38;5;28;01melse\u001b[39;00m EC.presence_of_element_located\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m WebDriverWait(drv, sec).until(cond((By.XPATH, xp)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saiva\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:138\u001b[39m, in \u001b[36mWebDriverWait.until\u001b[39m\u001b[34m(self, method, message)\u001b[39m\n\u001b[32m    137\u001b[39m     time.sleep(\u001b[38;5;28mself\u001b[39m._poll)\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[31mTimeoutException\u001b[39m: Message: \nStacktrace:\n\tGetHandleVerifier [0x0x7ff6bbcfe8e5+80021]\n\tGetHandleVerifier [0x0x7ff6bbcfe940+80112]\n\t(No symbol) [0x0x7ff6bba8060f]\n\t(No symbol) [0x0x7ff6bbad8854]\n\t(No symbol) [0x0x7ff6bbad8b1c]\n\t(No symbol) [0x0x7ff6bbb2c927]\n\t(No symbol) [0x0x7ff6bbb0126f]\n\t(No symbol) [0x0x7ff6bbb2968a]\n\t(No symbol) [0x0x7ff6bbb01003]\n\t(No symbol) [0x0x7ff6bbac95d1]\n\t(No symbol) [0x0x7ff6bbaca3f3]\n\tGetHandleVerifier [0x0x7ff6bbfbdc7d+2960429]\n\tGetHandleVerifier [0x0x7ff6bbfb7f3a+2936554]\n\tGetHandleVerifier [0x0x7ff6bbfd8977+3070247]\n\tGetHandleVerifier [0x0x7ff6bbd183ce+185214]\n\tGetHandleVerifier [0x0x7ff6bbd1fe1f+216527]\n\tGetHandleVerifier [0x0x7ff6bbd07b24+117460]\n\tGetHandleVerifier [0x0x7ff6bbd07cdf+117903]\n\tGetHandleVerifier [0x0x7ff6bbcedbb8+11112]\n\tBaseThreadInitThunk [0x0x7ff8f535e8d7+23]\n\tRtlUserThreadStart [0x0x7ff8f71ac53c+44]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 204\u001b[39m\n\u001b[32m    201\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m: \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 192\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    191\u001b[39m     driver.get(HOME)\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     wait(driver, LIST_ANCHORS_X)\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutException:\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 128\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(drv, xp, sec, clickable)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwait\u001b[39m(drv, xp, sec=\u001b[32m25\u001b[39m, clickable=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    127\u001b[39m     cond = EC.element_to_be_clickable \u001b[38;5;28;01mif\u001b[39;00m clickable \u001b[38;5;28;01melse\u001b[39;00m EC.presence_of_element_located\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m WebDriverWait(drv, sec).until(cond((By.XPATH, xp)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saiva\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:137\u001b[39m, in \u001b[36mWebDriverWait.until\u001b[39m\u001b[34m(self, method, message)\u001b[39m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m time.monotonic() > end_time:\n\u001b[32m    136\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     time.sleep(\u001b[38;5;28mself\u001b[39m._poll)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# thai ticket major\n",
    "# pip install selenium pandas\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "HOME = \"https://www.thaiticketmajor.com/concert/\"\n",
    "\n",
    "# ---- Your detail-page XPaths (kept as-is) ----\n",
    "NAME_X  = \"/html/body/div[1]/main/div[1]/div[2]/div/div[2]/div/div[1]/div/h1/font/font\"\n",
    "VENUE_X = \"/html/body/div[1]/main/div[1]/div[2]/div/div[2]/div/div[2]/div[1]/ul/li[2]/p/span/font/font\"\n",
    "PRICE_X = \"/html/body/div[1]/main/div[1]/div[2]/div/div[2]/div/div[2]/div[2]/ul/li[2]/div\"\n",
    "\n",
    "# Listing: weâ€™ll collect all anchors in section[2] (your click path lives under here)\n",
    "# This is more reliable than clicking the nested /font/font node.\n",
    "LIST_ANCHORS_X = \"//body/div[1]/main/section[2]//a[@href]\"\n",
    "\n",
    "# ================== APPEND+DEDUP ADD-ON (no changes to your code below) ==================\n",
    "import atexit, os, re\n",
    "\n",
    "_OUTPUT_CSV_PATH = \"all_bangkok.csv\"\n",
    "\n",
    "def _normalize_link(u: str) -> str:\n",
    "    u = (u or \"\").strip().lower()\n",
    "    u = re.sub(r\"#.*$\", \"\", u)           # drop fragments\n",
    "    u = re.sub(r\"\\?.*$\", \"\", u)          # drop query params\n",
    "    return u.rstrip(\"/\")\n",
    "\n",
    "try:\n",
    "    _old_df_ttm = pd.read_csv(_OUTPUT_CSV_PATH)\n",
    "except Exception:\n",
    "    _old_df_ttm = pd.DataFrame()\n",
    "\n",
    "def _merge_back_ttm():\n",
    "    \"\"\"Runs AFTER your script's own to_csv.\n",
    "       - Restores previous rows\n",
    "       - Removes duplicates\n",
    "       - Prints which events are newly added this run (or none)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        new_df = pd.read_csv(_OUTPUT_CSV_PATH)\n",
    "    except Exception:\n",
    "        # If the fresh write failed, nothing to merge\n",
    "        return\n",
    "\n",
    "    # Ensure consistent columns even if earlier files differ\n",
    "    need_cols = [\"Event Name\", \"Venue\", \"Ticket Prices\", \"Link\", \"Scraped At (UTC)\"]\n",
    "    for df in (_old_df_ttm, new_df):\n",
    "        for c in need_cols:\n",
    "            if c not in df.columns:\n",
    "                df[c] = \"\"\n",
    "\n",
    "    # Normalize links for strong dedup\n",
    "    _old_df_ttm[\"LinkNorm\"] = _old_df_ttm[\"Link\"].map(_normalize_link)\n",
    "    new_df[\"LinkNorm\"]      = new_df[\"Link\"].map(_normalize_link)\n",
    "\n",
    "    # --- Identify which rows are NEW vs the old file (before merging) ---\n",
    "    old_keys = set(_old_df_ttm[\"LinkNorm\"].astype(str))\n",
    "    # For rows with empty LinkNorm, fallback to (Event Name, Venue)\n",
    "    if \"\" in old_keys:\n",
    "        old_fallback = set(zip(\n",
    "            _old_df_ttm[\"Event Name\"].astype(str).str.strip().str.lower(),\n",
    "            _old_df_ttm[\"Venue\"].astype(str).str.strip().str.lower()\n",
    "        ))\n",
    "    else:\n",
    "        old_fallback = set()\n",
    "\n",
    "    newly_added_rows = []\n",
    "    for _, r in new_df.iterrows():\n",
    "        ln = str(r.get(\"LinkNorm\", \"\") or \"\")\n",
    "        if ln:\n",
    "            if ln not in old_keys:\n",
    "                newly_added_rows.append(r)\n",
    "        else:\n",
    "            key = (\n",
    "                str(r.get(\"Event Name\",\"\")).strip().lower(),\n",
    "                str(r.get(\"Venue\",\"\")).strip().lower()\n",
    "            )\n",
    "            if key not in old_fallback:\n",
    "                newly_added_rows.append(r)\n",
    "\n",
    "    # Merge and dedup\n",
    "    merged = pd.concat([_old_df_ttm, new_df], ignore_index=True)\n",
    "\n",
    "    # Primary dedup by normalized link (keep first = preserve earliest row)\n",
    "    merged = merged.drop_duplicates(subset=[\"LinkNorm\"], keep=\"first\")\n",
    "\n",
    "    # Secondary guard when links are missing/unstable\n",
    "    merged = merged.drop_duplicates(subset=[\"Event Name\", \"Venue\"], keep=\"first\")\n",
    "\n",
    "    # Persist without helper column\n",
    "    merged = merged.drop(columns=[\"LinkNorm\"], errors=\"ignore\")\n",
    "    merged.to_csv(_OUTPUT_CSV_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # --- Print summary of new events ---\n",
    "    if newly_added_rows:\n",
    "        print(f\"\\nğŸ†• {len(newly_added_rows)} new event(s) added this run:\")\n",
    "        for r in newly_added_rows:\n",
    "            title = str(r.get(\"Event Name\",\"\")).strip()\n",
    "            venue = str(r.get(\"Venue\",\"\")).strip()\n",
    "            link  = str(r.get(\"Link\",\"\")).strip()\n",
    "            date  = str(r.get(\"Scraped At (UTC)\",\"\")).strip()\n",
    "            print(f\"  â€¢ {title} | {venue} | {link} | scraped {date}\")\n",
    "    else:\n",
    "        print(\"\\nâ„¹ No new events added this run.\")\n",
    "\n",
    "# Ensure merge runs after your script finishes writing the CSV\n",
    "atexit.register(_merge_back_ttm)\n",
    "# ================== END ADD-ON ==================\n",
    "\n",
    "def build_driver():\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    opts.add_argument(\"--start-maximized\")\n",
    "    opts.add_argument(\"--disable-notifications\")\n",
    "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    opts.page_load_strategy = \"eager\"\n",
    "    d = webdriver.Chrome(options=opts)\n",
    "    d.set_page_load_timeout(90)\n",
    "    return d\n",
    "\n",
    "def wait(drv, xp, sec=25, clickable=False):\n",
    "    cond = EC.element_to_be_clickable if clickable else EC.presence_of_element_located\n",
    "    return WebDriverWait(drv, sec).until(cond((By.XPATH, xp)))\n",
    "\n",
    "def t(drv, xp):\n",
    "    try:\n",
    "        el = drv.find_element(By.XPATH, xp)\n",
    "        # textContent is safer with nested <font> etc.\n",
    "        return \" \".join((el.get_attribute(\"textContent\") or \"\").split())\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def main():\n",
    "    driver = build_driver()\n",
    "    rows = []\n",
    "    try:\n",
    "        driver.get(HOME)\n",
    "        # Wait for listing anchors to exist\n",
    "        wait(driver, LIST_ANCHORS_X)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        anchors = driver.find_elements(By.XPATH, LIST_ANCHORS_X)\n",
    "        hrefs = []\n",
    "        for a in anchors:\n",
    "            href = a.get_attribute(\"href\")\n",
    "            # Keep only event-detail-like links (most are /event/... or /concert/...)\n",
    "            if href and \"thaiticketmajor.com\" in href and href not in hrefs:\n",
    "                hrefs.append(href)\n",
    "\n",
    "        if not hrefs:\n",
    "            print(\"No events found on the listing.\")\n",
    "            return\n",
    "\n",
    "        for i, href in enumerate(hrefs, 1):\n",
    "            try:\n",
    "                driver.get(href)\n",
    "                # Wait for name node to appear\n",
    "                wait(driver, NAME_X, sec=20)\n",
    "\n",
    "                name  = t(driver, NAME_X)\n",
    "                venue = t(driver, VENUE_X)\n",
    "                price = t(driver, PRICE_X)\n",
    "\n",
    "                rows.append({\n",
    "                    \"Event Name\": name,\n",
    "                    \"Venue\": venue,\n",
    "                    \"Ticket Prices\": price,\n",
    "                    \"Link\": href,\n",
    "                    \"Scraped At (UTC)\": datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                })\n",
    "\n",
    "                # If you strictly want to â€œgo back and click the restâ€, uncomment these two lines:\n",
    "                # driver.back()\n",
    "                # wait(driver, LIST_ANCHORS_X)\n",
    "\n",
    "            except (TimeoutException, StaleElementReferenceException):\n",
    "                rows.append({\n",
    "                    \"Event Name\": \"\",\n",
    "                    \"Venue\": \"\",\n",
    "                    \"Ticket Prices\": \"\",\n",
    "                    \"Link\": href,\n",
    "                    \"Scraped At (UTC)\": datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                })\n",
    "                # Try to return to list if something failed\n",
    "                try:\n",
    "                    driver.get(HOME)\n",
    "                    wait(driver, LIST_ANCHORS_X)\n",
    "                except TimeoutException:\n",
    "                    pass\n",
    "\n",
    "        pd.DataFrame(rows).to_csv(\"all_bangkok.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Saved {len(rows)} rows to all_bangkok.csv\")\n",
    "\n",
    "    finally:\n",
    "        try: driver.quit()\n",
    "        except: pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c18524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ticketmelon (all asia)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e98151d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5 rows to all_bangkok.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "URL = \"https://www.livenationtero.co.th/en?utm_source=chatgpt.com\"\n",
    "\n",
    "LIST_X   = \"//*[@id='upcoming-shows']/div/ul\"\n",
    "ITEM_X   = LIST_X + \"/li[{i}]\"\n",
    "ANCHOR_X = ITEM_X + \"/a\"\n",
    "NAME_X   = ITEM_X + \"/a/p[1]\"      # event name\n",
    "DATE_X   = ITEM_X + \"/a/small\"     # event date  âœ…\n",
    "\n",
    "VENUE_X_PRIMARY = \"//*[@id='main']/div/div[1]/div/div[2]/div/p[4]\"\n",
    "VENUE_FALLBACKS = [\n",
    "    \"//*[@id='main']//p[contains(., 'Venue')]/following-sibling::p[1]\",\n",
    "    \"//*[@id='main']//div[contains(@class,'event') or contains(@class,'details')]//p[contains(@class,'venue')]\",\n",
    "    \"//*[contains(@class,'EventDetails')]//p[contains(@class,'venue')]\",\n",
    "]\n",
    "\n",
    "# ================== APPEND + DEDUP ADD-ON (ONLY ADDITIONS BELOW) ==================\n",
    "import os, re, atexit\n",
    "\n",
    "_OUTPUT_CSV_PATH = \"all_bangkok.csv\"\n",
    "\n",
    "def _normalize_link(u: str) -> str:\n",
    "    u = (u or \"\").strip().lower()\n",
    "    u = re.sub(r\"#.*$\", \"\", u)          # drop fragments\n",
    "    u = re.sub(r\"\\?.*$\", \"\", u)         # drop query params\n",
    "    return u.rstrip(\"/\")\n",
    "\n",
    "# Preload old CSV (if any) so we can merge it back after your own to_csv runs\n",
    "try:\n",
    "    _old_df_lnt = pd.read_csv(_OUTPUT_CSV_PATH)\n",
    "except Exception:\n",
    "    _old_df_lnt = pd.DataFrame()\n",
    "\n",
    "def _merge_back_lnt():\n",
    "    \"\"\"Runs AFTER your script writes all_bangkok.csv.\n",
    "       Merges old + new, removes duplicates, saves back to file,\n",
    "       and prints which events are newly added (or none).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        new_df = pd.read_csv(_OUTPUT_CSV_PATH)\n",
    "    except Exception:\n",
    "        return  # nothing written, nothing to merge\n",
    "\n",
    "    # Ensure the expected schema exists on both sides\n",
    "    need_cols = [\"Event Name\", \"Event Date\", \"Venue\", \"Link\", \"Scraped At (UTC)\"]\n",
    "    for df in (_old_df_lnt, new_df):\n",
    "        for c in need_cols:\n",
    "            if c not in df.columns:\n",
    "                df[c] = \"\"\n",
    "\n",
    "    # Normalize links for robust dedup (handles tracking params, trailing slashes)\n",
    "    _old_df_lnt[\"LinkNorm\"] = _old_df_lnt[\"Link\"].map(_normalize_link)\n",
    "    new_df[\"LinkNorm\"]      = new_df[\"Link\"].map(_normalize_link)\n",
    "\n",
    "    # ---- Determine which rows are NEW compared to old file ----\n",
    "    old_linknorms = set(_old_df_lnt[\"LinkNorm\"].astype(str))\n",
    "    # Fallback identity if Link is blank/unstable: (Event Name, Venue)\n",
    "    old_fallback_keys = set(zip(\n",
    "        _old_df_lnt[\"Event Name\"].astype(str).str.strip().str.lower(),\n",
    "        _old_df_lnt[\"Venue\"].astype(str).str.strip().str.lower()\n",
    "    ))\n",
    "\n",
    "    newly_added_rows = []\n",
    "    for _, r in new_df.iterrows():\n",
    "        ln = str(r.get(\"LinkNorm\", \"\") or \"\")\n",
    "        if ln:\n",
    "            if ln not in old_linknorms:\n",
    "                newly_added_rows.append(r)\n",
    "        else:\n",
    "            key = (\n",
    "                str(r.get(\"Event Name\",\"\")).strip().lower(),\n",
    "                str(r.get(\"Venue\",\"\")).strip().lower()\n",
    "            )\n",
    "            if key not in old_fallback_keys:\n",
    "                newly_added_rows.append(r)\n",
    "\n",
    "    # ---- Merge & deduplicate globally ----\n",
    "    merged = pd.concat([_old_df_lnt, new_df], ignore_index=True)\n",
    "    merged = merged.drop_duplicates(subset=[\"LinkNorm\"], keep=\"first\")\n",
    "    merged = merged.drop_duplicates(subset=[\"Event Name\", \"Venue\"], keep=\"first\")\n",
    "\n",
    "    # Persist without helper column\n",
    "    merged = merged.drop(columns=[\"LinkNorm\"], errors=\"ignore\")\n",
    "    merged.to_csv(_OUTPUT_CSV_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # ---- Print summary of new events (or none) ----\n",
    "    if newly_added_rows:\n",
    "        print(f\"\\nğŸ†• {len(newly_added_rows)} new event(s) added this run:\")\n",
    "        for r in newly_added_rows:\n",
    "            title = str(r.get(\"Event Name\",\"\")).strip()\n",
    "            date  = str(r.get(\"Event Date\",\"\")).strip()\n",
    "            venue = str(r.get(\"Venue\",\"\")).strip()\n",
    "            link  = str(r.get(\"Link\",\"\")).strip()\n",
    "            when  = str(r.get(\"Scraped At (UTC)\",\"\")).strip()\n",
    "            print(f\"  â€¢ {title} | {date} | {venue} | {link} | scraped {when}\")\n",
    "    else:\n",
    "        print(\"\\nâ„¹ No new events added this run.\")\n",
    "\n",
    "# Register the merge so it runs automatically at program exit\n",
    "atexit.register(_merge_back_lnt)\n",
    "# ================== END ADD-ON ==================\n",
    "\n",
    "def build_driver():\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    opts.add_argument(\"--start-maximized\")\n",
    "    opts.add_argument(\"--disable-notifications\")\n",
    "    return webdriver.Chrome(options=opts)\n",
    "\n",
    "def wait_xpath(drv, xpath, sec=25):\n",
    "    return WebDriverWait(drv, sec).until(EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "\n",
    "def wait_clickable(drv, xpath, sec=25):\n",
    "    return WebDriverWait(drv, sec).until(EC.element_to_be_clickable((By.XPATH, xpath)))\n",
    "\n",
    "def node_text(drv, xpath):\n",
    "    try:\n",
    "        el = drv.find_element(By.XPATH, xpath)\n",
    "        # textContent catches nested spans / line breaks better than .text sometimes\n",
    "        txt = el.get_attribute(\"textContent\") or \"\"\n",
    "        return \" \".join(txt.split())\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def get_venue_with_fallbacks(drv, timeout=8):\n",
    "    end = time.time() + timeout\n",
    "    try:\n",
    "        el = WebDriverWait(drv, min(4, timeout)).until(\n",
    "            EC.presence_of_element_located((By.XPATH, VENUE_X_PRIMARY))\n",
    "        )\n",
    "        txt = el.text.strip()\n",
    "        if txt: return txt\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "    for xp in VENUE_FALLBACKS:\n",
    "        if time.time() > end: break\n",
    "        try:\n",
    "            el = WebDriverWait(drv, max(1, int(end - time.time()))).until(\n",
    "                EC.presence_of_element_located((By.XPATH, xp))\n",
    "            )\n",
    "            txt = el.text.strip()\n",
    "            if txt: return txt\n",
    "        except TimeoutException:\n",
    "            continue\n",
    "    return \"\"\n",
    "\n",
    "def main():\n",
    "    driver = build_driver()\n",
    "    rows = []\n",
    "    try:\n",
    "        driver.get(URL)\n",
    "        wait_xpath(driver, LIST_X)\n",
    "        items = driver.find_elements(By.XPATH, LIST_X + \"/li\")\n",
    "        n = len(items)\n",
    "        if n == 0:\n",
    "            print(\"No events found under #upcoming-shows.\")\n",
    "            return\n",
    "\n",
    "        for i in range(1, n + 1):\n",
    "            try:\n",
    "                li = driver.find_element(By.XPATH, ITEM_X.format(i=i))\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", li)\n",
    "                time.sleep(0.2)\n",
    "\n",
    "                event_name = node_text(driver, NAME_X.format(i=i))\n",
    "                event_date = node_text(driver, DATE_X.format(i=i))   # âœ… now using <small>\n",
    "\n",
    "                a = wait_clickable(driver, ANCHOR_X.format(i=i))\n",
    "                href = a.get_attribute(\"href\")\n",
    "                a.click()\n",
    "\n",
    "                venue = get_venue_with_fallbacks(driver, timeout=8)\n",
    "\n",
    "                rows.append({\n",
    "                    \"Event Name\": event_name,\n",
    "                    \"Event Date\": event_date,\n",
    "                    \"Venue\": venue,\n",
    "                    \"Link\": href or driver.current_url,\n",
    "                    \"Scraped At (UTC)\": datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                })\n",
    "\n",
    "            except (TimeoutException, StaleElementReferenceException):\n",
    "                rows.append({\n",
    "                    \"Event Name\": event_name if 'event_name' in locals() else \"\",\n",
    "                    \"Event Date\": event_date if 'event_date' in locals() else \"\",\n",
    "                    \"Venue\": \"\",\n",
    "                    \"Link\": href if 'href' in locals() else \"\",\n",
    "                    \"Scraped At (UTC)\": datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                })\n",
    "            finally:\n",
    "                try:\n",
    "                    driver.back()\n",
    "                    wait_xpath(driver, LIST_X)\n",
    "                except TimeoutException:\n",
    "                    driver.get(URL)\n",
    "                    wait_xpath(driver, LIST_X)\n",
    "\n",
    "        if rows:\n",
    "            pd.DataFrame(rows).to_csv(\"all_bangkok.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "            print(f\"Saved {len(rows)} rows to all_bangkok.csv\")\n",
    "        else:\n",
    "            print(\"No rows collected.\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "# need to add code to show how many new events were added\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89ed574",
   "metadata": {},
   "source": [
    "# Australia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df2046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening: https://www.ticketmaster.com.au/browse/dance-electronic-catid-201/music-rid-10001#pageInfo?startDate=2025-10-27&endDate=2025-12-31\n",
      "Found 20 event cards (raw).\n",
      "De-duplicated: 20 -> 20\n",
      "\n",
      "Saved 20 total rows -> all_australia.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ticketmaster AU â€” Dance/Electronic (date-filtered) scraper\n",
    "- Source URL (example): https://www.ticketmaster.com.au/browse/dance-electronic-catid-201/music-rid-10001#pageInfo?startDate=2025-10-27&endDate=2025-12-31\n",
    "- Columns: Title, Date, Location, Link, Source, ScrapedAtUTC\n",
    "\n",
    "Usage (VS Code / terminal):\n",
    "    pip install selenium pandas\n",
    "    python scrape_ticketmaster_au_elec.py\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import sys\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    StaleElementReferenceException,\n",
    ")\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "START_DATE = \"2025-10-27\"\n",
    "END_DATE   = \"2025-12-31\"\n",
    "\n",
    "# Your provided (category) page + date filters.\n",
    "# NOTE: Put the \"#pageInfo\" anchor before the query string as per your link.\n",
    "BASE_URL = (\n",
    "    \"https://www.ticketmaster.com.au/browse/dance-electronic-catid-201/music-rid-10001\"\n",
    "    \"#pageInfo\"\n",
    "    f\"?startDate={START_DATE}&endDate={END_DATE}\"\n",
    ")\n",
    "\n",
    "OUTPUT_CSV = \"all_australia.csv\"\n",
    "WAIT_SEC   = 25\n",
    "\n",
    "# Your provided XPaths\n",
    "X_SECTION  = \"//*[@id='pageInfo']/div[1]/div/div[2]/div[2]/div[1]\"\n",
    "X_LIST     = X_SECTION + \"/div[2]/div[2]/ul\"\n",
    "X_ITEMS    = X_LIST + \"/li\"\n",
    "\n",
    "# Relative XPaths from each <li> item (based on your example for li[1])\n",
    "X_NAME_REL      = \".//div[1]/div/div[2]/div[2]/span[1]/span\"\n",
    "X_LOCATION_REL  = \".//div[1]/div/div[2]/div[2]/span[2]/span[2]\"\n",
    "X_DATE_REL      = \".//div[1]/div/div[2]/div[1]/span/span[2]\"\n",
    "# Link: try to grab an <a> within the card\n",
    "X_LINK_REL      = \".//a[@href]\"\n",
    "\n",
    "# ===================== DRIVER =====================\n",
    "def build_driver(headless: bool = True) -> webdriver.Chrome:\n",
    "    ua = (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "          \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\")\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    opts.add_argument(f\"user-agent={ua}\")\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--disable-notifications\")\n",
    "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "    # Selenium Manager will fetch the correct driver automatically (Selenium 4.6+).\n",
    "    return webdriver.Chrome(options=opts)\n",
    "\n",
    "# ===================== HELPERS =====================\n",
    "def wait_for(xpath: str, driver: webdriver.Chrome, timeout: int = WAIT_SEC):\n",
    "    return WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_element_located((By.XPATH, xpath))\n",
    "    )\n",
    "\n",
    "def scroll_lazy(driver: webdriver.Chrome, max_rounds: int = 18, pause: float = 0.75):\n",
    "    \"\"\"\n",
    "    Scroll down repeatedly to trigger lazy loading.\n",
    "    Stops early if page height no longer grows.\n",
    "    \"\"\"\n",
    "    last_h = 0\n",
    "    for i in range(max_rounds):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(pause)\n",
    "        h = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "        if h == last_h:\n",
    "            break\n",
    "        last_h = h\n",
    "\n",
    "def get_text_safe(node, rel_xpath: str) -> str:\n",
    "    try:\n",
    "        return node.find_element(By.XPATH, rel_xpath).text.strip()\n",
    "    except NoSuchElementException:\n",
    "        return \"\"\n",
    "    except StaleElementReferenceException:\n",
    "        try:\n",
    "            return node.find_element(By.XPATH, rel_xpath).text.strip()\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "def get_link_safe(node, rel_xpath: str) -> str:\n",
    "    try:\n",
    "        a = node.find_element(By.XPATH, rel_xpath)\n",
    "        href = a.get_attribute(\"href\") or \"\"\n",
    "        return href.strip()\n",
    "    except NoSuchElementException:\n",
    "        return \"\"\n",
    "    except StaleElementReferenceException:\n",
    "        try:\n",
    "            a = node.find_element(By.XPATH, rel_xpath)\n",
    "            href = a.get_attribute(\"href\") or \"\"\n",
    "            return href.strip()\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "# ===================== MAIN SCRAPER =====================\n",
    "def scrape_ticketmaster_au(driver: webdriver.Chrome) -> pd.DataFrame:\n",
    "    print(f\"Opening: {BASE_URL}\")\n",
    "    driver.get(BASE_URL)\n",
    "\n",
    "    # Wait until the section & list exist\n",
    "    try:\n",
    "        wait_for(X_SECTION, driver)\n",
    "        wait_for(X_LIST, driver)\n",
    "    except TimeoutException:\n",
    "        raise TimeoutException(\"Could not find the events section on the page (XPaths may have changed).\")\n",
    "\n",
    "    # Scroll to load all items\n",
    "    scroll_lazy(driver, max_rounds=20, pause=0.8)\n",
    "\n",
    "    # Grab items\n",
    "    try:\n",
    "        items = driver.find_elements(By.XPATH, X_ITEMS)\n",
    "    except NoSuchElementException:\n",
    "        items = []\n",
    "\n",
    "    print(f\"Found {len(items)} event cards (raw).\")\n",
    "\n",
    "    rows = []\n",
    "    scraped_at = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "    source = \"Ticketmaster AU\"\n",
    "\n",
    "    for idx, li in enumerate(items, start=1):\n",
    "        try:\n",
    "            title    = get_text_safe(li, X_NAME_REL)\n",
    "            location = get_text_safe(li, X_LOCATION_REL)\n",
    "            date_str = get_text_safe(li, X_DATE_REL)\n",
    "            link     = get_link_safe(li, X_LINK_REL)\n",
    "\n",
    "            # Skip empty rows with no title\n",
    "            if not title:\n",
    "                continue\n",
    "\n",
    "            rows.append({\n",
    "                \"Title\": title,\n",
    "                \"Date\": date_str,\n",
    "                \"Location\": location,\n",
    "                \"Link\": link,\n",
    "                \"Source\": source,\n",
    "                \"ScrapedAtUTC\": scraped_at,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            # Keep going if one card explodes\n",
    "            print(f\"[warn] Skipped li #{idx} due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"Title\", \"Date\", \"Location\", \"Link\", \"Source\", \"ScrapedAtUTC\"])\n",
    "    # Deduplicate by Link, then by (Title, Date, Location)\n",
    "    if not df.empty:\n",
    "        before = len(df)\n",
    "        df = df.drop_duplicates(subset=[\"Link\"]).copy()\n",
    "        df = df.drop_duplicates(subset=[\"Title\", \"Date\", \"Location\"]).copy()\n",
    "        after = len(df)\n",
    "        print(f\"De-duplicated: {before} -> {after}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_with_existing(df_new: pd.DataFrame, path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        df_old = pd.read_csv(path)\n",
    "        merged = pd.concat([df_old, df_new], ignore_index=True)\n",
    "        # Global dedupe again\n",
    "        merged = merged.drop_duplicates(subset=[\"Link\"]).drop_duplicates(subset=[\"Title\", \"Date\", \"Location\"])\n",
    "        return merged\n",
    "    except FileNotFoundError:\n",
    "        return df_new\n",
    "\n",
    "def main():\n",
    "    driver = build_driver(headless=True)\n",
    "    try:\n",
    "        df = scrape_ticketmaster_au(driver)\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"No events found (empty DataFrame).\")\n",
    "        return\n",
    "\n",
    "    out = merge_with_existing(df, OUTPUT_CSV)\n",
    "    out.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nSaved {len(out)} total rows -> {OUTPUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"[fatal] {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "# need to add code to show how many new events were added\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac728a",
   "metadata": {},
   "source": [
    "# Bali"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
